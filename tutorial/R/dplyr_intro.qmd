---
title: "dplyr入門"
date: "`r Sys.Date()`"
link-external-newwindow: true
toc: true
---

```{r common}
#| include: false
source("_common.R")
```

## はじめに

* 以下の内容は現在執筆中の内容の一部となります。
    * [Song Jaehyun・矢内勇生『私たちのR: ベストプラクティスの探求』(E-book)](https://www.jaysong.net/RBook/)
* いきなり**オブジェクト**、**関数**、**引数**といった馴染みのない概念が出てきます。これらの概念に馴染みのない方は、予め「[Rプログラミング入門の入門](rprogramming.qmd)」をご一読ください。
* 実習用データ: <span class="jay_btn"> [Ramen.csv](Data/Ramen.csv) [Ramen2.csv](Data/Ramen2.csv)</span>

## パッケージと実習用データの読み込み

　パッケージは{dplyr}でも、{tidyverse}でもどれでも構いません。ここではデータをtibble型として読み込むため、{tidyverse}を読み込んでおきます。

```{r}
#| message: false
library(tidyverse)
```

　それでは今回の実習用データを読み込みましょう。[Ramen.csv](Data/Ramen.csv)には「[ぐるなび](https://www.gnavi.co.jp)」から取得したラーメン屋6292店舗の情報が入っています。具体的には東京、神奈川、千葉、埼玉、大阪、京都、兵庫、奈良、和歌山それぞれ都府県にあるラーメン屋の中から最大1000店舗の情報を抽出したものです。東京都は、ぐるなびに登録したラーメン屋が3000店舗以上ですが、1000店舗の基準はぐるなびの「おすすめ」の順で上位1000店舗となります。また、店側またはぐるなびが登録したカテゴリを基準に抽出したため、実際はラーメン屋ではないにもかかわらずラーメン屋としてデータ内に含まれている可能性があります。

　まず、このデータを読み込み、`df`という名付けます。R内蔵関数である`read.csv()`を使ってデータを読み込んでも以下の内容を実習するにあたって全く問題はございません。`read.csv()`から読み込まれたデータのクラスはデータフレーム、`read_csv()`の場合はtibbleです。tibbleはデータフレームの拡張版であり、データフレームで可能な操作は全てtibbleにおいても可能です。ここではtibbleを使いますが、こちらの方が、結果が読みやすく出力されるからです。

```{r}
#| message: false
# ファイルのパスは適宜修正してください
df <- read_csv("Data/Ramen.csv")
```

　本サンプルデータはUTF-8で保存されており、文字化けが生じる場合、以下のように対処してください。

```{r}
#| eval: false
# readrパッケージのread_csv()を使う場合
df <- read_csv("Data/Ramen.csv", locale = locale(encoding = "utf8"))
```


　データの中身を確認してみましょう。

```{r}
df
```

　1行目の`# A tibble: 6,292 x 14`から、ケース数 (店舗数)は6292、変数は14個あることが分かります。各変数の詳細は以下の通りです。

```{r}
#| echo: false
#| message: false
library(gt)
tibble(Var = c("ID", "Name", "Pref", "Zipcode", "Latitude", "Longitude",
               "Line", "Station", "Walk", "Bus", "Car", "Budget", 
               "ScoreN", "Score"),
       Desc = c("店舗ID",
                "店舗名",
                "店舗の所在地 (都府県)",
                "店舗の郵便番号",
                "緯度",
                "経度",
                "最寄りの駅の路線",
                "最寄りの駅",
                "最寄りの駅からの距離 (徒歩; 分)",
                "最寄りの駅からの距離 (バス; 分)",
                "最寄りの駅からの距離 (車; 分)",
                "平均予算 (円)",
                "口コミの数",
                "口コミ評価の平均値")) %>%
  gt() %>%
  cols_label("Var" = "変数名", "Desc" = "説明")
```

　それではここからは`df`を用いた{dplyr}の様々な機能を紹介していきます。

## パイプ演算子 (`%>%`)

:::{.callout-important}
## 新しいパイプ演算子`|>`

これまでのパイプ演算子 (`%>%`) は{magrittr}パッケージから提供されましたが、R 4.1からはR内蔵のパイプ演算子 (`|>`) が追加されました。使い方はやや異なりますが、本ページの内容であれば、`%>%`の代わりに`|>`を使って頂いても問題有りません。
:::

　{dplyr}パッケージを利用する前にパイプ演算子について説明します。パイプ演算子は{dplyr}に含まれている演算子ではなく、{magrittr}という別のパッケージから提供される演算子ですが、{tidyverse}パッケージを読み込むと自動的に読み込まれます。パイプ演算子は`x %>% y()`のような書き方となりますが、これは「`x`を`y()`の第一引数として渡す」ことを意味します。`x`の部分はベクトルやデータフレームのようなオブジェクトでも、関数でも構いません。なぜなら、関数から得られた結果もまたベクトルやデータフレームといったものになるからです。つまり、`x() %>% y()`という使い方も可能です。そして、パイプは無限に繋ぐこともできます。「データ`df`を関数`x()`で処理をし、その結果をまた関数`y()`で処理する」ことは、パイプを使うと`df %>% x() %>% y()`のような書き方となります。

　たとえば、「`paste(3, "+", 5, "=", 8)`を実行し、その結果を`rep()`関数を使って3回複製し、それを`print()`を使って出力する」コードを考えてみましょう。方法としては2つ考えられます。まずは、それぞれの処理を別途のオブジェクトに格納する方法です。そして二つ目は関数の中に関数を使う方法です。

```{r}
# 方法1: 一関数一オブジェクト
Result1 <- paste(3, "+", 5, "=", 8)
Result2 <- rep(Result1, 3)
print(Result2)

# 方法2: 関数の中に関数の中に関数
print(rep(paste(3, "+", 5, "=", 8), 3))
```

　どれも結果は同じです。コードを書く手間を考えれば、後者の方が楽かも知れませんが、可読性があまりよくありません。一方、前者は可読性は良いものの、コードも長くなり、オブジェクトを2つも作ってしまうのでメモリの無駄遣いになります。

　コードの可読性と書く手間、両方を満足する書き方がパイプ演算子`%>%`です。まずは、例から見ましょう。

```{r}
# %>%を使う
paste(3, "+", 5, "=", 8) %>% rep(3) %>% print()
```

　まず、結果は先ほどと同じです。それではコードの説明をしましょう。まずは、`paste(3, "+", 5, "=", 8)`を実行します。そしてその結果をそのまま`rep()`関数の第一引数として渡されます。つまり、`rep(paste(3, "+", 5, "=", 8), 3)`になるわけです。ここでは`rep(3)`と書きましたが、第一引数が渡されたため、`3`は第二引数扱いになります (パイプ演算子前のオブジェクトを第二、三引数として渡す方法は適宜説明します。)。そして、これをまた`print()`関数に渡します。結果としては`print(rep(paste(3, "+", 5, "=", 8), 3))`となります。

　関数を重ねると読む順番は「カッコの内側から外側へ」になりますが、パイプ演算子を使うと「左 (上)から右 (下)へ」といったより自然な読み方が可能になります。また、以下のコードのように、パイプ演算子後に改行を行うことでより読みやすいコードになります。これからはパイプ演算子の後は必ず改行をします。

```{r}
#| eval: false
# 改行 (+字下げ)したらもっと読みやすくなる
paste(3, "+", 5, "=", 8) %>% 
    rep(3) %>% 
    print()
```

　パイプ演算子を使わない方法 @fig-pipe1 のようにイメージできます。一回の処理ごとに結果を保存し、それをまた次の処理時においてデータとして使うイメージです。

```{r fig-pipe1}
#| echo: false
#| fig-cap: "パイプ演算子を使わない場合"
#| out-width: "100%"
knitr::include_graphics("Figs/Handling1/Pipeline1.png")
```

　一方、 @fig-pipe2 はパイプ演算子を使う場合のプロセスです。処理後の結果を保存せず、すぐに次のプロセスに渡すことで、メモリ (図だとボウル)や時間、コードの無駄を減らすことができます。むろん、 @fig-pipe1 の結果1を使って色々試してみたい場合は、一旦結果1までは格納し、適宜引き出して使った方が効率的でしょう。パイプ演算子はたしかに便利で、「今どき」のRの書き方を象徴するようなものですが、一つの結果を出すまであまりにも多くのパイプ演算子を使うことはあ望ましくありません。

```{r fig-pipe2}
#| echo: false
#| fig-cap: "パイプ演算子を使う場合"
#| out-width: "100%"
knitr::include_graphics("Figs/Handling1/Pipeline2.png")
```

　データハンドリングもこれど同様に、様々な作業を順に沿って行う必要があります。例えば、「(1) 列を選択して、(2) 欠損値を含む列を除去して、 (3) ある変数の値を100倍にして、(4) ある変数の値がが小さい行から大きい順へ並び替える」といった手順です。これらの作業はパイプ演算子を使えば、スムーズに行うことが可能です。

## 列の抽出

### 特定の列を抽出する

　まずは、データフレーム (または、tibble)から特定の列のみを残す、除去する方法について紹介します。たとえば、`df`から`ID`、`Name`、`Pref`、`Score`のみを残すとします。{dplyr}を使わない方法と{dplyr}の`select()`関数を使った方法を紹介します。

```{r}
# dplyrを使わない方法
df[, c("ID", "Name", "Pref", "Score")]

# dplyr::select()を使う方法
# select(df, ID, Name, Pref, Score)でもOK
df %>%
  select(ID, Name, Pref, Score)
```

　どれも結果は同じですが、`select()`関数を使った方がより読みやすいコードになっているでしょう。むろん、`select()`関数を使わない方がスッキリする方も知るかも知れません。実際、自分でパッケージなどを作成する際は`select()`を使わない場合が多いです。ただし、一般的な分析の流れでは`select()`の方がコードも意味も明確となり、パイプ演算子でつなぐのも容易です。

　`select()`関数の使い方は非常に簡単です。第一引数はデータフレーム (または、tibble)ですが、パイプ演算子を使う場合は省略可能です。第二引数以降の引数はデータフレーム/tibble内の変数名です。つまり、ここには残す変数名のみを書くだけで十分です。

　また、`select()`関数を使って列の順番を変えることもできます。たとえば、`ID`、`Pref`、`Name`、`Score`の順で列を残すなら、この順番で引数を書くだけです。

```{r}
df %>%
  select(ID, Pref, Name)
```

### 特定の列を抽出し、列名を変更する

　また、特定の列を残す際、変数名を変更することも可能です。今回も`ID`、`Name`、`Pref`、`Score`のみを残しますが、`Pref`列は`Prefecture`に変えてみましょう。

```{r}
df %>%
  select(ID, Name, Prefecture = Pref, Score)
```

　抽出する際、変数を`新しい変数名 = 既存の変数名`にするだけで、変数名が簡単に変更できました。もし、特定の列は抽出しないものの、変数名を変えるにはどうすれば良いでしょうか。ここでは`df`の`Pref`を`Prefecture`に、`Walk`を`Distance`に変更してみます。{dplyr}を使わない場合と{dplyr}の`rename()`関数を使う場合を両方紹介します。

　まずは、`name()`関数についてですが、これはデータフレーム (または、tibble)の変数名をベクトルとして出力する関数です。

```{r}
names(df)
```

　察しの良い読者は気づいたかも知れませんが、`names(データフレーム/tibble名)`の結果はベクトルであり、上書きも可能です。つまり、`names(df)`の3番目と9番目の要素を`"Prefecture"`と`"Distance"`に上書きすることができるということです。

```{r}
# dplyrを使わずに列名を変更する方法
names(df)[c(3, 9)] <- c("Prefecture", "Distance")

# dfの中身を出力
df
```

　簡単に変数名の変更ができました。続いて、{dplyr}の`rename()`関数を使った方法です。今回は、`Prefecture`を`Pref`に、`Distance`を`Walk`に戻して見ましょう。そして、出力するだけにとどまらず、`df`に上書きしましょう。

```{r}
# dfのPrefectureをPrefに、DistanceをWalkに変更し、上書きする
df <- df %>%
  rename(Pref = Prefecture, Walk = Distance)
```

　これで終わりです。実は`select()`関数と使い方がほぼ同じです。ただし、残す変数名を指定する必要がなく、名前を変更する変数名と新しい変数名を入れるだけです。変数が少ないデータなら`select()`でもあまり不便は感じないかも知れませんが、変数が多くなると`rename()`関数は非常に便利です。

### 特定の列を除外する

　逆に、一部の変数をデータフレーム (または、tibble)から除去したい場合もあるでしょう。たとえば、緯度 (`Latitude`)と経度 (`Longitude`)はラーメン屋の情報としては不要かもしれません。この2つの変数を除外するためにはどうすれば良いでしょうか。まず考えられるのは、この2つの変数を除いた変数を指定・抽出する方法です。

```{r}
df %>%
  select(ID, Name, Pref, Zipcode, 
         Line, Station, Walk, Bus, Car, Budget, ScoreN, Score)
```

　かなり長いコードになりましたね。しかし、もっと簡単な方法があります。それは`-`を使う方法です。

```{r}
df %>%
  select(-Latitude, -Longitude) # select(-c(Latitude, Longitude))
```

　除外したい変数名の前に`-`を付けただけです。また、`-Latitude`と`-Longitude`をそれぞれ指定せず、`-c(Latitude, Longitude)`のように`c()`でまとめるのも可能です。

### 隣接した列を指定する

　先ほど、`df`から緯度 (`Latitude`)と経度 (`Longitude`)を除外する例を考えてみましょう。`-`を使うと簡単ですが、場合によっては残す変数名を指定する必要もあります。

```{r}
#| eval: false
df %>%
  select(ID, Name, Pref, Zipcode, 
         Line, Station, Walk, Bus, Car, Budget, ScoreN, Score)
```

　よく考えてみれば、`ID`から`Zipcode`は隣接した列ですし、`Line`から`Score`までもそうです。これは`names()`関数で確認できます。

```{r}
names(df)
```

　ここで便利な演算子が`:`です。これまで、`x`から`y`までの公差1の等差数列を作成する際に`x:y`を使って来ましたが、これに非常に似ています。データフレーム (または、tibble)の「`x`列から`y`列まで」の表記も`select()`関数内では`:`と書くことができます。したがって、上記のコードは以下のように短縮可能です。

```{r}
#| eval: false
df %>%
  select(ID:Zipcode, Line:Score)
```

　「`df`の`ID`から`Zipcode`まで、そして`Line`から`Score`までの列を選択する」という意味です。非常に便利な演算子ですので、`-`と合わせて覚えておきましょう。

### 一部の列の順番だけを変える

　ある列の位置を替えたいとします。たとえば、`Score`と`ScoreN`をそれぞれ1列目、2列目にしたい場合、どうすれば良いでしょうか。これまで勉強したことを考えると、以下のようなコードで問題ないでしょう。

```{r}
df %>%
  select(Score, ScoreN, ID:Budget)
```

　しかし、{dplyr}には`relocate()`というより便利な専用関数を提供しています。`relocate()`には変数名を指定するだけですが、ここで指定した変数がデータフレーム (または、tibble)の最初列の方に移動します。

```{r}
df %>%
  relocate(Score, ScoreN)
```

　`relocate()`を使うと`ID:Budget`が省略可能となり、より短いコードになります。もう一つの例は、最初に持ってくるのではなく、「ある変数の前」または「ある変数の後」に移動させるケースです。これも`relocate()`で可能ですが、もう一つの引数が必要です。`Pref`と`Zipcdoe`の順番を変えるなら、まずは以下のような方法が考えられます。

```{r}
df %>%
  select(ID:Name, Zipcode, Pref, Latitude:Score)
```

　これを`relocate()`で書き換えるなら、`.after`または`.before`引数が必要になります。`relocate(変数名1, .after = 変数名2)`は「変数1を変数2の直後に移動させる」
ことを意味します。

```{r}
df %>%
  relocate(Pref, .after = Zipcode)
```

　`.before`を使うことできます。この場合は「`Zipcode`を`Pref`の直前に移動させる」
ことを指定する必要があります。結果は省略しますが、自分でコードを走らせ、上と同じ結果が得られるかを確認してみてください。


```{r}
#| eval: false
df %>%
  relocate(Zipcode, .before = Pref)
```

### `select()`の便利な機能

　`select()`関数は他にも便利な機能がいくつかあります。ここではいくつの機能を紹介しますが、より詳しい内容は`?dplyr::select`を参照してください。

**`starts_with()`と`ends_with()`、`contains()`、`num_range()`: 特定の文字を含む変数を選択する**

　まずは、特定の文字を含む変数名を指定する方法です。`starts_with("X")`、`ends_with("X")`、`contains("X")`は変数名が`"X"`で始まるか、`"X"`で終わるか、`"X"`を含むかを判断し、条件に合う変数名を返す関数です。実際の例を見ましょう。

```{r}
# ID、Nameに続いて、Scoreで始まる変数名を抽出
df %>%
  select(ID, Name, starts_with("Score"))

# eで終わる変数名を除去
df %>%
  select(-ends_with("e")) # !ends_with("e")も可能

# reを含む変数名を抽出するが、ScoreNは除去する
df %>%
  select(contains("re"), -ScoreN)
```

　他の使い方としては`X1`、`X2`のような「文字+数字」の変数を選択する際、`starts_with()`が活躍します。たとえば、以下のような`myDF1`があるとします。

```{r}
# tibble()の代わりにdata.frame()も使用可能
myDF1 <- tibble(
  ID  = 1:5,
  X1  = c(2, 4, 6, 2, 7),
  Y1  = c(3, 5, 1, 1, 0),
  X1D = c(4, 2, 1, 6, 9),
  X2  = c(5, 5, 6, 0, 2),
  Y2  = c(3, 3, 2, 3, 1),
  X2D = c(8, 9, 5, 0, 1),
  X3  = c(3, 0, 3, 0, 2),
  Y3  = c(1, 5, 9, 1, 3),
  X3D = c(9, 1, 3, 3, 8)
)

myDF1
```

　この`myDF1`から`ID`、`Y1`、`Y2`、`Y3`を抽出するにはどうすれば良いでしょうか。これらの変数は隣接していないため、`:`も使えませんが、`starts_with()`を使えば簡単です。

```{r}
myDF1 %>%
  select(ID, starts_with("Y"))
```

　それでは、`ID`、`X1`、`X2`、`X3`はどうでしょうか。`starts_with("X")`だと、`X1c`なども選択されてしまいますね。ここで`-ends_with()`の出番です。つまり、「まずは`starts_with("X")`で`X`で始まる変数を選択し、続いて、`D`で終わるものを除外すればいいじゃん？」です。それでは、やってみましょうか。

```{r}
myDF1 %>%
  select(ID, starts_with("X"), -ends_with("D"))
```

　あらら、`ID`も同時になくなりましたね[^numrange]。実はこのような時のために用意された関数があり、それが`num_range()`です。`num_range()`の第一引数は`starts_with()`関数と同じですが、第二引数も必要です。この第二引数にはnumeric型のベクトルが必要です。`1:3`でも、`c(1, 2, 3)`でも構いません。たとえば、`ID`、`X1`、`X2`、`X3`するには以下のように書きます。

[^numrange]: 実は`select(starts_with("X"), -ends_with("D"), ID)`のように順番を変えると`ID`は最後の列になりますが、とりあえず残ります。なぜなら、`select()`関数は左側から右側の方へコードを実行するからです。

```{r}
myDF1 %>%
  select(ID, num_range("X", 1:3))
```

**`all_of()`と`any_of()`: 文字型ベクトルを用いた変数の選択**

　`all_of()`と`any_of()`は`select()`内の変数名として文字型ベクトルを使う際に用いる関数です。これは抽出したい列名が既にcharacter型ベクトルとして用意されている場合、便利な関数です。たとえば、以下の`Name_Vec`を考えてみましょう。

```{r}
Name_Vec <- c("X1", "X2", "X3")
```

　この`Name_Vec`の要素と同じ列名を持つ列と`ID`列を`myDF1`から抽出する方法は以下の2通りです。

```{r}
myDF1[, c("ID", Name_Vec)]

myDF1 %>%
  select(ID, all_of(Name_Vec))
```

　今の例だと、`select()`を使わない前者の方が便利かも知れませんが、`select()`内に外の変数名も指定する場合も多いので、後者の方が汎用性は高いです。私から見れば、今の例でも後者の方が読みやすく、使いやすいと思います。

　それでは以下のような`Name_Vec`はどうでしょう。今回は、`myDF1`に含まれていない`X4`と`X5`もあります。

```{r}
#| error: true
Name_Vec <- c("X1", "X2", "X3", "X4", "X5")

myDF1 %>%
  select(all_of(Name_Vec))
```

　このようにエラーが出てしまします。つまり、`all_of()`の場合、引数の要素全てがデータフレーム (または、tibble)に存在する必要があります。もし、ないものは無視して、合致する列だけ取り出したいはどうすれば良いでしょうか。そこで登場するのが`any_of()`です。

```{r}
myDF1 %>%
  select(any_of(Name_Vec))
```

　`any_of()`の方がより使いやすいと思う方も多いでしょうが、必ずしもそうとは限りません。たとえば、`Name_Vec`に誤字などが含まれる場合、`any_of()`だと誤字が含まれている変数は取り出しません。この場合はむしろちゃんとエラーを表示してくれた方が嬉しいですね。

**`last_col()`: 最後の列を選択する**

　普段あまり使わない機能ですが、最後の列を選択する`last_col()`という関数もあります。たとえば、`last_col(0)`にすると最後の列を選択し、`last_col(1)`なら最後から2番目の列を選択します。たとえば、`df`から`ID`と最後の列を取り出してみましょう。

```{r}
# IDと最後の列のみを抽出
df %>%
  select(ID, last_col(0))
```

　最後の2行分を取り出すことも可能です。この場合は`last_col()`の引数を長さ1ベクトルでなく、長さ2以上のベクトルにします。最後の行が`0`、その手前の行が`1`ですから、中の引数は`1:0`となります。`0:1`でも可能ですが、結果が若干異なります。

```{r}
# IDと最後の2列分を抽出 (引数を1:0と設定)
df %>%
  select(ID, last_col(1:0))
```

```{r}
# IDと最後の2列分を抽出 (引数を0:1と設定)
df %>%
  select(ID, last_col(0:1))
```

　`last_col()`の引数を`1:0`にするか`0:1`にするかによって抽出される順番が異なります。`1:0`は`c(1, 0)`、`0:1`は`c(0, 1)`と同じであることを考えると理由は簡単です。`c(1, 0)`の場合、`last_col(1), last_col(0)`の順番で処理をし、`c(0, 1)`は`last_col(0)`、`last_col(1)`の順番で処理を行うからです。

　この`last_col()`の引数を空っぽにするとそれは最後の列を意味します。これを利用すれば、「ある変数の最後の列へ移動させる」こともできます。たとえば、`ID`を最後の列に移動させたい場合、`relocate(ID, .after = last_col())`のように書きます。

**`where()`: データ型から変数を選択する**

　最後に、「numeric型の列のみ抽出したい」、「character型の列だけほしい」場合に便利な`where()`関数を紹介します。`where()`の中に入る引数は一つだけであり、データ型を判定する関数名が入ります。たとえば、numeric型か否かを判断する関数は`is.numeric`です。`df`からnumeric型の変数のみを抽出したい場合は以下のように書きます。

```{r}
# numeric型の列を抽出する
df %>%
  select(where(is.numeric))
```

　`!`を使って条件に合致する列を除外することも可能です。もし、character型の列を除外する場合は以下のように`!where(is.character)`を指定します。

```{r}
# character型でない列を抽出する
df %>%
  select(!where(is.character))
```

　`&`を使って複数の条件を使うことも可能です。たとえば、`ID`変数に加えて「`"L"`で始まる変数の中でnumeric型の列を抽出」するコードは以下のようになります。

```{r}
# IDと、Lで始まるnumeric型の列を抽出する
df %>%
  select(ID, starts_with("L") & where(is.numeric))
```

## 行の抽出

### 指定した行を抽出する

　他にも特定の行を抽出する場合があります。たとえば、「`df`の最初の5行」や「`df`の8行目のケース」といった場合です。この操作には{dplyr}の`slice_*()`関数群が便利です。それではそれぞれの関数の使い方について紹介していきます。その前に、実習用データとして`df`から一部の列のみを抽出した`selelct.df`を作成します。

```{r}
select.df <- df %>% 
  select(ID, Name, Pref, Budget, Score)
```

**`slice()`: 指定した番号の行のみ抽出する**

　`select.df`から2, 8, 9行目の行を抽出したいとします。このような簡単な操作はパッケージを使わず、以下のように抽出することができます。

```{r}
# select.dfから2, 8, 9行目の行を抽出し、出力する
select.df[c(2, 8, 9),]
```

　しかし、以下の`slice()`関数を使うとパイプ演算子を前後に付けることが可能であり[^slice1]、コードの可読性も高いです。`slice()`関数には以下のように抽出したい行の番号を入れるだけです。

[^slice1]: 実は`select.df[, c(2, 8, 9)]`でも前後にパイプ演算子を使うことは可能ですが、コードが読みにくくなるため、推奨しません。

```{r}
# select.dfから2, 8, 9行目の行を抽出し、出力する
select.df %>% 
  slice(2, 8, 9) # slice(c(2, 8, 9))もOK
```

　`slice(2, 8, 9)`でも`slice(c(2, 8, 9))`でも構いません。また、隣接した行でしたら`:`を使うことも可能です。たとえば、10行目から15行目まで抽出する場合は`slice(10:15)`のような書き方も出来ます。

**`slice_head()`: 最初のn行を抽出する**

```{r}
# select.dfから最初の3行抽出し、出力する
select.df %>% 
  slice_head(n = 3)
```

　これは`head(データ名, n = 出力する個数)`と同じ動きをする関数です。注意点としては引数`n = `を必ず付ける点です。たとえば、`slice_head(3)`にすると、`select.df`の3行目のみ抽出されます。

**`slice_tail()`: 最後のn行を抽出する**

```{r}
# select.dfから最後の7行を抽出し、出力する
select.df %>% 
  slice_tail(n = 7)
```

　これは`tail(データ名, n = 出力する個数)`と同じ動きをする関数です。ちなみに、この`n`引数も`n =`を明記する必要があります。

**`slice_max()`: 指定した変数が大きい順でn行抽出する**

　`slice_max()`は指定した変数が大きい順で`n`行抽出する関数です。たとえば、`Budget`が高い順で4店舗を抽出する場合は以下のように書きます。

```{r}
# select.dfからScoreの値が高い順で5行を抽出し、出力する
select.df %>% 
  slice_max(Budget, n = 4)
```

**`slice_min()`: 指定した変数が小さい順でn行抽出する**

　一方、`slice_min()`関数が小さい順で抽出します。

```{r}
# select.dfからScoreの値が低い順で3行を抽出し、出力する
select.df %>% 
  slice_min(Score, n = 3)
```

　ただし、`n = 3`と指定したはずなのに、4行が抽出されました。これは同点のケースがあるからです。実際、`select.df`には`Score`が1のケースが4つあります。もし、同点の存在により`n`に収まらない場合、`slice_max()`、`slice_min()`関数は`n`を超える行を出力します。これを強制的に`n`行に合わせるためには`with_ties = FALSE`引数を付けます。この場合、データで格納されている順で`n`個のみ出力されます。

```{r}
select.df %>% 
  slice_min(Score, n = 3, with_ties = FALSE)
```

**`slice_sample()`: 無作為にn行を抽出する**

　最後に無作為に`n`行を抽出する`slice_sample()`関数です。引数は`n`であり、抽出したい行数を指定します。たとえば、`select.df`から無作為に10行抽出したい場合は、

```{r}
# select.dfから無作為に5行を抽出し、出力する
select.df %>% 
  slice_sample(n = 10)
```

　のように書きます。ブートストラップ法や機械学習における交差検証 (cross-validation)の際に有用な関数ですが、ブートストラップや機械学習のパッケージの多くはサンプル分割の関数を提供しているため、あまり使う機会はないでしょう。また、`slice_sample()`関数をブートストラップ法のために用いる場合は、ケースを反復抽出する必要があり、`replace = TRUE`を付けると反復抽出を行います。デフォルト値は`FALSE`です。

### 条件に合致する行を抽出する

　これまで見てきた`slice()`を用いる行の抽出は、実際あまり使う機会がありません。多くの場合、「何かの条件と合致するケースのみ抽出する」または、「何かの条件と合致しないケースのみを抽出する」やこれらの組み合わせで行の抽出を行います。そこで登場するのが`dplyr()`パッケージの`filter()`関数です。`filter()`関数の使い方は以下の通りです。

```r
# dplyr::filter()の使い方
filter(データフレーム/tibble名, 条件1, 条件2, ...)
```

　むろん、第一引数がデータですから、`%>%`を使うことも可能です。

```r
# dplyr::filter()の使い方 (パイプを使う方法)
データフレーム/tibble名 %>%
  filter(条件1, 条件2, ...)
```

　まずは、条件が一つの場合を考えてみましょう。ここでは「`Pref`が`"京都府"`であるケースのみに絞り、`Name`と`Station`、`Score`列のみを出力する」ケースを考えてみましょう。まず、`filter()`関数で行を抽出し、続いて`select()`関数で抽出する列を指定します。むろん、今回の場合、`filter()`と`select()`の順番は替えても構いません。

```{r}
# dfからPrefが"京都府"であるケースのみ残し、df2という名で保存
df2 <- df %>%
  filter(Pref == "京都府")

# df2からName, Station, Score列を抽出
df2 %>%
  select(Name, Station, Score)
```

　これは`df`から`Pref == "京都府"`のケースのみ残したものを`df2`として格納し、それをまた`select()`関数を使って列を抽出するコードです。これでも問題ありませんが、これだとパイプ演算子の便利さが分かりません。パイプ演算子は複数使うことが可能です。

```{r}
df %>%
  filter(Pref == "京都府") %>%
  select(Name, Station, Score)
```

　全く同じ結果ですが、無駄に`df2`というデータフレーム (または、tibble)を作らず済むので、メモリの観点からも嬉しいですし、何よりコードが短く、しかも可読性も上がりました。

　今回は`==`を使って**合致する**ものに絞りましたが、`!=`を使って**合致しない**ものに絞ることも可能です。または、比較演算子 (`<`、`>`、`>=`、`<=`など)を使うことも可能です。それでは、組み込み数 (`ScoreN`)が*0ではない*ケースを取り出し、`Name`、`Station`、`ScoreN`、`Score`列を出力させてみましょう。

```{r}
df %>%
  filter(ScoreN != 0) %>%
  select(Name, Station, starts_with("Score"))
```

　これで口コミ数が1以上の店舗のみに絞ることができました。ただし、店によっては口コミはあっても、評価 (`Score`)が付いていないところもあります。たとえば、「刀削麺・火鍋・西安料理 XI'AN（シーアン） 後楽園店」の場合、口コミはありますが、評価はありません。したがって、今回は評価が付いている店舗に絞ってみましょう。

```{r}
df %>%
  filter(Score != NA) %>%
  select(Name, Station, starts_with("Score"))
```

　あらら、何の結果も表示されませんでした。これは`filter()`内の条件に合致するケースが存在しないことを意味します。しかし、先ほどの結果を見ても、評価が付いている店はいっぱいありましたね。これはなぜでしょう。

　察しの良い読者さんは気づいているかと思いますが、`NA`か否かを判定する際は`==`や`!=`は使えません。`is.na()`を使います。`filter(is.na(Score))`なら「`Score`が`NA`**である**ケースに絞る」ことを意味しますが、今回は「`Score`が`NA`**でない**ケースに絞る」ことが目的ですので、`is.na()`の前に`!`を付けます。

```{r}
df %>%
  filter(!is.na(Score)) %>%
  select(Name, Station, starts_with("Score"))
```

　これで口コミ評価が登録された店舗に絞ることができました。

　続いて、複数の条件を持つケースを考えてみましょう。例えば、「京都府内の店舗で、口コミ評価が3.5以上の店舗」を出力したい場合、以下のようなコードとなります。

```{r}
df %>%
  filter(Pref == "京都府", Score >= 3.5) %>%
  select(Name, Station, ScoreN, Score)
```

　条件を`filter()`内に追加するだけです。今回は`!is.na(Score)`は不要です。なぜなら、`Score >= 3.5`という条件で既に欠損値は対象外になるからです。条件文が複数ある場合、ANDかORかを指定する必要があります。つまり、条件文AとBがある場合、「AとB両方満たすものを出力する」か「AとBどちらかを満たすものを出力するか」を指定する必要があります。今の結果ってANDでしたよね。`filter()`関数は、別途の指定がない場合、全てAND扱いになります。RのAND演算子は`&`ですので、以上のコードは以下のコードと同じです。

```{r}
df %>%
  filter(Pref == "京都府" & Score >= 3.5) %>%
  select(Name, Station, ScoreN, Score)
```

　AND演算子 (`&`)が使えるということはOR演算子 (`|`)も使えることを意味します。たとえば、`Station`が`"高田馬場駅"`か`"三田駅"`の条件を指定したい場合、

```{r}
df %>% 
  filter(Station == "高田馬場駅" | Station == "三田駅") %>%
  select(Name, Station, ScoreN, Score)
```

　のように書きます（ちなみに高田馬場の「やまぐち」は本当に美味しいです）。むろん、複数の変数を用いたORも可能です。たとえば、「`Pref`が`"京都府"`か`Score`が3以上」のような条件も可能ですが (`Pref == "京都府" | Score >= 3`)、実際、このような例はあまりありません。よく使うのは「変数`X`が`a`か`b`か`c`か」のような例です。ただし、この場合は`|`を使わないもっと簡単な方法があります。それは`%in%`演算子です。以下のコードは上のコードと同じものです。

```{r}
df %>% 
  filter(Station %in% c("高田馬場駅", "三田駅")) %>%
  select(Name, Station, ScoreN, Score)
```

　結局、`|`が使われるケースがかなり限定されます。あるとすれば、「変数`X`が`a`以下か、`b`以上か」のようなケースですね。ただし、`&`と`|`を同時に使うケースは考えられます。たとえば、大阪駅と京都駅周辺のうまいラーメン屋を調べるとします。問題は美味しさの基準ですが、3.5点以上としましょう。ただし、京都府民はラーメンに非常に厳しく、3点以上なら美味しいと仮定します。この場合、「(`Station`が`"大阪駅"`かつ`Score >= 3.5`)、または(`Station`が`"京都駅"`かつ`Score >= 3`)」のような条件が必要になります。`()`は「`()`の中から判定せよ」という、普通の算数での使い方と同じです。それでは、実際に検索してみましょう。

```{r}
df %>%
  filter((Station == "大阪駅" & Score >= 3.5) | (Station == "京都駅" & Score >= 3)) %>%
  select(Name, Station, Walk, ScoreN, Score)
```

　Songが大好きな神座がヒットして嬉しいです。

## 行のソート

　続いて、行のソートについて解説します。「食べログ」などのレビューサービスを利用する場合、口コミ評価が高い順で見るのが一般的でしょう[^tabelog]。また、サッカーのランキングも多くは1位から下の順位で掲載されるのが一般的です。ここではこのようにある変数の値順に行を並び替える方法について説明します。

[^tabelog]: サービスによってはこの機能が有料になっていたりもしますね。

　ソートには{dplyr}パッケージの`arrange()`関数を使います。引数は変数名のみです。たとえば、奈良県のラーメン屋を検索してみましょう。並び替える順は駅から近い店舗を上位に、遠い店舗を下位に並べます。このような順は**昇順 (ascending)**と呼ばれ、ランキング表などでよく見ます。駅から近い順にソートするので、まず最寄りの駅情報が欠損でないことが必要です。また、ラーメン屋の評価も気になるので口コミが1つ以上付いている店舗に絞りましょう。表示する列は店舗名、最寄りの駅、徒歩距離、口コミ数、点数です。

```{r}
df %>%
  filter(Pref == "奈良県", !is.na(Station), ScoreN > 0) %>%
  select(Name, Station, Walk, ScoreN, Score) %>%
  arrange(Walk) %>%
  print(n = Inf)
```

　3行まではこれまで習ってきたもので、4行目がソートの関数、`arrange()`です。引数はソートの基準となる変数で、今回は最寄りの駅からの徒歩距離を表す`Walk`です。5行目は省略可能ですが、`tibble`クラスの場合、10行までしか出力されないので、`print(n = Inf)`で「すべての行を表示」させます。`n`を指定することで出力される行数が調整可能です。奈良県のラーメン屋の中で最寄りの駅から最も近い店は「[麺屋 あまのじゃく 本店](https://www.menya-amanojaku.com)」で徒歩2分でした。京田辺店も駅から約2分ですし、近いですね。ちなみにSongはここの塩とんこつが好きです。世界一こってりなラーメンとも言われる「チョモランマ」で有名な「[まりお流ラーメン](http://www.marioramen.com)」は新大宮駅から徒歩20分でかなり遠いことが分かります。

　続いて、駅からの距離ではなく、評価が高い順にしてみましょう。評価が高いほど上に来るので、今回は昇順でなく、**降順 (descending)**でソートする必要があります。`arrange()`関数は基本的に、指定された変数を基準に昇順でソートします。降順にするためには`desc()`関数を更に用います。たとえば、`arrange(desc(変数名))`のようにです。それでは実際にやってみましょう。上のコードの4行目を`arange(Walk)`から`arrange(desc(Score))`にちょっと修正するだけです。

```{r}
df %>%
  filter(Pref == "奈良県", !is.na(Station), ScoreN > 0) %>%
  select(Name, Station, Walk, ScoreN, Score) %>%
  arrange(desc(Score)) %>%
  print(n = Inf)
```

　よく考えてみれば、「評価が同点の場合、どうなるの?」と疑問を抱く方がいるかも知れません。たとえば、7行目の「河童ラーメン本舗 押熊店」と8行目の「無鉄砲がむしゃら」はどれも評価が4点ですが、「河童ラーメン本舗 押熊店」が先に表示されます。そのこれは簡単です。同点の場合、データセット内で上に位置する行が先に表示されます。これを確認するには`which()`関数を使います。`()`内に条件文を指定することで、この条件に合致する要素の位置を返します。もし、条件に合致するものが複数あった場合は全ての位置を返します[^which]。

[^which]: たとえば、データ内に「ラーメンショップ」という店舗は3店舗あり、この場合、長さ3のベクトルが返されます。

```{r}
which(df$Name == "河童ラーメン本舗 押熊店")
which(df$Name == "無鉄砲がむしゃら")
```

　データ内に「河童ラーメン本舗 押熊店」がより上に位置することが分かります。「もし同点なら口コミ評価数が多いところにしたい」場合はどうすれば良いでしょうか。これは`arrange()`内に変数名を足すだけで十分です。

```{r}
df %>%
  filter(Pref == "奈良県", !is.na(Station), ScoreN > 0) %>%
  select(Name, Station, Walk, ScoreN, Score) %>%
  arrange(desc(Score), desc(ScoreN)) %>%
  print(n = Inf)
```

　ソートの基準は`arrange()`内において先に指定された変数の順番となります。「口コミ評価も評価数も同じなら、駅から近いところにしたい」場合は変数が3つとなり、`Score`、`ScoreN`、`Walk`の順で入れます。

```{r}
df %>%
  filter(Pref == "奈良県", !is.na(Station), ScoreN > 0) %>%
  select(Name, Station, Walk, ScoreN, Score) %>%
  arrange(desc(Score), desc(ScoreN), Walk) %>%
  print(n = Inf)
```

## 記述統計量の計算

### `summarise()`による記述統計量の計算

　ある変数の平均値や標準偏差、最小値、最大値などの記述統計量 (要約統計量)を計算することも可能です。これは`summarize()`または`summarise()`関数を使いますが、この関数は後で紹介する`group_by()`関数と組み合わせることで力を発揮します。ここではグルーピングを考えずに、全データの記述統計量を計算する方法を紹介します。

　`summarise()`関数の使い方は以下の通りです。

```r
# summarise()関数の使い方
データフレーム/tibble名 %>%
  summarise(新しい変数名 = 関数名(計算の対象となる変数名))
```

　もし、`Score`変数の平均値を計算し、その結果を`Mean`という列にしたい場合は以下のようなコードになります。

```{r}
df %>%
  summarise(Mean = mean(Score))
```

　ただし、`mean()`関数は欠損値が含まれるベクトルの場合、`NA`を返します。この場合方法は2つ考えられます。

1. `filter()`関数を使って`Score`が欠損しているケースを予め除去する。
2. `na.rm`引数を指定し、欠損値を除去した平均値を求める。

　ここでは2番目の方法を使います。

```{r}
df %>%
  summarise(Mean = mean(Score, na.rm = TRUE))
```

　`df`の`Score`変数の平均値は`r round(mean(df$Score), 2)`であることが分かります。また、`summarise()`関数は複数の記述統計量を同時に計算することも可能です。以下は`Score`変数の平均値、中央値、標準偏差、最小値、最大値、第一四分位点、第三四分位点を計算し、`Score.Desc`という名のデータフレーム (または、tibble)に格納するコードです。

```{r}
Score.Desc <- df %>%
  summarize(Mean   =     mean(Score,       na.rm = TRUE),  # 平均値
            Median =   median(Score,       na.rm = TRUE),  # 中央値
            SD     =       sd(Score,       na.rm = TRUE),  # 標準偏差
            Min    =      min(Score,       na.rm = TRUE),  # 最小値
            Max    =      max(Score,       na.rm = TRUE),  # 最大値
            Q1     = quantile(Score, 0.25, na.rm = TRUE),  # 第一四分位点
            Q3     = quantile(Score, 0.75, na.rm = TRUE))  # 第三四分位点

Score.Desc
```

　むろん、複数の変数に対して記述統計量を計算することも可能です。たとえば、平均予算 (`Budget`)、口コミ数 (`ScoreN`)、口コミ評価 (`Score`)の平均値を求めるとしたら、

```{r}
df %>%
  summarize(Budget_Mean = mean(Budget, na.rm = TRUE), # 平均予算の平均値
            SocreN_Mean = mean(ScoreN, na.rm = TRUE), # 口コミ数の平均値
            Score_Mean  = mean(Score,  na.rm = TRUE)) # 評価の平均値
```

のように書きます。実は`summarise()`はこれくらいで十分便利です。ただし、以上の操作はもっと簡単なコードに置換できます。ただし、ラムダ式など、やや高度な内容になるため、以下の内容は飛ばして、次の節 (グルーピング)を読んでいただいても構いません。

　まずは、複数の変数に対して同じ記述統計量を求める例を考えてみましょう。たとえば、`Budget`、`ScoreN`、`Score`に対して平均値を求める例です。これは`across()`関数を使うとよりコードが短くなります。まずは`across()`関数の書き方から見ましょう。

```r
# across()の使い方
データフレーム/tibble名 %>%
  summarise(across(変数名のベクトル, 記述統計を計算する関数名, 関数の引数))
```

　*変数名のベクトル*は長さ1以上のベクトルです。たとえば、`Budget`、`ScoreN`、`Score`の場合`c(Budget, ScoreN, Score)`になります。これは`df`内で隣接する変数ですから`Budget:Score`の書き方も使えます。また、`where()`や`any_of()`、`starts_with()`のような関数を使って変数を指定することも可能です。*関数名*は`mean`や`sd`などの関数名です。ここは`関数名()`でななく、`関数名`であることに注意してください。*引数*は前の関数に必要な引数です。引数を必要としない関数なら省略可能ですが、`na.rm = TRUE`などの引数が必要な場合は指定する必要があります。それでは`Budget`、`ScoreN`、`Score`の平均値を計算してみましょう。

```{r}
df %>%
  summarize(across(Budget:Score, mean, na.rm = TRUE))
```

　`across()`使わない場合、4行必要だったコードが2行になりました。変数が少ない場合は`across()`を使わない方が、可読性が高くなる場合もあります。しかし、変数が多くなる場合、可読性がやや落ちても`across()`を使った方が効率的でしょう。

　次は、ある変数に対して複数の記述統計量を計算したい場合について考えます。`Budget`、`ScoreN`、`Score`変数の第一四分位点と第三四分位点を`across()`を使わずに計算すると家のような7行のコードになります。

```{r}
df %>%
  summarize(Budget_Q1 = quantile(Budget, 0.25, na.rm = TRUE),
            Budget_Q3 = quantile(Budget, 0.75, na.rm = TRUE),
            ScoreN_Q1 = quantile(ScoreN, 0.25, na.rm = TRUE),
            ScoreN_Q3 = quantile(ScoreN, 0.75, na.rm = TRUE),
            Score_Q1  = quantile(Score,  0.25, na.rm = TRUE),
            Score_Q3  = quantile(Score,  0.75, na.rm = TRUE))
```

　この作業も`across()`を使ってより短縮することができます。ここではラムダ式の知識が必要になります。ラムダ関数とは関数名を持たない[無名関数 (anonymous functions)](https://ja.wikipedia.org/wiki/無名関数)を意味しますが、詳細は割愛します。興味のある読者は[Wikipedia](https://ja.wikipedia.org/wiki/無名関数)などを参照してください。簡単にいうとその場で即席に関数を作成し、計算が終わったら破棄する関数です。ただ、Rは基本的にラムダ式を提供しているのではなく、{purrr}パッケージのラムダ式スタイルを使用します。まずは、書き方から確認します。

```r
# ラムダ式を用いたacross()の使い方
データフレーム/tibble名 %>%
  summarise(across(変数名のベクトル, list(結果の変数名 = ラムダ式)))
```

　先ほどの書き方と似ていますが、関数を複数書く必要があるため、今回は関数名をlist型にまとめます。そして、*結果の変数名*は結果として出力されるデータフレーム (または、tibble)の列名を指定する引数です。たとえば、`Mean`にすると結果は`元の変数名1_Mean`、`元の変数名2_Mean`...のように出力されます。そして、ラムダ式が実際の関数が入る箇所です。とりあえず今回はコードを走らせ、結果から確認してみましょう。

```{r}
df %>%
  summarize(across(Budget:Score, list(Q1 = ~quantile(.x, 0.25, na.rm = TRUE),
                                      Q3 = ~quantile(.x, 0.75, na.rm = TRUE))))
```

　結果の列名が`Budget_Q1`、`Budget_Q3`、`ScoreN_Q1`...のようになり、それぞれの変数の第一四分位点と第三四分位点が出力されます。問題はラムダ式の方ですが、普通の関数に非常に近いことが分かります。`across()`内のラムダ式は`~関数名(.x, その他の引数)`のような書き方になります。関数名の前に`~`が付いていることに注意してください。分位数を求める関数は`quantile()`であり、`quantile(ベクトル, 分位数)`であり、必要に応じて`na.rm`を付けます。この分位数が0.25なら第一四分位点、0.5なら第二四分位点 (=中央値)、0.75なら第三四分位点になります。それではラムダ式`~quantile(.x, 0.25, na.rm = TRUE)`はどういう意味でしょうか。これは`.x`の箇所に`Budget`や`ScoreN`、`Score`が入ることを意味します。`.x`という書き方は決まりです。`.y`とか`.Song-san-Daisuki`などはダメです。そして、`0.25`を付けることによって第一四分位点を出力するように指定します。また、`Budget`、`ScoreN`、`Score`に欠損値がある場合、無視するように`na.rm = TRUE`を付けます。

　ラムダ式を自分で定義する関数で表現すると、以下のようになります。

```r
# 以下の3つは同じ機能をする関数である

# ラムダ式
~quantile(.x, 0.25, na.rm = TRUE)

# 一般的な関数の書き方1
名無し関数 <- function(x) {
  quantile(x, 0.25, na.rm = TRUE)
}

# 一般的な関数の書き方2
名無し関数 <- function(x) quantile(x, 0.25, na.rm = TRUE)
```

　この3つは全て同じですが、ラムダ式は関数名を持たず、その場で使い捨てる関数です。むろん、ラムダ式を使わずに事前に第一四分位点と第三四分位点を求める関数を予め作成し、ラムダ式の代わりに使うことも可能です。まずは第一四分位点と第三四分位点を求める自作関数`FuncQ1`と`FuncQ2`を作成します。

```{r}
# ラムダ式を使わない場合は事前に関数を定義しておく必要がある
FuncQ1 <- function(x) {
  quantile(x, 0.25, na.rm = TRUE)
}
FuncQ3 <- function(x) {
  quantile(x, 0.75, na.rm = TRUE)
}
```

　後は先ほどのほぼ同じ書き方ですが、今回はラムダ式を使わないため関数名に`~`を付けず、関数名のみで十分です。

```{r}
# やっておくと、summarise()文は簡潔になる
df %>%
  summarize(across(Budget:Score, list(Q1 = FuncQ1, Q3 = FuncQ3)))
```

　事前に関数を用意するのが面倒ですが、`across()`の中身はかなりスッキリしますね。もし、このような作業を何回も行うなら、ラムダ式を使わず、自作関数を用いることも可能です。ただし、自作関数であっても引数が2つ以上必要な場合はラムダ式を使います。

### `summarise()`に使える便利な関数

　以下の内容は後で説明する`group_by()`関数を使っているため、まだ`group_by()`に馴染みのない読者はまずはここを読み飛ばし、グルーピングの節にお進みください。

**`IQR()`: 四分位範囲を求める**

　四分位範囲は第三四分位点から第一四分位点を引いた値であり、Rの内蔵関数である`IQR()`を使えば便利です。この関数は`mean`や`sd()`関数と同じ使い方となります。

```{r}
df %>%
  filter(!is.na(Walk)) %>% # 予め欠損したケースを除くと、後でna.rm = TRUEが不要
  group_by(Pref) %>%
  summarise(Mean    = mean(Walk),
            SD      = sd(Walk),
            IQR     = IQR(Walk),
            N       = n(),
            .groups = "drop") %>%
  arrange(Mean)
```

**`first()`、`last()`、`nth()`: n番目の要素を求める**

　稀なケースかも知れませんが、データ内、またはグループ内の`n`番目の行を抽出する時があります。たとえば、市区町村の情報が格納されているデータセットで、人口が大きい順でデータがソートされているとします。各都道府県ごとに最も人口が大きい市区町村のデータ、あるいは最も少ない市区町村のデータが必要な際、`first()`と`last()`関数が有効です。

　それでは各都道府県ごとに「最も駅から遠いラーメン屋」の店舗名と最寄りの駅からの徒歩距離を出力したいとします。まずは、徒歩距離のデータが欠損しているケースを除去し、データを徒歩距離順でソートします。これは`filter()`と`arrange()`関数を使えば簡単です。続いて、`group_by()`を使って都府県単位でデータをグループ化します。最後に`summarise()`関数内に`last()`関数を使います。データは駅から近い順に鳴っているため、各都府県内の最後の行は駅から最も遠い店舗になるからです。

```{r}
df %>%
  filter(!is.na(Walk)) %>%
  arrange(Walk) %>%
  group_by(Pref) %>%
  summarise(Farthest  = last(Name),
            Distance  = last(Walk),
            .groups   = "drop")
```

　この`last()`を`first()`に変えると、最寄りの駅から最も近い店舗情報が表示されます。また、「`n`番目の情報」が必要な際は`nth()`関数を使います。`nth(Name, 2)`に変えることで2番目の店舗名が抽出できます。

**`n_distinct()`: ユニーク値の個数を求める**

　`n_distinct()`は何種類の要素が含まれているかを計算する関数であり、`length(unique())`関数と同じ機能をします。たとえば、以下の`myVec1`に対して何種類の要素があるかを確認してみましょう。

```{r}
myVec1 <- c("A", "B", "B", "D", "A", "B", "D", "C", "A")

unique(myVec1)
```

　`myVec1`は`"A"`、`"B"`、`"D"`、`"C"`の要素で構成されていることが分かります。これが`myVec1`の**ユニーク値 (unique values)**です。そして、このユニーク値の個数を調べるために`length()`を使います。

```{r}
length(unique(myVec1))
```

　これで`myVec1`は4種類の値が存在することが分かります。これと全く同じ機能をする関数が`n_distinct()`です。

```{r}
n_distinct(myVec1)
```

　この関数を`summarise()`に使うことで、都府県ごとに駅の個数が分かります。あるいは「東京都内の選挙区に、これまでの衆院選において何人の候補者が存在したか」も分かります。ここでは`df`内の都府県ごとに駅の個数を計算してみましょう。最後の駅数が多い順でソートします。

```{r}
df %>%
  filter(!is.na(Station)) %>% # 最寄りの駅が欠損しているケースを除去
  group_by(Pref) %>%
  summarise(N_Station = n_distinct(Station),
            .groups   = "drop") %>%
  arrange(desc(N_Station))
```

　当たり前かも知れませんが、駅数が最も多いのは東京都で次が大阪府であることが分かります。

**`any()`、`all()`: 条件に合致するか否かを求める**

　`any()`と`all()`はベクトル内の全要素に対して条件に合致するか否かを判定する関数です。ただし、`any()`は一つの要素でも条件に合致すれば`TRUE`を、全要素が合致しない場合`FALSE`を返します。一方、`all()`は全要素に対して条件を満たせば`TRUE`、一つでも満たさない要素があれば`FALSE`を返します。以下は`any()`と`all()`の例です。

```{r}
myVec1 <- c(1, 2, 3, 4, 5)
myVec2 <- c(1, 3, 5, 7, 11)

any(myVec1 %% 2 == 0) # myVec1を2で割った場合、一つでも余りが0か
all(myVec1 %% 2 == 0) # myVec1を2で割った場合、全ての余りが0か
all(myVec2 %% 2 != 0) # myVec2を2で割った場合、全ての余りが0ではないか
```

　それでは実際に`df`に対して`any()`と`all()`関数を使ってみましょう。一つ目は「ある都府県に最寄りの駅から徒歩60分以上の店舗が**一つでも**あるか」であり、二つ目は「ある都府県の店舗は**全て**最寄りの駅から徒歩30分以下か」です。それぞれの結果を`Over60`と`Within30`という列で出力してみましょう。

```{r}
df %>%
  group_by(Pref) %>%
  summarise(Over60   = any(Walk >= 60, na.rm = TRUE),
            Within30 = all(Walk <= 30, na.rm = TRUE),
            .groups  = "drop")
```

　埼玉県と神奈川県において、最寄りの駅から徒歩60以上の店がありました。また、京都府、東京都、奈良県、和歌山県の場合、全店舗が最寄りの駅から徒歩30分以下ということが分かります。当たり前ですが`Over60`が`TRUE`なら`Within30`は必ず`FALSE`になりますね。

## グルーピング

### `group_by()`によるグループ化

　先ほどの`summarise()`関数は確かに便利ですが、特段に便利とも言いにくいです。`df`の`Score`の平均値を計算するだけなら、`summarise()`関数を使わない方が楽です。

```{r}
# これまでのやり方
df %>%
  summarise(Mean = mean(Score, na.rm = TRUE))

# 普通にこれでええんちゃう?
mean(df$Score, na.rm = TRUE)
```

　しかし、これをグループごとに計算するならどうでしょう。たとえば、`Score`の平均値を都府県ごとに計算するとします。この場合、以下のようなコードになります。

```{r}
mean(df$Score[df$Pref == "東京都"],   na.rm = TRUE)
mean(df$Score[df$Pref == "神奈川県"], na.rm = TRUE)
mean(df$Score[df$Pref == "千葉県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "埼玉県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "大阪府"],   na.rm = TRUE)
mean(df$Score[df$Pref == "京都府"],   na.rm = TRUE)
mean(df$Score[df$Pref == "兵庫県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "奈良県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "和歌山県"], na.rm = TRUE)
```

　変わったのは`df$Score`が`df$Score[df$Pref == "東京都"]`に変わっただけです。`df$Pref`が`"東京都"`であるか否かを`TRUE`と`FALSE`で判定し、これを基準に`df$Score`を抽出する仕組みです。`df$Score`と`df$Pref`は同じデータフレーム (または、tibble)ですから、このような書き方で問題ありません。

　これだけでもかなり書くのが面倒ですが、これが47都道府県なら、あるいは200ヶ国ならかなり骨の折れる作業でしょう。ここで大活躍するのが{dplyr}パッケージの`group_by()`関数です。引数はグループ化する変数名だけです。先ほどの作業を{dplyr}を使うなら`Pref`変数でグループ化し、`summarise()`関数で平均値を求めるだけです。今回は`Score`だけでなく、`ScoreN`の平均値も求めてみましょう。そして、評価が高い順にソートもしてみます。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df %>%
  group_by(Pref) %>%
  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE)) %>%
  arrange(desc(Score_Mean))
```

　評判が最も高い都府県は和歌山県、最も低いのは神奈川県ですね。Songも和歌山ラーメンは井出系も車庫前系も好きです。しかし、大事なのは「井出系」と「車庫前系」といった分類が正しいかどうかではありません。コードが非常に簡潔となり、ソートなども自由自在であることです。都府県ごとに`ScoreN`と`Score`の平均値を求める場合、`dplyr()`を使わなかったら18行のコードとなり、ソートも自分でやる必要があります。一方、`group_by()`関数を使うことによってコードが5行になりました。

　また、これは2020年6月に公開された{dplyr}1.0.0からの問題ですが、`group_by()`の後に`summarise()`を使うと以下のようなメッセージが出力されます。

```
## `summarise()` ungrouping output (override with `.groups` argument)
```

　これは`group_by()`で指定された変数のグループ化が自動的に解除されたことを意味します。なぜなら`summarise()`をする際は`Pref`をグループ変数として使いましたが、出力された結果の`Pref`変数はもはやグループとして機能できなくなるからです。元の`df`には`Pref`が`"東京都"`だったケースが1000行、`"京都府"`だったのが414行あったので、`Pref`変数でグループ化する意味がありました。しかし、`summarise()`から得られたデータフレーム (または、tibble)は`Pref == "東京都"`の行が1つしかありません。これはグループ化する意味がなくなったことを意味します。したがって、自動的にグループを解除してくれます。自動的にやってくれるのはありがたいことですが、可能ならば関数内に自分で明記することが推奨されます。そこで使う引数が`.groups`であり、`"drop"`を指定すると**全ての**グループ化変数を解除します。以下のようなコードだと先ほどのメッセージが表示されません。今後、意識的に入れるようにしましょう。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df %>%
  group_by(Pref) %>%
  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") %>%
  arrange(desc(Score_Mean))
```

　続いて、一つ便利な関数を紹介します。それはグループのサイズを計算する関数、`n()`です。この関数を`summarise()`内に使うと、各グループに属するケース数を出力します。先ほどのコードを修正し、各グループのサイズを`N`という名の列として追加してみましょう。そしてソートの順番は`N`を最優先とし、同じ場合は`Score_Mean`が高い方を上に出力させます。また、`ScoreN_Mean`の前に、口コミ数の合計も出してみましょう。

```{r}
# Prefごとに口コミ数の合計、口コミ数の平均値、評価の平均値、店舗数を求める
# 店舗数-評価の平均値順でソートする
df %>%
  group_by(Pref) %>%
  summarise(ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            N           = n(),
            .groups     = "drop") %>%
  arrange(desc(N), desc(Score_Mean))
```

　記述統計をグループごとに求めるのは普通にあり得るケースですし、実験データの場合はほぼ必須の作業でう。統制群と処置群間においてグループサイズが均一か、共変量のバラツキが十分に小さいかなどを判断する際に`group_by()`と`summarise()`関数の組み合わせは非常に便利です。

### 複数の変数を用いたグループ化

　グループ化変数は2つ以上指定することも可能です。たとえば、都府県 (`Pref`)と最寄りの駅の路線 (`Line`)でグループ化することも可能です。それでは`Pref`と`Line`でグループ化し、店舗数と口コミ数、評価の平均値を計算し、ソートの順番は店舗数、店舗数が同じなら評価の平均値が高い順にしましょう。今回も`summarise()`内に`.group = "drop"`を指定し、グループ化を解除します。今回はTop 20まで出してみましょう。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df %>%
  filter(!is.na(Line)) %>% # Lineが欠損していないケースのみ残す
  group_by(Pref, Line) %>% # PrefとLineでグループ化
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") %>%
  arrange(desc(N), desc(Score_Mean)) %>%
  print(n = 20)
```

　ぐるなびに登録されているラーメン屋が最も多い路線は埼玉県内の東武東上線で122店舗があります。東武東上線は東京都と埼玉県をまたがる路線ですので、東武東上線だけならもっと多いかも知れませんね。

　ここで一つ考えたいのは`summarise()`内の`.groups`引数です。前回はグループ化に使った変数ごとに1行しか残っていなかったのでグループ化を全て解除しました。しかし、今回は状況がやや異なります。グループ化変数に使った`Pref`を考えると、まだ`Pref == "東京都"`であるケースがいくつかあります。やろうとすればまだグループ化出来る状態です。これは`Line`についても同じです。`Line == "東武東上線"`の行はここには表示されていないものの、まだデータに残っています。もし、これ以上グループ化しないなら今のように`.groups = "drop"`が正しいですが、もしもう一回グループ化したい場合はどうすればよいでしょうか。方法は2つ考えられます。

1. もう一度パイプ演算子を使って`group_by()`関数を使う (以下の9行目)。
    * 結果を見ると`## # Groups:   Pref, Line [523]`で、ちゃんとグループ化されていることが分かります。

```{r}
df %>%
  filter(!is.na(Line)) %>% 
  group_by(Pref, Line) %>% 
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") %>%
  arrange(desc(N), desc(Score_Mean)) %>%
  group_by(Pref, Line) %>% # group_by()、もう一度
  print(n = 5)
```

2. `.groups`引数を何とかする。

　推奨される方法は2番です。具体的には`.groups = "keep"`を指定するだけであり、こっちの方が無駄なコードを省けることができます。

```{r}
df %>%
  filter(!is.na(Line)) %>% 
  group_by(Pref, Line) %>% 
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "keep") %>%
  arrange(desc(N), desc(Score_Mean)) %>%
  print(n = 5)
```

　`.groups`引数は`"drop"`と`"keep"`以外にも`"drop_last"`があります。実は`summarise()`に`.groups`引数を指定したい場合のデフォルト値は`.groups == "drop_last"`または`"keep"`ですが、ここがややこしいです。主なケースにおいてデフォルト値は`"drop"`となりますとなります。`.groups == "drop_last"`これは最後のグループ化変数のみ解除する意味です。今回の例だと、2番目のグループ化変数である`Line`がグループ化変数から外され、`Pref`のみがグループ化変数として残る仕組みです。

　それではデフォルト値が`"keep"`になるのはいつでしょうか。それは記述統計量の結果が長さ2以上のベクトルである場合です。平均値を求める`mean()`、標準偏差を求める`sd()`などは、結果として長さ1のベクトルを返します。しかし、長さ2以上ののベクトルを返す関数もあります。たとえば、分位数を求める`quantile()`関数があります。`quantile(ベクトル名, 0.25)`の場合、第一四分位点のみ返すため、結果は長さ1のベクトルです。しかし、`quantile(ベクトル名, c(0.25, 0.5, 0.75))`のように第一四分位点から第三四分位点を同時に計算し、長さ3のベクトルが返されるケースもありますし、第二引数を省略すると、最小値・第一四分位点・第二四分位点・第三四分位点・最大値、つまり、長さ5のベクトルが返される場合があります。

```{r}
# 第一四分位点のみを求める (長さ1のベクトル)
quantile(df$Walk, 0.25, na.rm = TRUE)

# 引数を省略する (長さ5のベクトル)
quantile(df$Walk, na.rm = TRUE)
```

　`.groups`のデフォルト値が`"keep"`になるのは、このように長さ2以上のベクトルが返されるケースです。たとえば、都府県と最寄りの駅の路線でグループ化し、店舗までの徒歩距離の平均値を求めるとします。デフォルト値の変化を見るために、ここではあえて`.groups`引数を省略しました。

```{r}
df %>%
  filter(!is.na(Walk)) %>%
  group_by(Pref, Line) %>%
  summarise(Mean = mean(Walk))
```

　最初は`Pref`と`Line`でグループ化しましたが、`summarise()`の後、`Line`がグループ化変数から外されました。つまり、引数が`"drop_last"`になっていることです。

　それでは、平均値に加えて、第一四分位点と第三四分位点も計算し、`Quantile`という名で格納してみましょう。

```{r}
df %>%
  filter(!is.na(Walk)) %>%
  group_by(Pref, Line) %>%
  summarise(Mean     = mean(Walk),
            Quantile = quantile(Walk, c(0.25, 0.75)))
```

　同じ`Pref`、`Line`のケースが2つずつ出来ています。最初に来る数値は第一四分位点、次に来るのが第三四分位点です。そして最初のグループ化変数であった`Pref`と`Line`が、`summarise()`後もグループ化変数として残っていることが分かります。

　`.groups`引数は記述統計量だけを計算する意味ではあまり意識する必要がありません。しかし、得られた記述統計量から何らかの計算をしたり、さらにもう一回記述統計量を求めたりする際、予期せぬ結果が得られる可能性があるため注意する必要があります。出来る限り`.groups`引数は指定するようにしましょう。

## 変数の計算

### `mutate()`関数の使い方

　続いて、データフレーム (または、tibble)内の変数を用いて計算を行い、その結果を新しい列として格納する`mutate()`関数について紹介します。まず、`mutate()`関数の書き方からです。

```r
# mutate()関数の使い方
データフレーム/tibble名 %>%
  mutate(新しい変数名 = 処理内容)
```

　これは何らかの処理を行い、その結果を新しい変数としてデータフレーム (または、tibble)に追加することを意味します。新しく出来た変数は、基本的に最後の列になります。ここでは分単位である`Walk`を時間単位に変換した`Walk_Hour`変数を作成するとします。処理内容は`Walk / 60`です。最後に、都府県名、店舗名、徒歩距離 (分)、徒歩距離 (時間)のみを残し、遠い順にソートします。

```{r}
df %>%
  filter(!is.na(Walk)) %>%
  mutate(Walk_Hour = Walk / 60) %>%
  select(Pref, Name, Walk, Walk_Hour) %>%
  arrange(desc(Walk_Hour))
```

　`mutate()`は3行目に登場しますが、これは`Walk`を60に割った結果を`Walk_Hour`としてデータフレーム (または、tibble)の最後の列として格納することを意味します。もし、最後の列でなく、ある変数の前、または後にしたい場合は、`.before`または`.after`引数を追加します。これは`select()`関数の`.before`と`.after`と同じ使い方です。たとえば、新しく出来た`Walk_Hour`を`ID`と`Name`の間に入れたい場合は

```{r}
#| eval: false
# コードの3行名を修正 (.before使用)
mutate(Walk_Hour = Walk / 60,
       .before   = Name)

# コードの3行名を修正 (.after使用)
mutate(Walk_Hour = Walk / 60,
       .after    = ID)
```

のようにコードを修正します。

　むろん、変数間同士の計算も可能です。たとえば、以下のような`df2`があり、1店舗当たりの平均口コミ数を計算し、`ScoreN_Mean`という変数名で`ScoreN_Sum`の後に格納うするとします。この場合、`ScoreN_Sum`変数を`N`で割るだけです。

```{r}
df2 <- df %>%
  group_by(Pref) %>%
  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),
            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score, na.rm = TRUE),
            N           = n(),
            .groups     = "drop")
```

```{r}
df2 %>%
  mutate(ScoreN_Mean = ScoreN_Sum / N,
         .after      = ScoreN_Sum)
```

　このように、データ内の変数を用いた計算結果を新しい列として追加する場合は、`mutate()`が便利です。これを`mutate()`を使わずに処理する場合、以下のようなコードになりますが、可読性が相対的に低いことが分かります。

```{r}
#| eval: false
df2$ScoreN_Mean <- df2$ScoreN_Sum / df2$N
df2 <- df2[, c("Pref", "Budget_Mean", "Walk_Mean", 
               "ScoreN_Sum", "ScoreN_Mean", "Score_Mean", "N")]
```

　むろん、計算には`+`や`/`のような演算子だけでなく、関数を使うことも可能です。たとえば、`Budget`が1000円未満なら`"Cheap"`、1000円以上なら`"Expensive"`と示す変数`Budget2`を作成する場合は`ifelse()`関数が使えます。

```{r}
df %>% 
  mutate(Budget2 = ifelse(Budget < 1000, "Cheap", "Expensive")) %>%
  filter(!is.na(Budget2)) %>% # Budget2が欠損した店舗を除外
  group_by(Pref, Budget2) %>% # PrefとBudget2でグループ化
  summarise(N = n(),          # 店舗数を表示
            .groups = "drop")
```

　これは各都府県ごとの予算1000円未満の店と以上の店の店舗数をまとめた表となります。もし、500円未満なら`"Cheap"`、500円以上~1000円未満なら`"Reasonable"`、1000円以上なら`"Expensive"`になる`Budget3`変数を作るにはどうすればよいでしょうか。これは`ifelse()`を重ねることも出来ますが、ここでは`case_when()`関数が便利です。まずは、`ifelse()`を使ったコードは以下の通りです。

```{r}
#| eval: false
# ifelse()を使う場合
df %>% 
  mutate(Budget3 = ifelse(Budget < 500, "Cheap", 
                          ifelse(Budget >= 500 & Budget < 1000, "Reasonable",
                                 "Expensive"))) %>%
  filter(!is.na(Budget3)) %>%
  group_by(Pref, Budget3) %>%
  summarise(N = n(),         
            .groups = "drop")
```

　`case_when()`を使うと以下のような書き方になります。

```{r}
#| eval: false
# case_when()を使う場合
df %>% 
  mutate(Budget3 = case_when(Budget < 500                  ~ "Cheap",
                             Budget >= 500 & Budget < 1000 ~ "Reasonable",
                             Budget >= 1000                ~ "Expensive"),
         # 新しく出来た変数をfactor型にその場で変換することも可能
         Budget3 = factor(Budget3, 
                          levels = c("Cheap", "Reasonable", "Expensive"))) %>%
  filter(!is.na(Budget3)) %>%
  group_by(Pref, Budget3) %>%
  summarise(N = n(),         
            .groups = "drop")
```

　書く手間の観点では`case_when()`は`ifelse()`と大きく違いはないかも知れませんが、コードが非常に読みやすくなっています。`case_when()`関数の書き方は以下の通りです。

```r
# case_when()の使い方
データフレーム/tibble名 %>%
  mutate(新変数名 = case_when(条件1 ~ 条件1を満たす場合の結果値, 
                             条件2 ~ 条件2を満たす場合の結果値, 
                             条件3 ~ 条件3を満たす場合の結果値, 
                             ...))
```

　似たような機能をする関数として`recode()`関数があります。これは変数の値を単純に置換したい場合に便利な関数です。たとえば、都府県名をローマ字に変換するケースを考えてみましょう。

```{r}
# recode()を使う場合
df2 %>% 
  mutate(Pref2 = recode(Pref,
                        "東京都"   = "Tokyo",
                        "神奈川県" = "Kanagawa",
                        "千葉県"   = "Chiba",
                        "埼玉県"   = "Saitama",
                        "大阪府"   = "Osaka",
                        "京都府"   = "Kyoto",
                        "兵庫県"   = "Hyogo",
                        "奈良県"   = "Nara",
                        "和歌山県" = "Wakayama",
                        .default   = "NA"))
```

　使い方は非常に直感的です。

```r
# recode()の使い方
データフレーム/tibble名 %>%
  mutate(新変数名 = recode(元の変数名,
                            元の値1 =  新しい値1, 
                            元の値2 =  新しい値2, 
                            元の値3 =  新しい値3, 
                            ...,
                            .default = 該当しない場合の値))
```

　最後の`.default`引数は、もし該当する値がない場合に返す値を意味し、長さ1のベクトルを指定します。もし、指定しない場合は`NA`が表示されます。また、ここには紹介しておりませんでしたが、`.missing`引数もあり、これは欠損値の場合に返す値を意味します。

　もう一つ注意すべきところは、今回はcharacter型変数をcharacter型へ変換したため、「`"東京都" = "Tokyo"`」のような書き方をしました。しかし、numeric型からcharacter型に変換する場合は数字の部分を`` ` ``で囲む必要があります。たとえば、「`` `1` = "Tokyo"``」といった形式です。ただし、character型からnumeric型への場合は「`"東京都" = 1`」で構いません。

　`recode()`は値をまとめる際にも便利です。たとえば、`EastJapan`という変数を作成し、関東なら`1`を、それ以外なら`0`を付けるとします。そして、これは`Pref`変数の後に位置づけます。

```{r}
# 都府県を関東か否かでまとめる
df2 %>% 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            "大阪府"   = 0,
                            "京都府"   = 0,
                            "兵庫県"   = 0,
                            "奈良県"   = 0,
                            "和歌山県" = 0,
                            .default  = 0),
         .after = Pref)
```

　ただし、関東以外は全て0になるため、以下のように省略することも可能です。

```{r}
# .default引数を指定する場合
df3 <- df2 %>% 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            .default  = 0),
         .after = Pref)

df3
```

　新しく出来た`EastJapan`のデータ型はなんでしょうか。

```{r}
class(df3$EastJapan)
```

　`EastJapan`はnumeric型ですね。もし、これをfactor型にしたい場合はどうすればよいでしょうか。それは`mutate()`内で`EastJapan`を生成した後に`factor()`関数を使うだけです。

```{r}
# EastJapan変数をfactor型にする
df3 <- df2 %>% 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            .default  = 0),
         EastJapan = factor(EastJapan, levels = c(0, 1)),
         .after = Pref)

df3$EastJapan
```

　`EastJapan`がfactor型になりました。実は、`recode`は再コーディングと同時にfactor化をしてくれる機能があります。ただし、`recode()`関数でなく、`recode_factor()`関数を使います。

```{r}
# recode_factor()を使う方法
df3 <- df2 %>% 
  mutate(EastJapan = recode_factor(Pref,
                                   "東京都"   = 1,
                                   "神奈川県" = 1,
                                   "千葉県"   = 1,
                                   "埼玉県"   = 1,
                                   .default  = 0),
         .after = Pref)

df3$EastJapan
```

　ただし、levelの順番は`recode_factor()`内で定義された順番になることに注意してください。

## factor型の処理に便利な関数

### factor型の必要性

　話がずれますが、可視化における名目変数の扱いについて考えたいと思います。横軸、または縦軸が気温、成績、身長のような連続変数ではなく、都道府県や国、企業のような名目変数になる場合があります。たとえば、棒グラフの横軸は @fig-factor1 のように、一般的に名目変数になる場合が多いです。

```{r fig-factor1}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 3
#| fig-cap: "横軸が名目変数の棒グラフ"
df %>%
    group_by(Pref) %>%
    summarise(Score   = mean(Score, na.rm = TRUE),
              .groups = "drop") %>%
    arrange(Score) %>%
    ggplot() +
    geom_bar(aes(x = Pref, y = Score), stat = "identity") +
    coord_cartesian(ylim = c(1, 5)) +
    labs(x = "都府県", y = "口コミ評価の平均値 (1~5)") +
    theme_gray(base_family = "HiraKakuProN-W3") +
    theme(text = element_text(size = 12))
```

　ここでは横軸の順番に注目してください。京都府、埼玉県、神奈川県、...の順番になっていますね。「この順番で大満足だよ!」という方がいるかも知れませんが、そうでない方もおおいでしょう。普通考えられるものとしては、都道府県コードの順か、縦軸が高い順 (低い順)でしょう。都道府県コードの順だと、埼玉県、千葉県、東京都、神奈川県、京都府、大阪府、兵庫県、奈良県、和歌山県の順番になります。または、縦軸 (口コミ評価の平均値)が高い順なら和歌山県、奈良県、大阪府、...の順番になります。あるいは50音順も考えられるでしょう。アメリカの場合、州を並べる際、アルファベット順で並べます。

　自分でこの順番をコントロールするには可視化の前の段階、つまりデータハンドリングの段階で順番を決めなくてはなりません。これを決めておかない場合、Rが勝手に順番を指定します。具体的にはロケール (locale)というパソコン内の空間に文字情報が含まれているわけですが、そこに保存されている文字の順番となります。たとえば、日本語ロケールには「京」が「埼」よりも先に保存されているわけです。

　したがって、名目変数がグラフに含まれる場合は、名目変数の表示順番を決める必要があり、そこで必要なのがfactor型です。名目変数がcharacter型の場合、ロケールに保存されている順でソートされますが、factor型の場合、予め指定した順番でソートされます。

　たとえば、`df`を用いて、都道府県ごとの口コミ評価の平均値を計算し、その結果を`Score_df`として保存します。

```{r}
Score_df <- df %>%
    group_by(Pref) %>%
    summarise(Score   = mean(Score, na.rm = TRUE),
              .groups = "drop")

Score_df
```

　この時点で勝手にロケール順になります。実際、表示された`Score_df`を見ると`Pref`の下に``<chr>`と表記されており、`Pref`はcharacter型であることが分かります。これをこのまま棒グラフに出してみましょう。可視化の方法は執筆中のE-Bookで詳細に解説する予定ですので、ここでは結果だけに注目してください。

```{r fig-factor2}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 3
#| fig-cap: "Prefがcharacter型の場合 (1)"
Score_df %>%
  ggplot() +
  geom_bar(aes(x = Pref, y = Score), stat = "identity") +
  labs(x = "都府県", y = "口コミ評価の平均値 (1~5)") +
  theme_gray(base_family = "HiraKakuProN-W3") +
  theme(text = element_text(size = 12))
```

　横軸の順番があまり直感的ではありませんね。それでは、`Score_df`を`Score`が高い順にソートし、`Score_df2`で保存してから、もう一回試してみます。

```{r}
Score_df2 <- Score_df %>%
  arrange(desc(Score))

Score_df2
```

　ここでも`Pref`はcharacter型ですが、とりあえず、これで図を出してみます。

```{r fig-factor3}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 3
#| fig-cap: "Prefがcharacter型の場合 (2)"
Score_df2 %>%
  ggplot() +
  geom_bar(aes(x = Pref, y = Score), stat = "identity") +
  labs(x = "都府県", y = "口コミ評価の平均値 (1~5)") +
  theme_gray(base_family = "HiraKakuProN-W3") +
  theme(text = element_text(size = 12))
```

　結果は全く変わっておりません。それでは、`Score_df`の`Pref`列をfactor型に変換し、順番は口コミ評価の平均値が高い順番にしてみましょう。結果は`Score_df_f1`という名で保存します。

```{r}
Score_df_f1 <- Score_df %>%
  mutate(Pref = factor(Pref, levels = c("和歌山県", "奈良県", "大阪府",
                                        "千葉県", "京都府", "東京都",
                                        "埼玉県", "兵庫県", "神奈川県")))

Score_df_f1
```

　表示される順番は`Score_df`と`Score_df_f1`も同じですが、`Pref`のデータ型が`<fct>`、つまりfactor型であることが分かります。実際、`Pref`列だけ抽出した場合、factor型として、和歌山県から神奈川県の順になっていることが確認できます。

```{r}
Score_df_f1$Pref
```

　この`Score_df_f1`データを使って、 @fig-factor2 と全く同じコードを実行した結果が @fig-factor4 です。

```{r fig-factor4}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 3
#| fig-cap: "Prefがfactor型の場合"
Score_df_f1 %>%
  ggplot() +
  geom_bar(aes(x = Pref, y = Score), stat = "identity") +
  labs(x = "都府県", y = "口コミ評価の平均値 (1~5)") +
  theme_gray(base_family = "HiraKakuProN-W3") +
  theme(text = element_text(size = 12))
```

　これまでの話をまとめるの以下の2点が分かります。

1. 変数がcharacter型である場合、自動的にロケール順でソートされる。
2. 変数がfactor型である場合、データ内の順番やロケール順と関係なく、指定されたレベル (水準)の順でソートされる。

　特に2番目の点についてですが、これは必ずしも順序付きfactorである必要はありません。順序付きfactor型でなくても、`factor()`内で指定した順にソートされます。むろん、順序付きfactor型なら指定された順序でソートされます。

　これからはfactor型変換の際に便利な関数をいくつか紹介しますが、その前に数値として表現された名目変数について話します。たとえば、`Score_df_f1`に関東地域なら`1`を、その他の地域なら`0`を付けた`Kanto`という変数があるとします。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Kanto = ifelse(Pref %in% c("東京都", "神奈川県", "千葉県", "埼玉県"), 1, 0))

Score_df_f1
```

　`Kanto`変数のデータ型は、`<dbl>`、つまりnumeric型です。しかし、これは明らかに名目変数ですね。これをこのまま`Kanto`を横軸にした図を出すと @fig-factor5 のようになります。

```{r fig-factor5}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Kantoがnumeric型の場合"
Score_df_f1 %>%
  ggplot() +
  geom_bar(aes(x = Kanto, y = Score), stat = "identity") +
  labs(x = "関東地域", y = "口コミ評価の平均値 (1~5)") +
  theme_gray(base_family = "HiraKakuProN-W3") +
  theme(text = element_text(size = 12))
```

　この場合、図の横軸は`Kanto`の値が小さい順でソートされます。ただし、このような図は非常に見にくいため、`1`に`"関東"`、`0`に`"関西"`とラベルを付けたfactor型に変換した方が望ましいです。numeric型をラベル付きのfactor型にするためには、`levels`引数には元の数値を、`labels`引数にはそれぞれの数値に対応したラベルを指定します。また、関東の方を先に出したいので、`factor()`内の`levels`引数は`c(0, 1)`でなく、`c(1, 0)`にします。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Kanto = factor(Kanto, levels = c(1, 0), labels = c("関東", "その他")))

Score_df_f1
```

　`Kanto`変数がfactor型に変換されたことが分かります。

```{r}
Score_df_f1$Kanto
```

　また、`"関東"`、`"その他"`の順になっていますね。これを図として出力した結果が @fig-factor6 です。

```{r fig-factor6}
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Kantoがfactor型の場合"
Score_df_f1 %>%
  ggplot() +
  geom_bar(aes(x = Kanto, y = Score), stat = "identity") +
  labs(x = "地域", y = "口コミ評価の平均値 (1~5)") +
  theme_gray(base_family = "HiraKakuProN-W3") +
  theme(text = element_text(size = 12))
```

　このように数値型名目変数でも、factor化することによって、自由に横軸の順番を変えることができます。それでは、factor化に使える便利な関数をいくつか紹介します。

### {forcats}パッケージについて

　実はfactor型への変換や、順番に変更などは全てR内蔵の`factor()`関数で対応可能ですが、ここでは{forcats}パッケージが提供している`fct_*()`関数を使用します。{forcats}パッケージは{tidyverse}を読み込む際、自動的に読み込まれるため、既に{tidyverse}を読み込んでいる場合、別途のコードは要りません。

　実はfactor型への変換や、順番に変更などは全てR内蔵の`factor()`関数で対応可能ですが、ここでは{forcats}パッケージが提供している`fct_*()`関数を使用します。{forcats}パッケージは{tidyverse}を読み込む際、自動的に読み込まれるため、既に{tidyverse}を読み込んでいる場合、別途のコードは要りません。

**`fct_relevel()`: 水準の順番を変更する**

　`Score_df_f1`の`f1`は`Score`が高い順になっています。これを50音順に変更する際、`fct_relevel()`関数を使います。

```r
# 新しい変数名と元となる変数名が一致すると上書きになる
データフレーム名 %>%
  mutate(新しい変数名 = fct_releve(元となる変数名, 
                                    "水準1", "水準2", "水準3", ...))
```

　ここでは、`Pref`変数を再調整した`Pref2`変数を作ってみましょう。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Pref2 = fct_relevel(Pref, "大阪府", "神奈川県", "京都府", 
                             "埼玉県", "千葉県", "東京都", 
                             "奈良県", "兵庫県", "和歌山県"))

Score_df_f1
```

　一見、`Pref`と`Pref2`変数は同じように見えますが、水準はどうなっているでしょうか。

```{r}
levels(Score_df_f1$Pref)  # Prefの水準
levels(Score_df_f1$Pref2) # Pref2の水準
```

　問題なく50音順になっていることが分かります。他にも`fct_relevel()`には全ての水準名を指定する必要がありません。一部の水準名も可能です。たとえば、「関東が関西の先に来るなんでけしからん！」と思う読者もいるでしょう。この場合、関西の府県名を入れると、指定した水準が最初に位置するようになります。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Pref3 = fct_relevel(Pref, "京都府", "大阪府",
                             "兵庫県", "奈良県", "和歌山県"))

levels(Score_df_f1$Pref3) # Pref3の水準
```

　一部の水準名のみを指定するとその水準が最初に移動されますが、`after`引数を指定すると、位置を調整することも可能です。`after = 2`の場合、元となる変数の1、3番目の水準は維持され、3番目以降に指定した水準、それに続いて指定されていない水準の順番になります。`Pref`は和歌山、奈良、大阪の順ですが、ここで京都と東京を、奈良と大阪の間に移動するなら、

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Pref4 = fct_relevel(Pref, "京都府", "東京都", after = 2))

levels(Score_df_f1$Pref4) # Pref4の水準
```

のように書きます。`after`を指定しない場合のデフォルト値は0であるため、最初に移動します。

**`fct_recode()`: 水準のラベルを変更する**

　`fct_recode()`は水準のラベルを変更する時に使う関数で、以下のように使います。

```r
# 新しい変数名と元となる変数名が一致すると上書きになる
データフレーム名 %>%
  mutate(新しい変数名 = fct_recode(元となる変数名, 
                                   新しいラベル1 = "既存のラベル1",
                                   新しいラベル2 = "既存のラベル2",
                                   新しいラベル3 = "既存のラベル3",
                                   ...))
```

　注意点としては新しいラベルは`"`で囲まず、既存のラベルは`"`で囲む点です。それでは、`Pref`のラベルをローマ字に変更してみましょう。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Pref5 = fct_recode(Pref,
                            Saitama  = "埼玉県",
                            Wakayama = "和歌山県",
                            Kyoto    = "京都府",
                            Osaka    = "大阪府",
                            Tokyo    = "東京都",
                            Nara     = "奈良県",
                            Kanagawa = "神奈川県",
                            Hyogo    = "兵庫県",
                            Chiba    = "千葉県"))

Score_df_f1
```

　`fct_recode()`の中に指定する水準の順番は無視されます。つまり、水準の順番はそのまま維持されるため、好きな順番で結構です。また、全ての水準を指定せず、一部のみ変更することも可能です。それでは`Pref5`の順番が`Pref`の順番と同じかを確認してみましょう。

```{r}
levels(Score_df_f1$Pref)  # Prefの水準
levels(Score_df_f1$Pref5) # Pref5の水準
```

**`fct_rev()`: 水準の順番を反転する**

　水準の順番を反転することは非常によくあります。たとえば、グラフの読みやすさのために、左右または上下を反転するケースがあります。既に何回も強調しましたように、名目変数は基本的にfactor型にすべきであり、ここで`fct_rev()`関数が非常に便利です。たとえば、`Pref2`の水準は50音順でありますが、これを反転し、`Pref6`という名の列として追加してみましょう。

```{r}
Score_df_f1 <- Score_df_f1 %>%
  mutate(Pref6 = fct_rev(Pref2))

levels(Score_df_f1$Pref6)
```

　関数一つで水準の順番が反転されました。

**`fct_infreq()`: 頻度順に順番を変更する**

　続いて、水準の順番を頻度順に合わせる`fct_infreq()`関数です。たとえば、`Score`が欠損でないケースのみで構成された`df2`を考えてみましょう。

```{r}
df2 <- df %>%
  filter(!is.na(Score))
```

　そして、都府県ごとのケース数を計算します。

```{r}
table(df2$Pref)
```

　ここで`Pref`をfactor化しますが、水準の順番を店舗数が多い方を先にするにはどうすれば良いでしょうか。`fct_infreq()`関数は指定された変数の各値の個数を計算し、多い順にfactorの水準を調整します。

```{r}
df2 <- df2 %>%
  # 多く出現した値順でfactor化する
  mutate(Pref = fct_infreq(Pref))

levels(df2$Pref) # df2のPref変数の水準を出力
```

　`"東京都"`、`"神奈川県"`、`"大阪府"`、...の順で水準の順番が調整され、これは`table(df$Pref2)`の順位とも一致します。

**`fct_inorder()`: データ内の出現順番に順番を変更する**

　続いて、`fct_inorder()`ですが、これは意外と頻繁に使われる関数です。たとえば、自分でデータフレームなどを作成し、ケースの順番も綺麗に整えたとします。しかし、既に指摘した通り、データフレーム (または、tibble)での順番とグラフにおける順番は一致するとは限りません。データフレームに格納された順番でfactorの水準が設定できれば非常に便利でしょう。そこで使うのが`fct_inorder()`です。

　たとえば、`df`の`Pref`は`"東京都"`が1000個並び、続いて`"神奈川県"`が1000個、`"千葉県"`が1000個、...の順番で格納されています。この順番をそのままfactorの順番にするには以下のように書きます。

```{r}
df3 <- df %>%
  # Pref変数をfactor化し、水準は出現順とする
  # 変換後の結果はPrefに上書きする
  mutate(Pref = fct_inorder(Pref))

levels(df3$Pref)
```

**`fct_shift()`: 水準の順番をずらす**

　続いて、水準の順番をずらす`fct_shift()`関数を紹介します。たとえば、「1:そう思う」〜「5:そう思わない」、「9:答えたくない」の6水準で構成された変数があるとします。

```{r}
df4 <- tibble(
  ID = 1:10,
  Q1 = c(1, 5, 3, 2, 9, 2, 4, 9, 5, 1)
) 

df4 <- df4 %>%
  mutate(Q1 = factor(Q1, levels = c(1:5, 9),
                     labels = c("そう思う", 
                                "どちらかと言えばそう思う",
                                "どちらとも言えない",
                                "どちらかと言えばそう思わない",
                                "そう思わない",
                                "答えたくない")))

df4
```

　水準の順番も「そう思う」〜「答えたくない」順で綺麗に整っています。この水準を反転するには`fct_rev()`関数が便利です。`Q1`の水準を反転した変数を`Q1_R`という新しい列として追加し、水準を確認してみましょう。

```{r}
df4 <- df4 %>%
  mutate(Q1_R = fct_rev(Q1))

df4
levels(df4$Q1_R)
```

　「答えたくない」が最初の順番に来ましてね。できれば、「そう思わない」〜「そう思う」、「答えたくない」の順番にしたいところです。ここで使うのが`fct_shift()`ですが、書き方がややこしいので、噛み砕いて解説します。

```r
# fct_shift()の使い方
データ名 %>%
  mutate(新しい変数名 = fct_shift(元の変数名, n = 左方向へずらす個数))
```

　問題は`n =`引数ですが、その挙動については以下の表を参照してください。

```{r}
#| echo: false
#| message: false
tibble(Order = c("n = -2", "n = -1", 
                 "n = 0 (初期状態)", 
                 "n = 1", "n = 2"),
       F1 = c('"E"', '"F"', '"A"', '"B"', '"C"'),
       F2 = c('"F"', '"A"', '"B"', '"C"', '"D"'),
       F3 = c('"A"', '"B"', '"C"', '"D"', '"E"'),
       F4 = c('"B"', '"C"', '"D"', '"E"', '"F"'),
       F5 = c('"C"', '"D"', '"E"', '"F"', '"A"'),
       F6 = c('"D"', '"E"', '"F"', '"A"', '"B"')) %>%
  gt() %>%
    cols_label("Order" = "",
               "F1" = "1番目", "F2" = "2番目", "F3" = "3番目",
               "F4" = "4番目", "F5" = "5番目", "F6" = "6番目")
```

　具体的には水準は左方向へ`n`個移動します。元の水準が`A`, `B`, `C`, ..., `F`の順で、`n = 1`の場合、`A`が`F`の後ろへ移動し、`B`, `C`, `D`, `E`, `F`が前の方へ1つずつ移動します。逆に右側へ1つ移動したい場合は`n = -1`のように書きます。今回は最初の水準を最後に移動させたいので、`n = 1`と指定します。

```{r}
df4 <- df4 %>%
  # Q1_Rの水準を左方向で1ずらす
  mutate(Q1_R = fct_shift(Q1_R, n = 1))

levels(df4$Q1_R)
```

　これで水準の反転が完了しました。`fct_shift()`はこのように世論調査データの処理に便利ですが、他にも曜日の処理に使えます。例えば、1週間の始まりを月曜にするか日曜にするかによって、`fct_shift()`を使うケースがあります。

**`fct_shuffle()`: 水準の順番をランダム化する**

　あまり使わない機能ですが、水準の順番をランダム化することも可能です。使い方は非常に簡単で、`fct_shuffle()`に元の変数名を入れるだけです。たとえば、`Score_df`の`Pref`の順番をランダム化し、`Pref2`として追加します。同じことをもう2回繰り返し、それぞれ`Pref3`と`Pref4`という名前で追加してみましょう。

```{r}
Score_df <- Score_df %>%
  mutate(Pref2 = fct_shuffle(Pref),
         Pref3 = fct_shuffle(Pref),
         Pref4 = fct_shuffle(Pref))

Score_df

levels(Score_df$Pref2)
levels(Score_df$Pref3)
levels(Score_df$Pref4)
```

　`Pref`から`Pref4`まで同じように見えますが、水準の順番が異なります (`Pref`はcharacter型だから水準がありません)。

**`fct_reorder()`: 別の1変数の値を基準に水準の順番を変更する**

　`fct_infreq()`は出現頻度順に並び替える関数でしたが、それと似たような関数として`fct_reorder()`があります。ただし、これは出現頻度を基準にするのではなく、ある変数の平均値が低い順、中央値が高い順などでソートされます。まずは使い方から確認します。

```r
データ名 %>%
  mutate(新しい変数名 = fct_reorder(元の変数名, 基準となる変数, 
                                   関数名, 関数の引数))
```

　必要な引数が多いですね。解説よりも実際の例を見ながら説明します。今回も`Pref`をfactor変数にし、`Pref_R`という列で格納しますが、平均予算が安い順でfactorの水準を決めたいと思います。

```{r}
df <- df %>%
  mutate(Pref_R = fct_reorder(Pref, Budget, mean, na.rm = TRUE))

levels(df$Pref_R)
```

　`Pref_R`の水準は千葉県、埼玉県、奈良県、...の順ですが、本当にそうでしょうか。`group_by()`と`summarise()`などを使って確認してみましょう。

```{r}
df %>% 
  group_by(Pref) %>%
  summarise(Budget  = mean(Budget, na.rm = TRUE),
            .groups = "drop") %>%
  arrange(Budget)
```

　問題なくソートされましたね。注意点としては`fct_reorder()`内に関数名を書く際、`()`は不要という点です。関数名の次の引数としてはその関数に別途必要な引数を指定します。引数が省略可能、あるいは不要な関数を使う場合は、省略しても構いませんし、数に制限はありません。

　また、低い順ではなく、高い順にすることも可能です。次は`Score`の中央値が高い順に水準を設定した`Pref_R2`を作ってみましょう。

```{r}
df <- df %>%
  mutate(Pref_R2 = fct_reorder(Pref, Score, median, na.rm = TRUE, .desc = TRUE))

levels(df$Pref_R2)
```

　変わったのは`mean`の代わりに`median`を使ったこと、そして`.desc`引数が追加された点です。`fct_reorder()`には`.desc = FALSE`がデフォルトとして指定されており、省略した場合は昇順でfactorの水準が決まります。ここで`.desc = TRUE`を指定すると、降順となります。実際、`Score`の中央値順になっているかを確認してみましょう。

```{r}
df %>% 
  group_by(Pref) %>%
  summarise(Score   = median(Score, na.rm = TRUE),
            .groups = "drop") %>%
  arrange(desc(Score))
```

**`fct_reorder2()`: 別の2変数の値を基準に水準の順番を変更する**

　この関数は別の変数を基準に水準が調整される点では`fct_reorder()`と類似しています。ただし、よく誤解されるのは「変数Aの値が同じなら変数Bを基準に...」といったものではありません。たとえば、`fct_reorder(x, y, mean)`の場合、`y`の平均値 (`mean()`)の順で`x`の水準を調整するという意味です。この`mean()`関数に必要なデータはベクトル1つです。しかし、関数によっては2つの変数が必要な場合があります。

　これは頻繁に直面する問題ではありませんが、この`fct_reorder2()`関数が活躍するケースを紹介します。以下は6月27日から7月1日までの5日間、5地域におけるCOVID-19新規感染者数を表したデータです[^fct_reorder2]。入力が面倒な方は[ここ](../Data/COVID19.csv)からダウンロードして読み込んでください。

[^fct_reorder2]: データの出典は[Google](https://news.google.com/covid19/map)です。

```{r}
# 入力が面倒ならデータをダウンロードし、
# Reorder2_df <- read_csv("Data/COVID19.csv")
Reorder2_df <- tibble(
  Country = rep(c("日本", "韓国", "中国 (本土)", "台湾", "香港"),
                each = 5),
  Date    = rep(c("2020/06/27", "2020/06/28", "2020/06/29",
                  "2020/06/30", "2020/07/01"), 5),
  NewPat  = c(100, 93, 86, 117, 130, 
               62, 42, 43,  50,  54,
               17, 12, 19,   3,   5,
                0,  0,  0,   0,   0,
                1,  2,  4,   2,  28)
)

Reorder2_df <- Reorder2_df %>%
  mutate(Date = as.Date(Date))

Reorder2_df
```

　可視化のコードはとりあえず無視し、グラフを出力してみましょう。

```{r}
#| fig-height: 3.5
Reorder2_df %>%
  ggplot() +
  geom_line(aes(x = Date, y = NewPat, color = Country),
            size = 1) +
  scale_x_date(date_labels = "%Y年%m月%d日") +
  labs(x = "年月日", y = "新規感染者数 (人)", color = "") +
  theme_gray(base_family = "HiraKakuProN-W3")
```

　このグラフに違和感はあまりありませんが、「読みやすさ」の麺では改善の余地があります。たとえば、7月1日の時点で、新規感染者数が多いのは日本、韓国、香港、中国 (本土)、台湾の順です。しかし、右側の凡例の順番はそうではありません。この順番が一致すれば、更に図は読みやすくなるでしょう。

```{r}
factor(Reorder2_df$Country)
```

　実際、何も指定せずに`Reorder2_df`の`Country`をfactor化すると、韓国、香港、台湾、...の順であり、これは上のグラフと一致します。これをグラフにおける7月1日の新規感染者数の順で並べるためには、`Date`を昇順にソートし、そして最後の要素 (`"2020/07/01"`)内で新規感染者数 (`NewPat`)を降順に並べ替えた場合の順番にする必要があります。実際、`Reorder2_df`を`Date`で昇順、`NewPat`で降順にソートし、最後の5行を抽出した結果が以下のコードです。

```{r}
Reorder2_df %>%
  arrange(Date, desc(NewPat)) %>%
  slice_tail(n = 5)
```

　このように、水準を調整する際に2つの変数 (`Date`と`NewPat`)が使用されます。`fct_reorder2()`は`fct_reorder()`と買い方がほぼ同じですが、基準となる変数がもう一つ加わります。

```r
データ名 %>%
  mutate(新しい変数名 = fct_reorder2(元の変数名, 
                                    基準となる変数1, 基準となる変数2,
                                    関数名, 関数の引数))
```

　重要なのはここの関数のところですが、`fct_reorder2()`はデフォルトで`last2()`という関数が指定されており、まさに私たちに必要な関数です。したがって、ここでは関数名も省略できますが、ここでは一応明記しておきます。

```{r}
Reorder2_df <- Reorder2_df %>%
  mutate(Country2 = fct_reorder2(Country, Date, NewPat, last2)) 
```

　それでは新しく出来た`Country2`の水準を確認してみましょう。

```{r}
levels(Reorder2_df$Country2)
```

　ちゃんと7月1日の新規感染者数基準で水準の順番が調整されましたので、これを使ってグラフをもう一回作ってみます。

```{r}
#| fig-height: 3.5
Reorder2_df %>%
  ggplot() +
  geom_line(aes(x = Date, y = NewPat, color = Country2),
            size = 1) +
  scale_x_date(date_labels = "%Y年%m月%d日") +
  labs(x = "年月日", y = "新規感染者数 (人)", color = "") +
  theme_gray(base_family = "HiraKakuProN-W3")
```

　これで図がさらに読みやすくなりました。ちなみに、{forcats}パッケージは`last2()`以外にも`first2()`という関数も提供しております。これを使うと、7月1日でなく、6月27日の新規感染者数の降順で水準の順番が調整されます。他にも引数を2つ使用する自作関数も使えますが、`fct_reorder2()`の主な使いみちは`last2()`で十分でしょう。

**`fct_collapse()`: 水準を統合する**

　水準数をより水準数に減らすためには、`fct_recode()`を使います。先ほど、`fct_shift()`で使った`df4`の例を考えてみましょう。`df4`の`Q1`の水準数は6つです。

```{r}
levels(df4$Q1)
```

　これを4つに減らして見ましょう。具体的には「そう思う」と「どちらかと言えばそう思う」を「そう思う」に、「そう思わない」と「どちらかと言えばそう思わない」を「そう思わない」に統合します。これを`fct_recode()`で処理したのが以下のコードです。

```{r}
# fct_recode()を使った例
df4 <- df4 %>% 
    mutate(Q1_R2 = fct_recode(Q1,
                              そう思う          = "そう思う",
                              そう思う          = "どちらかと言えばそう思う",
                              どちらとも言えない  = "どちらとも言えない",
                              そう思わない       = "どちらかと言えばそう思わない",
                              そう思わない       = "そう思わない",
                              答えたくない       = "答えたくない"))

df4
levels(df4$Q1_R2)
```

　しかし、水準を統合するに特化した`fct_collapse()`を使えばより便利です。使い方は、`fct_recode()`に非常に似ているため省略しますが、`=`の右側を`c()`でまとめることが出来ます。

```{r}
# fct_collapse()を使った例
df4 <- df4 %>% 
    mutate(Q1_R3 = fct_collapse(Q1,
                                そう思う = c("そう思う", "どちらかと言えばそう思う"),
                                どちらとも言えない = "どちらとも言えない",
                                そう思わない = c( "どちらかと言えばそう思わない", "そう思わない"),
                                答えたくない = "答えたくない"))

df4
levels(df4$Q1_R3)
```

　`fct_recode()`の結果と同じ結果が得られました。元の水準数や、減らされる水準数などによっては書く手間があまり変わらないので、好きな方を使っても良いでしょう。

**`fct_drop()`: 使われていない水準を除去する**

　水準としては存在するものの、データとしては存在しないケースもあります。これをここでは「空水準 (empty levels)」と呼びます。たとえば、以下のコードは`Pref`をfactor化してから`Pref == "奈良県"`のケースを落としたものです。

```{r}
Score_df_f2 <- df %>%
  mutate(Pref = fct_inorder(Pref)) %>%
  filter(Pref != "奈良県") %>%
  group_by(Pref) %>%
  summarise(Score   = mean(Score, na.rm = TRUE),
            .groups = "drop")

Score_df_f2
```

　このように結果としては、奈良県のデータを除外したため空水準である奈良県は表示されませんが、`Pref`変数はどうでしょうか。

```{r}
levels(Score_df_f2$Pref)
```

　このように水準としては残っていることが分かります。使われていない水準が分析や可視化に影響を与えないケースもありますが、与えるケースもあります。これもこれまで勉強してきた`fct_*()`関数群で対応可能ですが、`fct_drop()`関数を使えば一発で終わります。実際にやってみましょう。

```{r}
Score_df_f2 <- Score_df_f2 %>%
  mutate(Pref = fct_drop(Pref))
```

```{r}
levels(Score_df_f2$Pref)
```

　水準から奈良県が消えました。同じ機能をする関数としてはR内蔵関数である`droplevels()`関数があり、使い方は`fct_drop()`と同じです。

**`fct_expand()`: 水準を追加する**

　一方、空水準を追加することも可能です。`fct_expand()`関数には元の変数名に加え、追加する水準名を入れるだけです。たとえば、`df`の`Pref`の水準は関東と関西の9都府県名となっていますが、ここに`"滋賀県"`という水準を追加してみます。。

```{r}
df5 <- df %>%
  mutate(Pref = fct_expand(Pref, "滋賀県"))

levels(df5$Pref)
```

　`"滋賀県"`という新しい水準が出来ましたね。ただし、新しく追加された水準は最後の順番になりますので、修正が必要な場合は`fct_relevel()`などを使って適宜修正してください。

　新しく水準が追加されることによって、何かの変化はあるでしょうか。まずは都府県ごとに`Score`の平均値とケース数を計算してみましょう。

```{r}
df5 %>%
  group_by(Pref) %>%
  summarise(Score   = mean(Score, na.rm = TRUE),
            N       = n(),
            .groups = "drop")
```

　見た目は全く変わらず、滋賀県の行が新しく出来たわけでもありません。{dplyr}の`group_by()`の場合、空水準はグループ化の対象になりません。一方、多くのR内蔵関数はケースとして存在しなくても計算の対象となります。たとえば、ベクトル内のある値が何個格納されているか確認する`table()`関数の例を見てみましょう。

```{r}
table(df5$Pref)
```

　`"滋賀県"`という列があり、合致するケースが0と表示されます。`group_by()`でも空の水準まで含めて出力する引数`.drop`があります。デフォルトは`TRUE`ですが、これを`FALSE`に指定してみます。

```{r}
df5 %>%
  group_by(Pref, .drop = FALSE) %>%
  summarise(Score   = mean(Score, na.rm = TRUE),
            N       = n(),
            .groups = "drop")
```

　空水準も出力され、`Score`の平均値は計算不可 (`NaN`)、ケース数は0という結果が得られました。

**`fct_explicit_na()`: 欠損値に水準を与える**

　まずは、実習用データ`df6`を作ってみまます。`X1`はnumeric型変数ですが、これをfactor化します。最初から`tibble()`内でfactor化しておいても問題ありませんが、練習だと思ってください。

```{r}
df6 <- tibble(
  ID = 1:10,
  X1 = c(1, 3, 2, NA, 2, 2, 1, NA, 3, NA)
)

df6 <- df6 %>%
  mutate(X1 = factor(X1, 
                     levels = c(1, 2, 3),
                     labels = c("ラーメン", "うどん", "そば")))

df6
```

　それでは`X1`をグループ化変数とし、ケース数を計算してみましょう。

```{r}
df6 %>%
  group_by(X1) %>%
  summarise(N       = n(),
            .groups = "drop")
```

　`NA`もグループ化の対象となります。以下はこの欠損値も一つの水準として指定する方法について紹介します。欠損値を欠損値のままにするケースが多いですが、欠損値が何らかの意味を持つ場合、分析の対象になります。たとえば、多項ロジスティック回帰の応答変数として「分からない/答えたくない」を含めたり、[「分からない/答えたくない」を選択する要因を分析](https://ci.nii.ac.jp/naid/40021269699)したい場合は、欠損値に値を与える必要があります。なぜなら、一般的な分析において欠損値は分析対象から除外されるからです。

　まずは、これまで紹介した関数を使ったやり方から紹介します。

```{r}
df6 %>%
         # まず、X1をcharacter型に変換し、X2という列に保存
  mutate(X2 = as.character(X1),
         # X2がNAなら"欠損値"、それ以外なら元のX2の値に置換
         X2 = ifelse(is.na(X2), "欠損値", X2),
         # X2を再度factor化する
         X2 = factor(X2, 
                     levels = c("ラーメン", "うどん", "そば", "欠損値")))
```

　`X1`をcharacter型に戻す理由[^fct_explicit_na1]は、水準にない値が入るとfactor化が解除されるからです。factor型をcharacter型に戻さずに`df6$X1`の`NA`を`"欠損値"`に置換すると、以下のようになります。

[^fct_explicit_na1]: character型でなく、numeric型でも出来ます。

```{r}
# df6のX1がNAなら"欠損"、それ以外なら元のX1の値を返す
ifelse(is.na(df6$X1), "欠損値", df6$X1)
```

　`"ラーメン"`と`"うどん"`、`"そば"`がfactor化前の1, 2, 3に戻っただけでなく、`NA`が`"欠損値"`というcharacter型に置換されたため、全体がcharacter型に変換されました。このように欠損値に水準を与える作業は難しくはありませんが、面倒な作業です。そこで登場する関数が`fct_exlpicit_na()`関数です。使い方は、元の変数に加え、欠損値の水準名を指定する`na_level`です。

```{r}
df6 <- df6 %>%
  # na_levelのデフォルト値は"(Missing)"
  mutate(X2 = fct_explicit_na(X1, na_level = "欠損値"))

df6
```

　欠損値が一つの水準になったことが分かります。

```{r}
df6 %>%
  group_by(X2) %>%
  summarise(N       = n(),
            .groups = "drop")
```

　むろん、`group_by()`を使ってもちゃんと出力されます。

## 行単位の操作

　ここでは行単位の操作について考えたいと思います。`select()`の説明で使った`myDF1`を見てみましょう。

```{r}
myDF1
```

　ここで`X1`と`X2`と`X3`の平均値を計算し、`X_Mean`という名の変数にする場合、以下のような書き方が普通でしょう。

```{r}
myDF1 %>%
  mutate(X_Mean = mean(c(X1, X2, X3)))
```

　あらら、なんかおかしくありませんか。1行目の場合、`X1`と`X2`、`X3`それぞれ2、5、3であり、平均値は3.333であるはずなのに3.133になりました。これは2行目以降も同じです。なぜでしょうか。

　実は{dplyr}は行単位の計算が苦手です。実際、データフレーム (または、tibble)というのは既に説明したとおり、縦ベクトルを横に並べたものです。列をまたがる場合、データ型が異なる場合も多いため、そもそも使う場面も多くありません。したがって、以下のような書き方が必要でした。

```{r}
myDF1 %>%
  mutate(X_Mean = (X1 + X2 + X3) / 3)
```

　先ほどの`mean(c(X1, X2, X3))`は(`X1`列と`X2`列、`X3`列)の平均値です。`X1`は長さ1のベクトルではなく、その列全体を指すものです。つまり、`mean(c(X1, X2, X3))`は`mean(c(myD1F$X1, myDF1$X2, myDF1$X3))`と同じことになります。だから全て3.133という結果が得られました。ただし、後者はベクトル同士の加減乗除になるため問題ありません。実際`c(1, 2, 3) + c(3, 5, 0)`は同じ位置の要素同士の計算になります。

　ここで`mean()`関数を使う場合には全ての演算を、一行一行に分けて行う必要があります。ある一行のみに限定する場合、`mean(c(X1, X2, X3))`の`X1`などは長さ1のベクトルになるため、`(X1 + X2 + X3) / 3`と同じことになります。この「一行単位で処理を行う」ことを指定する関数が`rowwise()`関数です。これは行単位の作業を行う前に指定するだけです。

```{r}
myDF1 %>%
  rowwise() %>%
  mutate(X_Mean = mean(c(X1, X2, X3)))
```

　これで問題なく行単位の処理ができるようになりました。今回は変数が3つのみだったので、これで問題ありませんが、変数が多くなると`:`や`starts_with()`、`num_range()`などを使って変数を選択したくなります。この場合は計算する関数内に`c_across()`を入れます。ここでは`X1`列から`X3D`列までの平均値を求めてみましょう。

```{r}
myDF1 %>%
  rowwise() %>%
  mutate(X_Mean = mean(X1:X3D))
```

　実は`rowwise()`関数、2020年6月に公開された{dplyr} 1.0.0で注目されるようになった関数ですが、昔の{dplyr}にも`rowwise()`関数はありました。ただし、{purrr}パッケージや{tidyr}パッケージの`nest()`関数などにより使い道がなくなりましたが、なぜか華麗に復活しました。データ分析に使うデータは基本単位は列であるため、実際に`rowwise()`が使われる場面は今の段階では多くないでしょう。また、簡単な作業なら`X1 + X2`のような演算でも対応できます。それでも、覚えておけば便利な関数であることには間違いありません。

## データの結合

### 行の結合

　まずは、複数のデータフレームまたはtibbleを縦に結合する方法について解説します。イメージとしては @fig-merge-row のようなものです。

```{r fig-merge-row}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "行の結合"
knitr::include_graphics("Figs/Handling1/Merge1.png")
```

　行を結合する際には{dplyr}パッケージの`bind_rows()`関数を使います。この関数の使い方は以下の通りです。

```r
# 新しいデータ名ではなく、既にあるデータ名にすると上書きとなる
新しいデータ名 <-  bind_rows(データ1, データ2, ...)
```

　それでは早速実際に使ってみましょう。実習のために、4つのtibbleを作成します (tibbleでなくデータフレームでも問題ありません)。

```{r}
# tibble()の代わりにdata.frame()も可
rbind_df1 <- tibble(X1 = 1:3,
                    X2 = c("A", "B", "C"),
                    X3 = c(T, T, F)) # TRUEとFALSEはTはFと省略可能

rbind_df2 <- tibble(X1 = 4:6,
                    X2 = c("D", "E", "F"),
                    X3 = c(F, T, F))

rbind_df3 <- tibble(X1 = 7:9,
                    X3 = c(T, T, T),
                    X2 = c("G", "H", "I"))

rbind_df4 <- tibble(X1 = 10:12,
                    X2 = c("J", "K", "L"),
                    X5 = c("Song", "Yanai", "Hadley"))

rbind_df1 # rbind_df1を出力
rbind_df2 # rbind_df2を出力
rbind_df3 # rbind_df3を出力
rbind_df4 # rbind_df4を出力
```

　まずは、`rbind_df1`と`rbind_df2`を結合してみます。この2つのデータは同じ変数が同じ順番で並んでいますね。

```{r}
Binded_df1 <- bind_rows(rbind_df1, rbind_df2)
Binded_df1
```

　2つのデータが結合されたことが確認できます。それでは`rbind_df1`と`rbind_df2`、`rbind_df3`はどうでしょうか。確かに3つのデータは同じ変数を持ちますが、`rbind_df3`は変数の順番が`X1`、`X3`、`X2`になっています。このまま結合するとエラーが出るでしょうか。とりあえず、やってみます。

```{r}
Binded_df2 <- bind_rows(rbind_df1, rbind_df2, rbind_df3)
Binded_df2
```

　このように変数の順番が異なっても、先に指定したデータの変数順で問題なく結合できました。これまでの作業は{dplyr}パッケージの`bind_rows()`を使わずに、R内蔵関数の`rbind()`でも同じやり方でできます。`bind_rows()`の特徴は、変数名が一致しない場合、つまり今回の例だと`rbind_df4`が含まれる場合です。`rbind_df1`から`rbind_df3`までは順番が違っても`X1`、`X2`、`X3`変数で構成されていました。一方、`rbind_dr4`には`X3`がなく、新たに`X4`という変数があります。これを`rbind()`関数で結合するとエラーが出力されます。

```{r}
#| error: true
# rbind()を使う場合
rbind(rbind_df1, rbind_df2, rbind_df3, rbind_df4)
```

　一方、`bind_rows()`はどうでしょうか。

```{r}
Binded_df3 <- bind_rows(rbind_df1, rbind_df2, rbind_df3, rbind_df4)
Binded_df3
```

　`X1`から`X4`まで全ての列が生成され、元のデータにはなかった列に関しては`NA`で埋められています。

　ならば、`bind_rows()`の完全勝利かというと、そうとは限りません。自分で架空した複数のデータフレーム、またはtibbleを結合する際、「このデータは全て同じ変数を持っているはず」と事前に分かっているなら`rbind()`の方が効果的です。なぜなら、変数名が異なる場合、エラーが出力されるからです。`bind_rows()`を使うと、コーディングミスなどにより、列名の相違がある場合でも結合してくれてしまうので、分析の結果を歪ませる可能性があります。

### 列の結合

　実はデータ分析においてデータの結合といえば、列の結合が一般的です。これは @fig-merge-col のような操作を意味します。

```{r fig-merge-col}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "列の結合"
knitr::include_graphics("Figs/Handling1/Merge2.png")
```

　まずは、本章で作成した`df2`をもう一回作ってみます。

```{r}
df2 <- df %>%
  group_by(Pref) %>%
  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),
            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score, na.rm = TRUE),
            N           = n(),
            .groups     = "drop")

df2
```

　ラーメン屋の店舗ですが、たしかにデータには埼玉、東京、大阪などは1000店舗しか入っておりません。実はもっと多いですが、ぐるなびAPIの仕様上、最大1000店舗しか情報取得が出来ないからです。ここに実際の店舗数が入っている新しいデータセット、[Ramen2.csv](Data/Ramen2.csv)があります。これを読み込み、`df3`という名で格納しましょう。

```{r}
#| message: false
# データのパスは適宜修正すること
df3 <- read_csv("Data/Ramen2.csv")

df3
```

```{r}
#| echo: false
#| message: false
tibble(Var  = c("Pref", "Pop", "Area", "RamenN", "Turnout", 
                "LDP", "CDP", "DPFP", "Komei", "JIP", 
                "JCP", "SDP", "Reiwa", "NHK", "HRP"),
       Desc = c("都道府県名",
                "日本人人口 (2015年国勢調査)",
                "面積 (2015年国勢調査)",
                "ぐるなびに登録されたラーメン屋の店舗数",
                "2019年参院選: 投票率 (比例)",
                "2019年参院選: 自民党の得票率 (比例)",
                "2019年参院選: 立憲民主党の得票率 (比例)",
                "2019年参院選: 国民民主党の得票率 (比例)",
                "2019年参院選: 公明党の得票率 (比例)",
                "2019年参院選: 日本維新の会の得票率 (比例)",
                "2019年参院選: 日本共産党の得票率 (比例)",
                "2019年参院選: 社会民主党の得票率 (比例)",
                "2019年参院選: れいわ新選組の得票率 (比例)",
                "2019年参院選: NHKから国民を守る党の得票率 (比例)",
                "2019年参院選: 幸福実現党の得票率 (比例)")) %>%
    gt() %>%
    cols_label("Var" = "変数名", "Desc" = "説明")
```

　本データは都道府県ごとの人口、面積、ぐるなびに登録されたラーメン屋の店舗数、2019年参議院議員通常選挙の結果が格納されています。人口と面積は2015年国勢調査、ぐるなびの情報は2020年6月時点での情報です。

　`df2`にデータ上の店舗数ではなく、実際の店舗数を新しい列として追加したい場合はどうすれば良いでしょうか。簡単な方法としては`df3`から情報を取得し、それを自分で入れる方法です。

```{r}
df3 %>%
  # df2のPrefベクトルの要素と一致するものに絞る
  filter(Pref %in% df2$Pref) %>%
  # 都道府県名とラーメン屋の店舗数のみ抽出
  select(Pref, RamenN)
```

　そして、この情報を`df2$RamenN <- c(415, 1106, 1254, ...)`のように追加すればいいですね。

　しかし、このような方法は非効率的です。そもそも`df3`から得られた結果の順番と`df2`の順番も一致しないので、一々対照しながらベクトルを作ることになります。ここで登場する関数が{dplyr}の`*_join()`関数群です。この関数群には4つの関数が含まれており、以下のような使い方になります。

```r
# 新しいデータ名ではなく、データ1またはデータ2の名前に格納すると上書きとなる

# 1. データ1を基準に結合
新しいデータ名 <-  left_join(データ1, データ2, by = "共通変数名")

# 2. データ2を基準に結合
新しいデータ名 <- right_join(データ1, データ2, by = "共通変数名")

# 3. データ1とデータ2両方に共通するケースのみ結合
新しいデータ名 <- inner_join(データ1, データ2, by = "共通変数名")

# 4. データ1とデータ2、どれかに存在するケースを結合
新しいデータ名 <-  full_join(データ1, データ2, by = "共通変数名")
```

　4つの関数の違いについて説明する前に、`by`引数について話したいと思います。これは主にキー (key)変数と呼ばれる変数で、それぞれのデータに同じ名前の変数がある必要があります。`df2`と`df3`だとそれが`Pref`変数です。どの`*_join()`関数でも、`Pref`の値が同じもの同士を結合することになります。

　データのキー変数名が異なる場合もあります。たとえば、データ1の都道府県名は`Pref`という列に、データ2の都道府県名は`Prefecture`という列になっている場合、`by = "Pref"`でなく、`by = c("データ1のキー変数名" = "データ2のキー変数名")`、つまり、`by = c("Pref" = "Prefecture")`と指定します。

　それでは、`df3`から都道府県名とラーメン屋の店舗数だけ抽出し、`df4`として格納しておきます。

```{r}
df4 <- df3 %>%
  select(Pref, RamenN)

df4
```

　これから共通変数名の値をキー (key)と呼びます。今回の例だと`Pref`が`df2`と`df4`のキー変数であり、その値である`"東京都"`、`"北海道"`などがキーです。

　まずは、`inner_join()`の仕組みについて考えます。これは`df2`と`df4`に共通するキーを持つケースのみ結合する関数です。`df4`には`"北海道"`というキーがありますが、`df2`にはありません。したがって、キーが`"北海道"`のケースは結合から除外されます。これをイメージにしたものが@fig-inner-joinです[^merge1]。それぞれ3 $\times$ 2 (3行2列)のデータですが、キーが一致するケースは2つしかないため、結合後のデータは3 $\times$ 2となります。

[^merge1]: これらの図は[Garrett Grolemund and Hadley Wickham. 2017. *R for Data Science: Import, Tidy, Transform, Visualize, and Model Data.* O'Reilly.](https://r4ds.had.co.nz)を参考にしました。

```{r fig-inner-join}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "`inner_join()`の仕組み"
knitr::include_graphics("Figs/Handling1/Merge_Inner.png")
```

　実際にやってみましょう。

```{r}
inner_join(df2, df4, by = "Pref")
```

　共通するキーは9つのみであり、結果として返されたデータの大きさも9 $\times$ 6です。`df2`に足された`df4`は2列のデータですが、キー変数である`Pref`は共通するため、1列のみ足されました。キー変数を両方残す場合は`keep = TRUE`引数を追加してください。

　一方、`full_join()`は、すべてのキーに対して結合を行います (@fig-full-join)。たとえば、`df2`には`"北海道"`というキーがありません。それでも新しく出来上がるデータには北海道の列が追加されます。ただし、道内店舗の平均予算、口コミ数などの情報はないため、欠損値が代入されます。

```{r fig-full-join}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "`full_join()`の仕組み"
knitr::include_graphics("Figs/Handling1/Merge_Full.png")
```

　それでは実際、結果を確認してみましょう。今回は結合後、`RamenN`が大きい順で出力します。

```{r}
full_join(df2, df4, by = "Pref") %>%
  arrange(desc(RamenN)) # ぐるなびに登録された店舗の多い都道府県から出力
```

　`df2`にはなかった北海道や愛知県などの行ができました。そして、`df2`にはない情報はすべて欠損値 (`NA`)となりました。

　続いて、`left_join()`ですが、これは先に指定したデータに存在するキーのみで結合を行います (@fig-left-join)。今回は`df2`が先に指定されていますが、`df2`のキーは`df4`のキーの部分集合であるため、`inner_join()`と同じ結果が得られます。

```{r fig-left-join}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "`left_join()`の仕組み"
knitr::include_graphics("Figs/Handling1/Merge_Left.png")
```

　一方、`right_join()`は`left_join()`と逆の関数であり、後に指定したデータに存在するキーを基準に結合を行います (@fig-right-join)。後に指定された`df4`のキーは`df2`のキーを完全に含むので、`full_join()`と同じ結果が得られます。

```{r fig-right-join}
#| echo: false
#| message: false
#| out-width: "100%"
#| fig-cap: "`right_join()`の仕組み"
knitr::include_graphics("Figs/Handling1/Merge_Right.png")
```

　これからは`df2`と`df4`を結合することになりますが、この2つのtibbleの大きさが異なります。`df2`は9つの都府県のみであるに対し、`df4`は47都道府県全てのデータが入っているからです。

　ここまではキー変数が一つである場合についてのみ考えましたが、複数のキー変数が必要な場合もあります。たとえば、市区町村の人口・面積データと市区町村の投票率データを結合するとします。各自治体に与えられている「[全国地方公共団体コード](https://www.soumu.go.jp/denshijiti/code.html)」が両データに含まれている場合は、このコードをキー変数として使えば問題ありませんが、市区町村名をキー変数として使わざる得ないケースもあるでしょう。しかし、キー変数が複数ある場合もあります。たとえば、府中市は東京都と広島県にありますし、太子町は大阪府と兵庫県にあります。この場合、市区町村名のみでケースをマッチングすると、重複されてマッチングされる恐れがあります。この場合はキー変数を増やすことで対処できます。たとえば、同じ都道府県なら同じ市区町村は存在しないでしょう[^merge2]。キー変数を複数指定する方法は簡単です。たとえば、市区町村名変数が`Munip`、都道府県名変数が`Pref`なら`by = c("Munip", "Pref")`と指定するだけです。

[^merge2]: 行政区も含むなら多くの政令指定都市にある「南区」とか「北区」が重複しますが、ここでは考えないことにしましょう。

　最後に、キー変数以外の変数名が重複する場合について考えましょう。これはパネルデータを結合する時によく直面する問題です。同じ回答者に2回の調査を行った場合、回答者のIDでデータを結合することになります。ただし、それぞれのデータにおいて回答者の性別に関する変数が`F1`という名前の場合、どうなるでしょうか。同じデータの同じ名前の変数が複数あると、非常に扱いにくくなります。実際の結果を見てみましょう。

```{r}
Wave1_df <- tibble(ID = c(1, 2, 3, 4, 5),
                   F1 = c(1, 1, 0, 0, 1),
                   F2 = c(18, 77, 37, 50, 41),
                   Q1 = c(1, 5, 2, 2, 3))

Wave2_df <- tibble(ID = c(1, 3, 4, 6, 7),
                   F1 = c(1, 0, 0, 0, 1),
                   F2 = c(18, 37, 50, 20, 62),
                   Q1 = c(1, 2, 2, 5, 4))

full_join(Wave1_df, Wave2_df, by = "ID")
```

　それぞれの変数名の後に`.x`と`.y`が付きます。この接尾辞 (suffix)は`suffix`引数を指定することで、分析側からカスタマイズ可能です。たとえば、接尾辞を`_W1`、`_W2`にしたい場合は

```{r}
full_join(Wave1_df, Wave2_df, by = "ID", suffix = c("_W1", "_W2"))
```

のように、データ1とデータ2それぞれの接尾辞を指定するだけです。
