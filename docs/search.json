[
  {
    "objectID": "research/appendix/cjoint-n-sim.html",
    "href": "research/appendix/cjoint-n-sim.html",
    "title": "Conjoint Analysis Sample Size Simulator",
    "section": "",
    "text": "# コードの読み込み\nsource(\"https://raw.githubusercontent.com/JaehyunSong/Simulation/master/Cjoint_N_Simulation.R\")\n\nCjoint.N.Sim(n.Attr, n.Level, n.Choice, n.Sim, n.Min, n.Max, \n             chk.dupli, min.occur)\n\n\n並列演算を行うため{doMC}、{foreach}パッケージを事前にインストールしておいてください。\n\nCPUがシングルコアの場合でも、一応インストールしておいて下さい。\n本シミュレーターは、CPUのコア全てを使います。(並列演算を行わない場合と比べて数倍早くなりました。)\n\n引数について\n\nn.Attr : 属性の数 (スカラー)\nn.Level : 各属性の水準の数 (ベクトル)\nn.Choice : プロフィールの数 (スカラー)\nn.Sim : 各サンプルサイズごとのシミュレーション回数 (スカラー)\nn.Min : 最小サンプルサイズ (スカラー)\nn.Max : 最大サンプルサイズ (スカラー)\nchk.dupli : 同じ水準が同時に出現したら欠損扱いする (論理型)\nmin.occur : 各属性が何回出現されるべきか (スカラー)\n\n\n\n\n\n\n\n\n時短のためには\n\n\n\n属性の数が結果に与える影響はほぼありません。したがって、n.Attr = 2とし、n.Level = c(2, 水準の最大数)と設定した方が早くシミュレーションできます。"
  },
  {
    "objectID": "research/appendix/cjoint-n-sim.html#実行例",
    "href": "research/appendix/cjoint-n-sim.html#実行例",
    "title": "Conjoint Analysis Sample Size Simulator",
    "section": "実行例",
    "text": "実行例\n\nsource(\"https://raw.githubusercontent.com/JaehyunSong/Simulation/master/Cjoint_N_Simulation.R\")\n\nCjoint.N.Sim(n.Attr = 2, n.Level = c(2, 4), n.Choice = 2, \n             n.Sim = 100, n.Min = 1300, n.Max = 1550, \n             chk.dupli = TRUE, min.occur = 500)\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "進行中のプロジェクト\n\n[Under Review] Comparing Public Support for Nuclear and Wind Energy in Washington State (w/ Azusa Uji, Aseem Prakash and Nives Dolšak)\n[Under Review] Decarbonization and Supply Chains: Public Support for the Thacker Pass Lithium Mine (w/ Azusa Uji, Aseem Prakash and Nives Dolšak)\n[Submitted] Public Support for Suspending Federal Gas Tax: A Survey Experiment (w/ Azusa Uji, Aseem Prakash and Nives Dolšak)\nRestructuring Issue Space: Unifying Proximity and Direction Models\n\nWorking Paper\n\nThe Economy as the Most Important Problem: How does economic issue salience affect the economic voting? (w/ Beomseob Brandon Park)\nGet Out the Voter or Go Away? Evidence from Online Field Experiments. (w/ Masaki Hata)\nPublic Perception toward Migrants: Impact of Multicultural Policies in Japan and South Korea. (w/ Yujin Woo)\n復活当選が投票率に与える影響\nPost-Election Regret under Alternative Electoral Rules: Evidence from Mexico’s 2021 Federal Election (w/ Yuriko Takahashi and Sohei Shigemura)\n韓国における感情的分極化がもたらす政治的帰結 (w/ 磯崎典世)\n香港における国家安全法に対する支持態度とリスト実験の安定性 (w/ 小林哲郎)\n\n\n\n著書\n\n宋財泫・矢内勇生. 2019.「え！そっちに入れるんですか？ー伸縮する政策空間における有権者の投票行動モデリングー」豊田秀樹 (編著) 『たのしいベイズモデリング２』北大路書房. pp. 105-117\n\nData\n\nHata, Masaki, Jaehyun Song, and Yutaka Shinada. 2017. “Has the 3.11 Disaster Brought About Conservatism in Japan?.” in Kaoru Endo, Satoshi Kurihara, Takashi Kamihigashi, and Fujio Toriumi Ed. Reconstruction of the Public Sphere in the Socially Mediated Age, Springer, pp. 181-200\n\nFull-text\n\n[韓国語] 중앙선거관리위원회 선거연수원 편찬. 2016『각국의 정당·정치자금제도 비교연구』중앙선거관리위원회 선거연수원. (각 장의 일본 파트 담당, 비매품) (= 中央選挙管理委員会選挙研修員編. 2016.『海外諸国の政党・政治資金制度の比較研究』中央選挙管理委員会選挙研修院．(各章の日本パート担当、非売品))\n\nFull-text\n\n[韓国語] 중앙선거관리위원회 선거연수원 편찬. 2016『2016 해외연구관 보고서』중앙선거관리위원회 선거연수원. (각 장의 일본 파트 담당, 비매품) (= 中央選挙管理委員会選挙研修員編. 2016.『2016海外研究官報告書』中央選挙管理委員会選挙研修院．(各章の日本パート担当、非売品))\n\nFull-text\n\n[韓国語] 중앙선거관리위원회 선거연수원 편찬. 2015『2015 해외연구관 보고서』중앙선거관리위원회 선거연수원. (각 장의 일본 파트 담당, 비매품) (= 中央選挙管理委員会選挙研修員編. 2015.『2015海外研究官報告書』中央選挙管理委員会選挙研修院．(各章の日本パート担当、非売品))\n\nFull-text\n\n\n\n\n論文\n\n2022\n\n[Accepted] Iida, Takeshi, Jaehyun Song, José Luis Estrada, and Yuriko Takahashi. 2022 (forthcoming). “Fake News and its Electoral Consequences: A Survey Experiment on Mexico,” AI & Society: Knowledge, Culture and Communication.\n[Accepted] Song, Jaehyun, Takeshi Iida, Yuriko Takahashi, and Jesús Tovar. 2022 (forthcoming). “Buying Votes across Borders? A List Experiment on Mexican Immigrants in the US,” Canadian Journal of Political Science.\n\nWorking Paper\n\n[Accepted] Uji, Azusa, Sijeong Lim, and Jaehyun Song. 2022 (forthcoming). “From Plastic to Peace: Overcoming Public Antipathy through Environmental Cooperation,” Journal of Peace Research.\n宋財泫. 2022. 「国家間の「助け合い」はいかに実現されるか 」『自助・共助・公助の政治学』71-93.\n\nFull-text\n\n\n\n\n2021\n\nUji, Azusa, Jaehyun Song, Nives Dolšak, and Aseem Prakash. 2021. “Public Support for Climate Adaptation Aid and Migrants: A Conjoint Experiment in Japan,” Environmental Research Letters, 16(12): 124073.\n\nFull-text\n\nKobayashi, Tetsuro, Jaehyun Song, and Polly Chan. 2021. “Does repression undermine opposition demands? The case of the Hong Kong National Security Law.” Japanese Journal of Political Science, 22(4): 268-286.\n\nFull-text Replication Data Featured in Mingpao (June 12, 2020)\n\nBeiser-McGrath, Liam F., Thomas Bernauer, Jaehyun Song, and Azusa Uji. 2021. “Understanding Public Support for Domestic Contributions to Global Collective Goods: Results from a survey experiment on carbon taxation in Japan.” Climatic Change, 166:51.\n\nFull-text\n\nSONG Jaehyun. 2021.「政治学における因果推論アプローチ」『文化情報学』15 (1・2): 36-43.\nUji, Azusa, Aseem Prakash and Jaehyun Song. 2021. “Does the”NIMBY syndrome” Undermine Public Support for Nuclear Power in Japan?” Energy Policy, 148 (A): 111944\n\nFull-text Featured in The Regulatory Review (June 29, 2021)\n\n\n\n\n2020\n\nShigemura, Sohei, Jaehyun Song, and Yuki Yanai. 2020.「Who Gets Close to Government Policies, and Who Steps Away?」『選挙研究』36 (2): 139-150.\n\nAppendix\n\nSONG Jaehyun・日野愛郎. 2020.「マルチレベル選挙における動員と投票疲れ:亥年現象の解明に向けて」『選挙研究』36 (1): 23-34.\n秦正樹・SONG Jaehyun. 2020.「争点を束ねれば「イデオロギー」になる？：サーベイ実験とテキスト分析の融合を通じて」『年報政治学』2020 (I): 58-81.\n\nFull-text Appendix\n\nSONG Jaehyun・秦正樹. 2020. 「オンライン・サーベイ実験の方法: 理論編」『理論と方法』35 (1): 92-108.\n\nFull-text\n\n秦正樹・SONG Jaehyun. 2020. 「オンライン・サーベイ実験の方法: 実践編」『理論と方法』35 (1): 109-127.\n\nFull-text\n\n\n\n\n2018\n\nShigemura, Sohei, Jaehyun Song, Keisuke Tani, Yuki Yanai. 2018. “Playing is Believing: Teaching How Electoral Systems Change Political Outcomes Using a Role-Playing Simulation Game,” Japanese Political Science Review, 4: 117-143.\n\nFull-text\n\n\n\n\n2017\n\n宋財泫. 2017.「イデオロギーが投票行動に与える影響の日韓比較研究」『ESTRELA』278: 10-17.\n\nAppendix\n\n\n\n\n2016\n\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (七・完)」『選挙時報』64(11): 31-39.\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (六)」『選挙時報』64(10): 40-50.\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (五)」『選挙時報』64(9): 27-37.\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (四)」『選挙時報』64(8): 20-31.\n宋財泫・善教将大. 2016. 「コンジョイント実験の方法論的検討」『法と政治』67(2): 67-108.\n\nFull-text Simulation\n\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (三)」『選挙時報』64(7): 28-37.\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (二)」『選挙時報』64(5): 24-32.\n宋財泫・品田裕. 2016.「「投票行動、政治意識に関するアンケート」についての報告 (一)」『選挙時報』64(4): 14-24.\n\n\n\n2015\n\n宋財[ヒョン]. 2015. 「誰が選挙公約を見るのか―無党派性と政治的有効性感覚に着目した日韓比較―」『六甲台論集―法学政治学編』62(1): 1-22.\n\nFull-text\n\n\n\n\n\n口頭発表\n(共同研究において下線報告者を表す)\n\n2022\n\nUji, Azusa, Jaehyun Song, Nives Dolšak, and Aseem Prakash. Jun. 30, 2022. “Supply Chain Dimensions of Decarbonization: A Survey Experiment to Assess Local Support for Thacker Pass Lithium Mining,” Environmental Politics and Governance Conference, University Park:PA, USA.\n宋財泫. 2022年3月17日.「原因と結果の政治学」関西大学法学会政治学研究会. 於：オンライン\n\n\n\n2021\n\n重村壮平・品田裕・宋財泫. 2021年11月27日.「選挙ポスターは投票参加を促すか：2021年うるま市市長選の事例から」第5回東アジア日本研究者協議会国際学術大会. 於：オンライン.\n\nSlide\n\n宋財泫. 2021年10月6日. 「国家間の「助け合い」はいかに実現されるか」関西大学経済・政治研究所第248回産業セミナー. 於: 吹田\nWoo, Yujin and Jaehyun Song. Oct. 3, 2021. “Public Perception of Migrants: Impact of Multicultural Policy in Japan and Korea,” American Political Science Association, Online.\nTakahashi, Yuriko, Takeshi Iida, Jaehyun Song, and Jose Luis Estrada. Oct. 1, 2021. “Fake News and Its Electoral Consequence: A Survey Experiment on Mexico,” American Political Science Association, Online.\nKobayashi, Tetsuro, Jaehyun Song, and Polly Chan. Sep. 30, 2021. “Does Repression Undermine Opposition Demands? The Case of the Hong Kong National Security Law,” American Political Science Association, Online.\nSong Jaehyun. 2021年9月26日「復活当選が投票率に与える影響」日本政治学会2021年度大会. 於：オンライン\n\nPaper Slide\n\nKobayashi, Tetsuro, Jaehyun Song, and Polly Chan. Jul. 2021. “Does Repression Undermine Opposition Demands? The Case of the Hong Kong National Security Law.” International Society of Political Psychology, Online.\nUji, Azusa, Aseem Prakash, Nives Dolšak, and Jaehyun Song. Jun. 2021. “Supporting Climate Adaptation Overseas by Providing Climate Aid or Accepting Climate Refugees: A Conjoint experiment in Japan.” Environmental Politics and Governance Conference, Online.\nAzusa, Uji, Lim, Sijeong, and Jaehyun Song. Jun. 2021. “Cooperation Spillover from Environment to High Politics? Paired Experiments in Japan and South Korea.” Environmental Politics and Governance Conference, Online.\n矢内勇生・SONG Jaehyun. 2021年5月8日「Integrating Research on Voting Behavior in Japan: A Meta-analysis」日本選挙学会2021年度大会. 於：金沢大学 (Online)\n\nPaper Slide\n\nUji, Azusa and Jaehyun Song. Apr. 2021. “Support for Climate Aid to and Accepting Climate Refugees from Asia and Africa? A survey experiment in Japan,” Western Political Science Association, Online.\n\n\n\n2020\n\nKobayashi, Tetsuro, Jaehyun Song, and Polly Chan. Nov. 2020. “Does Repression Undermine Opposition Demands? The Case of Hong Kong National Security Law.” V-Dem East Asia Regional Center Virtual Workshop: Workshop on Contentious Politics in Asia, Online.\n[Online due to COVID-19] Beiser-McGrath, Liam F., Thomas Bernauer, Jaehyun Song, and Azusa Uji. Sep. 2020. “The Effect of Reciprocity on Support for Enacting More Stringent Carbon Taxes,” American Political Science Association, San Francisco:CA, USA (Only first two authors are appeared on the program)\n[Online due to COVID-19] Uji, Azusa, Liam F. Beiser-McGrath, Thomas Bernauer, and Jaehyun Song. Jun. 2020. “How to Design a Politically Feasible and Effective Carbon Tax? Results from a Choice Experiment in Japan.” Environmental Politics and Governance Conference, Olso, Norway.\n[論文提出により報告成立] SONG Jaehyun・日野愛郎. 2020年5月「マルチレベル選挙における動員と投票疲れ：亥年現象の解明に向けて」日本選挙学会2020年度大会. 於：高知大学\n[ポスター提出により報告成立] 秦正樹・SONG Jaehyun. 2020年5月「「清き一票」は重すぎる?—フィールド実験を通じた啓発効果の検証—」日本選挙学会2020年度大会. 於：高知大学\n\nPoster\n\n[Cancelled due to COVID-19] Shigemura, Sohei, Jaehyun Song, Keisuke Tani and Yuki Yanai. “Who Gets Close to the Government’s Policy Position and Who Steps Away? Evidence from Survey Experiments in Japan,” Midwest Political Science Association, Chicago:IL, USA, Apr. 2020\n[Cancelled due to COVID-19] Shigemura, Sohei, Jaehyun Song, Keisuke Tani and Yuki Yanai. “Fed up with Elections? A Natural Experiment Using Byelections Caused by Legislators’ Deaths,” Midwest Political Science Association, Chicago:IL, USA, Apr. 2020\n[Cancelled due to COVID-19] Woo, Yujin and Jaehyun Song. Apr. 2020, “Public Perception toward Migrants: Impact of Multicultural Policiesin Japan and South Korea,” Midwest Political Science Association, Chicago:IL, USA.\nWoo, Yujin and Jaehyun Song. Jan. 11, 2020, “Public Perception toward Migrants: Impact of Multicultural Policies in Japan and South Korea,” Southern Political Science Association, San Juan:PR, USA.\n\n\n\n2019\n\n秦正樹・SONG Jaehyun. 2019年12月27日「Get Out the Voter or Go Away? Evidence from Online Field Experiments」公共システム論研究会. 於：石川四高記念文化交流館\n秦正樹・SONG Jaehyun. 2019年10月6日「争点を束ねれば「イデオロギー」になる？：サーベイ実験とテキスト分析の融合を通じて」日本政治学会2019年度大会. 於：成蹊大学.\nHakiai, Daisuke and Jaehyun Song. Sep. 20, 2019. “Multi-Level Electoral Systems and Pork-barrel politics: The Case of Japan’s LDP Politicians.” Workshop on “Multifaceted Values in Multilevel Contexts”, Waseda University, Tokyo:JAPAN.\nIida, Takeshi, Jaehyun Song, Yuriko Takahashi, and Jesus Tovar. Aug. 30, 2019. “Buying Votes across Borders? A Survey Experiment on Mexican Immigrants in the US,” American Political Science Association, DC:USA\n矢内勇生・SONG Jaehyun. 2019年7月13日「定量的選挙研究における結果の解釈をめぐる問題」日本選挙学会2019年度大会. 於：東北大学.\n\nPaper Slide\n\n宋財泫. 2019年4月23日「Review: VanderWeele, Tyler J., 2019, “Principles of Confounder Selection,” European Journal of Epidemiology, 34(3): 211-219.」社会科学方法論研究会.\n\nSlide\n\nHakiai, Daisuke and Jaehyun Song. Mar. 9, 2019. “Multi-Level Electoral Systems and Pork-barrel politics: The Case of Japan’s LDP Politicians.” Conference for “Values in European and Japanese politics”, Institute for European Studies, Free University of Brussels, Brussels, Belgium.\n\n\n\n2018\n\n宋財泫・重村壮平・品田裕. 2018年10月28日「選挙制度改革による利益誘導政治の変容と継続」第3回 東アジア日本研究者協議会国際学術大会. 於：国際日本文化研究センター\n\nSlide\n\nSong, Jaehyun and Beomseob Brandon Park. 2018年10月14日. 「最重要課題としての経済―経済争点のセイリアンスが経済投票に与える影響―」日本政治学会2018年度大会. 於: 関西大学.\nTsutsumi, Hidenori, Takayoshi Uekami, Kazunori Inamasu, Hiroko Ide, Jaehyun Song, and Yutaka Shinada. Jun. 13, 2018. “The Impact of Voting Advice Applications on Voters’ Behavior and Political Interest: A Field Experiment in the 2016 Upper House Election in Japan.” International Conference for e-Democracy and Open Government, Asia 2018, Yokohama: Kanagawa, Japan\n[優秀ポスター賞受賞] SONG Jaehuyn. 2018年5月13日「伸縮争点空間と争点投票モデルの統合」日本選挙学会 2018年度大会. 於：拓殖大学\n\nPaper Poster\n\n宋財泫. 2018年3月9日「Review: Cyrus, Samii, Laura Paler, and Sarah Zukerman Daly. 2016. “Retrospective Causal Inference with Machine Learning Ensembles: An Application to Anti-recidivism Policies in Colombia.” Political Analysis. 22 (4) pp. 434-456」社会科学方法論研究会. 於：大阪大学.\n\nSlide\n\n宋財泫. 2018年3月7日「争点空間の歪みと有権者の選択: 伸縮近接性モデルによる争点投票理論の統合」神戸大学政治学研究会. 於：神戸大学.\n\nSlide\n\n\n\n\n2017\n\nSONG Jaehyun・品田裕. 2017年5月20日「制度変化とその帰結のシミュレーション－異なる投票方法の下での議席配分－」日本選挙学会 2017年度大会. 於：香川大学\n\nPaper Slide Code\n\n重村壮平・SONG Jaehyun. 2017年5月21日「政治的テキストの文法 ー機械学習のための政治的テキストデータの構造ー」日本選挙学会 2017年度大会. 於：香川大学.\n\nPaper Poster\n\n\n\n\n2016\n\n宋財泫. 2016年10月23日「Review: Imai, Kosuke, Luke Keele, Dustin Tingley, and Teppei Yamamoto. (2011). “Unpacking the Black Box of Causality: Learning about Causal Mechanisms from Experimental and Observational Studies.” American Political Science Review, Vol. 105, No. 4, pp. 765-789.」社会科学方法論研究会. 於：大阪大学.\n善教将大・SONG Jaehyun. 2016年5月14日「都構想の何が支持されたのか：コンジョイント分析による政策選好の推定」日本選挙学会 2016年度大会. 於：日本大学.\n\nPaper\n\n宋財泫・秦正樹. 2016年2月13日「社会的期待迎合バイアス低減とリスト実験の応用：日韓慰安婦問題の合意に対する国内世論」関西計量社会学研究会 第50回定例研究会，於：関西学院大学大阪梅田キャンパス\nNishino, Shinsuke, Jaehyun Song, Keisuke Tani, and Yuki Yanai. 2016. “Teaching How Electoral Systems Change Political Outcomes Using a Role-Playing Simulation Game.” American Political Science Association Teaching and Learning Conference, Portland, OR: USA.\n\nPaper Slide Appendix\n\n\n\n\n2015\n\n宋財泫・善教将大. 2015年12月25日.「コンジョイント分析の方法論的検討」神戸大学政治学研究会． 於：神戸大学\n\nSlide Simulation\n\n宋財泫. 2015年8月1日.「書評：Bendor, Jonathan et al., A Behavioral Theory of Elections」関西政治経済学研究会(Y&R) 於：神戸大学.\n宋財泫. 2015年5月28日.「誰が選挙公報を見るのか―無党派性と政治的有効性感覚に着目した日韓比較―」神戸大学政治学研究会 於：神戸大学.\n\nSlide\n\n宋財泫. 2015年3月5日.「韓国は地域主義を超えられるか―選挙公約の役割の実証分析―」神戸大学STP報告会 於：神戸大学.\n\nSlide\n\n\n\n\n\n討論\n\nSONG Jaehyun. 2022年5月7日「討論：方法論部会（多様な政治参加の形態と先端的方法）」日本選挙学会2022年度大会. 於：石川県文教会館・金沢商工会議所会館\nSONG Jaehyun. 2021年6月27日「討論：自由論題E（比較政治学における量的方法）」日本比較政治学会2021年度大会. 於：オンライン\nSONG Jaehyun. 2019年10月6日「討論：方法論部会（政党の政策ポジション推定と選挙制度）」日本政治学会2019年度大会. 於：成蹊大学\n\n\n\n講演\n\n「Qualtrics入門講座」早稲田大学（2018年6月）\n「Qualtrics+Rによるコンジョイント分析の実施と分析」関西学院大学（2019年2月）\n「Qualtrics講習会」高知工科大学（2020年12月）Slide (学外用)\n\n\n\n報告書\n\n[韓国語] 송재현. 2016. 「일본」『공무원 정치적 중립 및 입후보제한에 관한 연구』중앙선거관리위원회 선거연수원. pp. 28-41 (= 宋財泫. 2016.「日本」『公務員の政治的中立性および立候補制限に関する研究』中央選挙管理委員会・選挙研修院．28-41頁)\n[韓国語] 송재현. 2016. 「일본의 선거권 연령 18세 인하에 따른 참의원 선거 영향 분석」『브렉시트의 의의와 영향, 일본의 선거권 연령 18세 인하에 따른 참의원 선거 영향 분석』중앙선거관리위원회 선거연수원. pp. 7-14 (= 宋財泫. 2016.「日本の18歳選挙権が参議院選挙に与えた影響の分析」『Brexitの意義の影響・日本の18歳選挙権が参議院選挙に与えた影響の分析』中央選挙管理委員会・選挙研修院．7-14頁)\n[韓国語] 송재현. 2016. 「일본」『외국의 선거방송토론 사례연구』중앙선거관리위원회 선거연수원. pp. 38-47 (= 宋財泫. 2016.「日本」『諸外国の選挙討論番組の事例研究』中央選挙管理委員会・選挙研修院．38-47頁)\n[韓国語] 송재현. 2016. 「일본」『외국의 주민소환제도』중앙선거관리위원회 선거연수원. pp. 59-74 (= 宋財泫. 2016.「日本」『諸外国のリコール制度』中央選挙管理委員会・選挙研修院．59-74頁)\n[韓国語] 송재현. 2016. 「일본」『지역구국회의원 선거구 획정 절차 및 비례대표국회의원 당선인 결정방법』중앙선거관리위원회 선거연수원. pp. 64-76 (= 宋財泫. 2016.「日本」『選挙区選出国会議員選挙における選挙区画定手続きおよび比例代表選出国会議員の当選者決定方法』中央選挙管理委員会・選挙研修院．64-76頁)\n[韓国語] 송재현. 2015. 「일본」『정당의 구성 및 활동 등에 대한 제도 및 운영실태』중앙선거관리위원회 선거연수원. pp. 106-119 (= 宋財泫. 2015.「日本」『政党の構成および活動に関する制度』中央選挙管理委員会・選挙研修院．106-119頁)\n[韓国語] 송재현. 2015. 「일본」『정책선거 추진실태 및 정책연구소 활동상황』중앙선거관리위원회 선거연수원. pp. 69-96 (= 宋財泫. 2015.「日本」『政策選挙の推進実態および政策研究所の活動状況』中央選挙管理委員会・選挙研修院．69-96頁)\n[韓国語] 송재현. 2015. 「일본」『선거인 명부 작성방법 (재외선거, 거주불명등록자)』중앙선거관리위원회 선거연수원. pp. 60-77 (= 宋財泫. 2015.「日本」『選挙人名簿作成方法(在外選挙、住所不定)』中央選挙管理委員会・選挙研修院．60-77頁)\n[韓国語] 송재현. 2015. 「일본」『장애인 유권자를 위한 선거정보 및 투표편의 제공 방안』중앙선거관리위원회 선거연수원. pp. 73-87 (= 宋財泫. 2015.「日本」『障害を持つ有権者のための選挙情報および投票便宜の提供方案』中央選挙管理委員会・選挙研修院．73-87頁)\n\n\n\nソフトウェア\n\n[R Packages] BalanceR GitHub\n[R Packages] SimpleConjoint GitHub\n[R Packages] woRdle GitHub\n[R Packages] PRcalc GitHub\n[Javascript] SimpleConjoint.js GitHub\n[Web Application] PRcalc for Web GitHub ShinyApps\n\n\n\n受賞\n\n優秀ポスター賞. 日本選挙学会2018年度大会. 日本選挙学会. 2019年.\n銅賞. 情報オリンピアード大会. ソウル東部教育庁. 2001年.\n銅賞. 情報オリンピアード大会. ソウル東部教育庁. 2000年.\n金賞. 情報オリンピアード大会. ソウル東部教育庁. 1998年.\n金賞. 情報オリンピアード大会. ソウル東部教育庁. 1997年.\n\n\n\n競争的資金獲得\n\n[分担] Apr.2022~Mar.2026: 科学研究費 基盤研究 (B)「ポピュリストの体制化というパラドックス：エリート概念の拡張とポピュリズムの新測定」代表：日野愛郎 KAKEN\n[分担] Apr.2021~Mar.2026: 科学研究費 基盤研究 (B)「不確実性の高い社会における地方政治への信頼の変動要因とその帰結の実証的解明」代表：善教将大 KAKEN\n[分担] Apr.2021~Mar.2025: 科学研究費 基盤研究 (B)「在米メキシコ移民の政治意識・政治参加：混合手法を用いた実証分析」代表：高橋百合子 KAKEN\n[単独] Apr.2019~Mar.2023: 科学研究費 若手研究「争点空間の認知における歪みがもたらす政治的帰結と歪みの是正に関する研究」 KAKEN\n[分担] Apr.2019~Mar.2023: 科学研究費 基盤研究 (A)「選挙ガバナンスが正確な投票(Correct Voting)に与える影響に関する研究」代表：大西裕 KAKEN\n[分担] Apr.2019~Mar.2022: 科学研究費 挑戦的研究 (萌芽)「日本における選挙研究の統合:メタ分析による研究成果の統一とデータベースの構築」代表：矢内勇生 KAKEN\n[分担] Apr.2019~Mar.2022: 科学研究費 挑戦的研究 (萌芽)「在外投票における票買収の実証的研究」代表：高橋百合子 KAKEN\n[単独] Dec.2018~Mar.2021: ヒロセ国際奨学財団「有権者の認知レベルにおける争点空間の歪みがもたらす政治的帰結に関する研究」\n[単独] Apr.2018~Mar.2019: 早稲田大学 特定課題 (新任の教員等)「有権者の認知レベルにおける争点空間の歪みがもたらす政治的帰結の研究」\n[単独] Apr.2017~Mar.2018: 科学研究費 特別研究員奨励費「政策中心型民主主義における選挙公約の役割:有権者の政治リテラシー向上に向けて」 KAKEN\n[単独] 2013: 神戸大学 学外研究活動経費「Inter-university Consortium for Political and Social Research Summer Programへの参加」"
  },
  {
    "objectID": "tutorial/R/rprogramming.html",
    "href": "tutorial/R/rprogramming.html",
    "title": "Rプログラミング入門の入門",
    "section": "",
    "text": "以下の内容は現在執筆中の内容の一部となります。\n\nSong Jaehyun・矢内勇生『私たちのR: ベストプラクティスの探求』(E-book)"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#rプログラミング入門の入門",
    "href": "tutorial/R/rprogramming.html#rプログラミング入門の入門",
    "title": "Rプログラミング入門の入門",
    "section": "Rプログラミング入門の入門",
    "text": "Rプログラミング入門の入門\nここでは統計ソフトウェアではなく、プログラミング言語としてのRについて解説します。プログラミングとは難しそうなイメージがありますが、実は難しいです (?!?!)。ただし、プログラミングにおける重要概念は「代入」、「条件分岐」、「反復」この3つだけです。実はこの3つだけでほとんどのプログラムは作れます。しかし、この単純さがプログラミングの難しさでもあります。\nたとえば、ある数字列を小さいものから大きい順へ並び替えることを考えてみましょう。c(6, 3, 7, 2, 5, 1, 8, 4)の場合、人間ならあまり苦労することなく、c(1, 2, 3, 4, 5, 6, 7, 8)に並び替えるでしょう。しかし、「代入」、「条件分岐」、「反復」のみでこれを具現化できるでしょうか1。もちろんですが、できます。たしかに、Rにはこのためのsort()関数やorder()関数などが用意されていますし、これを使えば良いのではないかと思うでしょう。しかし、これも「代入」、「条件分岐」、「反復」を組み合わせてR開発チームが予め作っておいた関数です。\n「代入」、「条件分岐」、「反復」といった3つの概念さえ理解すれば何でも出来るという意味でプログラミングは簡単です。しかし、この3つだけで解決しないといけないという点でプログラミングは難しいです。ただし、ほとんどのプログラミング言語は既に作られた関数 (bulit-in function)が多く用意されており、それを使うのが賢明です。それでも条件分岐や反復について勉強する必要があるのは、我々一般ユーザーにとってもこれが必要な場面が多いからです。たとえば、同じ分析をデータだけ変えながら複数回走らせるには反復が有効です。これを使えばコードを数十分の一に減らすことも可能です。また、学歴が大卒以上なら「高学歴」、未満なら「その他」にデータを再分類したい場合は条件分岐が非常に便利です。これがないと、一々自分がデータを眺めてExcelなどで入力しないといけませんが、パソコンを使うと秒レベルで終わります。"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#programming-intro",
    "href": "tutorial/R/rprogramming.html#programming-intro",
    "title": "Rプログラミング入門の入門",
    "section": "R言語の基礎概念",
    "text": "R言語の基礎概念\nこれまで「オブジェクト」や「関数」、「引数」などという概念を何の断りもなく使ってきましたが、ここではもうちょっと詳細に定義したいと思います。これらはプログラミングをやっていく上である程度は意識すべき点でもあります。\nオブジェクト (object)とはメモリに割り当てられた何かです。それは長さ1のベクトルを含むベクトル、行列、データフレーム、リストだけでなく、後ほど紹介する関数もオブジェクトに含まれます。一般的にオブジェクトとは何かの名前が付いています。たとえば、1から5までの公差1の等比数列なら、\n\nmyVec1 <- c(1, 2, 3, 4, 5) # myVec1 <- 1:5も同じ\n\nのようにmyVec1という名前でオブジェクトに割り当てます。一旦、名前を付けてオブジェクトとしてメモリに割り当てると、今後myVec1と入力するだけで中身の内容を読み込むことができます。それでは以下のように、myVec1の要素を2倍にする操作を考えてみましょう。\n\nmyVec1 * 2\n\n[1]  2  4  6  8 10\n\n\nここでmyVec1はオブジェクトです。それでは2はどうでしょう。メモリに割り当てられていないし、これはオブジェクトではないでしょうか。実は、この数字2もオブジェクトです。計算する瞬間のみにおいてメモリに割り当てられ、計算が終わったらメモリから消されたと考えた方が簡単でしょう。R内の全てのものはオブジェクトです (“Everything that exists in R is an object”)。実は先ほどの*のような演算子すらもオブジェクトです。\nクラス (class)とはオブジェクトを特徴づけるものです。既にこれまで何回もclass()関数を作ってデータ型やデータ構造を確認してきましたが、このclass()関数はオブジェクトのクラスを確認する関数です。先ほど、myVec1も*も2もオブジェクトであると説明しましたが、これがオブジェクトということは何らかのクラスを持っていることになります。また、class()関数そのものもオブジェクトであるため、何らかのクラスを持っています。\n\nclass(myVec1)\n\n[1] \"numeric\"\n\nclass(`*`)\n\n[1] \"function\"\n\nclass(2)\n\n[1] \"numeric\"\n\nclass(class)\n\n[1] \"function\"\n\n\n統計言語としてのRでクラスを意識することは多くありません。しかし、Rでパッケージを開発したり、複雑な関数を自作する場合、オブジェクト指向プログラミング (Object-oriented Programming; OOP)の考え方が重要になりますが、その際はオブジェクトのクラスを厳密に定義する必要があります2。\n関数 (function)は入力されたデータを、関数内部で決められた手順に沿って処理し、その結果を返すものです。関数は関数名(データ)のように使いますが、class(myVec1)はmyVec1というデータのクラスを返す関数です。また、sum(myVec1)はmyVec1の要素の総和を計算し、返す関数です。関数は自分で作成することも可能です。複雑な行動を繰り返す場合、その行動を関数内部で記述することで、一行でその行動を再現することが可能になります。この関数を使う際に必要なものが引数 (argument)というものです。sum()関数はこれだけだと何もできません。何らかのデータが与えられないと結果は返せません。たとえば、sum(myVec1)のようにです。ここでmyVec1がsum()関数の引数です。また、引数は複数持つことができます。たとえば、欠損値を含む以下のmyVec2を考えてみましょう。\n\nmyVec2 <- c(1, 2, 3, NA, 5)\n\nこれをsum()関数で総和を求めると、結果は欠損値となります。\n\nsum(myVec2)\n\n[1] NA\n\n\nこれはsum()の基本仕様が「欠損値が含まれているなら結果は欠損値にする」ことになっているからです。そこでsum()はもう一つの引数があり、それがna.rm引数です。na.rm = TRUEを指定すると、欠損値を除外した上で総和を返します。\n\nsum(myVec2, na.rm = TRUE)\n\n[1] 11\n\n\n引数を指定する場合はmyVec2のように名前を付けないケースもありますが、ほとんどの場合、na.rm = ...のように、どのような引数かを明示する必要があります。関数によっては数十個の引数を必要する関数もあります。それらの多くはデフォルト値を持っています。たとえば、sum()関数のna.rm引数のデフォルト値はFALSEです。しかし、自分で引数を指定したい場合、その引数がどの引数かを明確にするために、引数名は書いた方が望ましいです。むろん、引数が1つのみの関数なら、省略しても構いませんし、そもそも引数名が付いていない場合もあります。sum()関数の最初の引数はnumericまたはcomplex型のベクトルですが、これらの引数名はそもそもありません。\nある関数がどのような引数を要求しているか、そのデフォルト値は何か、引数として何か決められたデータ型/データ構造があるかを調べるためには?関数名と入力します。ここでは()がないことに注意してください。多くの関数の場合、非常に充実なヘルプが付いているため、関数の具体的な使い方を調べるには?関数名が有用です。"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#programming-style",
    "href": "tutorial/R/rprogramming.html#programming-style",
    "title": "Rプログラミング入門の入門",
    "section": "スクリプトの書き方",
    "text": "スクリプトの書き方\nRのコードの書き方には正解がありません。文法さえ合っていれば、何の問題もありません。しかし、コードを「書く」仕事以外にも「修正する」仕事も非常に重要です。そのためにはコードの書き手である人間に優しい書き方をした方が良いでしょう。書き方に正解はありませんが、「このように書いたら読みやすい」、「多くの人が採用している書き方」というのはあります。ここではこれについて簡単に説明します。\n\nオブジェクト名\nベクトルやデータフレームなどの変数や自作の関数など、全てのオブジェクトには何らかの名前が付きます。むろん、ラムダ式など、名前のない無名関数などもありますが、多くの場合は名前を持ちます。\n名前を付ける決まったルールはありませんが、大事な原則があります。\nオブジェクト名は英数字と限られた記号のみにする。数字で始まる変数名は避ける。\nむろん、ローマ字以外にも日本語やハングルの変数名も可能ですが、推奨されておりません。ローマ字以外は文字化けの可能性もありますし、コードを書く際、列がずれる原因ともなります。\n\nVariable1 <- c(2, 3, 5, 7, 11) # 推奨\n変数1 <- c(2, 3, 5, 7, 11)     # 非推奨\n변수1 <- c(2, 3, 5, 7, 11)     # 非推奨\n\n\nVariable1\n\n[1]  2  3  5  7 11\n\n変数1\n\n[1]  2  3  5  7 11\n\n변수1\n\n[1]  2  3  5  7 11\n\n\nただし、数字で始まるオブジェクト名は作成できません。\n\n100A <- \"R\"\n\nError: <text>:1:4: unexpected symbol\n1: 100A\n       ^\n\n\n他にも変数名に.と_が入ることは禁じられておらず、むしろ積極的に使われる場合が多いです。\n予約語を避ける\nまた、Rで既に提供している関数やオブジェクト名は避けるべきです。例えば、Rには円周率 (\\(\\pi\\))の数値がpiという名前で含まれています。\n\npi\n\n[1] 3.141593\n\n\npiという新しい変数を作るのは可能ですが、既存のpiが上書きされるため避けましょう。このようにプロブラミング言語が最初から提供しているオブジェクトの名前を「予約語」といいます。多くのプロブラミング言語は予約語を変数名として使うことを禁じており、Rも一部は禁止されています。たとえば、piは上書き可能ですが、ifやforというオブジェクト名は禁止されています。\n\npi  <- 3\n\n\nif  <- \"Yanai\"\n\nError: <text>:1:5: unexpected assignment\n1: if  <-\n        ^\n\n\n\nfor <- \"All\"\n\nError: <text>:1:5: unexpected assignment\n1: for <-\n        ^\n\n\n短さと分かりやすさを求める\nできれば、オブジェクト名を見るだけで、中身がどのようなものかが分かればベストでしょう。たとえば、学生5人の数学成績が格納されたベクトルを作るとします。\n\nVariable1 <- c(30, 91, 43, 77, 100)\n\nこれでも問題ありませんが、プロブラミングの世界において変数はVariable1と全て書くよりもVar1やV1のように略す場合が多いです。データ分析の世界だと、変数名をX1やX2にする場合も多いです。しかし、これだけだと中身の内容が想像できません。したがって、変数名は可能な限り中身の内容をよく表現した名前が望ましいです。\n\nmathscore <- c(30, 91, 43, 77, 100)\n\nこれだと、「あ、この変数には数学成績が入っているんだろうな」と容易に想像できます。ただ、ここで変数の可読性をより高めることも可能です。\n\nMathScore  <- c(30, 91, 43, 77, 100)\nmathScore  <- c(30, 91, 43, 77, 100)\nmath_score <- c(30, 91, 43, 77, 100)\nmath.score <- c(30, 91, 43, 77, 100)\n\n以上の例はmathとscoreの間に何らかの方法を使って区切りを入れた変数名であり、こちらの方が可読性が高いです。大文字と小文字の組み合わせで区切る方法は「キャメルケース (camel case)」と呼ばれ、大文字から始まるキャメルケースをアッパーキャメルケース (upper camel case)、小文字から始まるキャメルケースをローワーキャメルケース (lower camel case)と呼びます。また、_ (アンダーバー)で区切る方法は「スネークケース (snake case)」と呼ばれます。他にも- (ハイフン)で区切る「チェーンケース (chain case)」というのもありますが、Rの場合、オブジェクト名に-は禁止されております。\n最後に.を使う方法がありますが、Rでは使用可能です。しかし、この方法を推奨しない方も多いです。現在、データサイエンスにおけるTop2の言語はRとPythonですが、Pythonの場合、.をメソット呼び出しに使うため、オブジェクト名として使えないからです。RとPython二刀流の方には混乱の原因となりうるため、.を嫌う方もいます。しかし、Rのみ使用する方なら.を使っても問題ありません。\n\n\n改行\nコードは1行が長すぎないように適宜改行します。Rやパッケージなどが提供している関数の中には十数個以上の引数を必要とするケースもあります。この場合、コードを一行に全部書いてしまうと、コードの可読性が著しく落ちてしまします。\n一行のどれくらいの文字を入れるべきかについて決まったルールはありませんが、昔は一行80字という基準がよく使われてきました。これま昔のパソコンで使ったパンチカード ( 図 1 )が一行に80個の穴を開けることができたのから由来します。\n\n\n\n\n\n図 1: パンチカードの例\n\n\n\n\n今はモニターのサイズも大きくなり、解像度も高くなっているので、80文字にこだわる必要はありません。ただし、少なくとも自分のRStudioのSource Paneに収まるように、改行を適宜行ってください。\n\n\nスペースとインデント\n適切なスペースはコードの可読性を向上させます。以下のコードは全く同じものですが、後者をスペースを入れなかったため、可読性があまりよくありません。\n\n# 良い例\nsum(myVec2, na.rm = TRUE)\n\n# 悪い例\nsum(myVec2,na.rm=TRUE)\n\nどこにスペースを入れるかも決まっておりませんが、「,の後にスペース」、「演算子の前後にスペース」などの例があります。ただし、^の前後にスペースを入れません。また、後ほど紹介するfor(){}、while(){}、if(){}は()前後にスペースを入れます。ただし、function(){}は()の後のみにスペースを入れます。その他の関数の場合、関数名と()の間にスペースは入れません。\nまた、スペースを2回以上入れるケースもあります。たとえば、あるデータフレームを作るとします。\n\n# 良い例\ndata.frame(\n  Name     = c(\"Song\", \"Yanai\", \"Hadley\"),\n  Favorite = c(\"Ramen\", \"Cat\", \"R\"),\n  Gender   = c(\"Male\", \"Male\", \"Male\")\n)\n\n# 悪い例\ndata.frame(\n  Name = c(\"Song\", \"Yanai\", \"Hadley\"),\n  Favorite = c(\"Ramen\", \"Cat\", \"R\"),\n  Gender = c(\"Male\", \"Male\", \"Male\")\n)\n\nどのコードも同じですが、前者の方が読みやすいです。ここでもう一つ見てもらいのは「字下げ」です。先ほどのコードは以下のように一行にまとめることは可能ですが、あまりにも可読性がよくありません。\n\n# 邪悪な例\ndata.frame(Name = c(\"Song\", \"Yanai\", \"Hadley\"), Favorite = c(\"Ramen\", \"Cat\", \"R\"), Gender = c(\"Male\", \"Male\", \"Male\"))\n\nこのように一行のコードが長い場合、「改行」が重要です。ただし、改行した場合、字下げを入れましょう。改行された行は2文字または4文字分の字下げをします。こうすることで、「この行は上の行の続きです」ということは一目で分かります。\n\n# 良い例\ndata.frame(\n  Name     = c(\"Song\", \"Yanai\", \"Hadley\"),\n  Favorite = c(\"Ramen\", \"Cat\", \"R\"),\n  Gender   = c(\"Male\", \"Male\", \"Male\")\n)\n\n# 悪い例\ndata.frame(\nName     = c(\"Song\", \"Yanai\", \"Hadley\"),\nFavorite = c(\"Ramen\", \"Cat\", \"R\"),\nGender   = c(\"Male\", \"Male\", \"Male\")\n)\n\nRStudioは自動的に字下げをしてくれるので、あまり気にする必要がありませんが、メモ帳などでコードを書く際は意識してください。\n\n\n代入\nオブジェクトに値を代入する演算子として、これまでは<-を使ってきましたが、=も使えます。実際、多くのプログラミング言語の場合、代入演算子は=を採用しています。しかし、本書では<-の使用を推奨します。\n以上の内容は書き方の一部に過ぎません。本書のコードは出来る限り、多くの人に受け入れられているコードの書き方をするように心がけております。本書のコードを自分の手で書いていくうちに、徐々に書き方が身につくと思います。\n最後に、より詳細な書き方のガイドについては以下の2つの資料が非常に参考になります。とりわけ、Hadley先生が書いたThe tidyverse style guideは事実上 (de facto)の業界標準であり、Google’s Style GuideはHadley先生が書いたスタイルガイドに基づいています。かなりの分量ですが、自分でパッケージ開発などを考えている場合は必ず一回目を通しておきましょう。\n\nThe tidyverse style guide\nGoogle’s Style Guide"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#programming-iteration",
    "href": "tutorial/R/rprogramming.html#programming-iteration",
    "title": "Rプログラミング入門の入門",
    "section": "反復",
    "text": "反復\nパソコンが最も得意とすることが「反復作業」です。普通の人間なら数時間〜数年かかるような退屈な反復作業を、パソコンを一瞬で終わらせます。Rもプログラミング言語である以上、反復作業のための様々な関数を提供しております。\n反復作業を行う際は、「いつまで繰り返せば良いのか」を考えなくてはありません。そこで、2つのケースが考えられます。\n\n指定した回数だけ処理を繰り返す場合: for()文\n一定の条件が満たされるまで処理を繰り返す場合: while()文\n\nここではこの2つのケースそれぞれについて解説します。\n\nfor()による反復\nまずは、for()文を用いた反復処理のコードの雛形をみてみましょう。\n\nfor (任意の変数 in ベクトル) {\n    処理内容\n}\n\n任意の変数は何でもいいんですが、よく使うのはiです (indexの意味)。この変数はfor(){}文の内部で使うために用いられる変数です。そして、inの後の「ベクトル」は長さ1以上のベクトルです。必ずしもnumeric型である必要はありません。\nまた、{}内の内容が1行くらいで短い場合、{}は省略することもできます。したがって、以下のような書き方も可能です。これはfor()だけでなく、if()やfunction()など、{}で処理内容を囲む関数において共通です。この場合、処理内容を()の直後に書くのが一般的です。例えば、以下のように書きます。\n\nfor (任意の変数 in ベクトル) 処理内容\n\nただし、処理内容が2行以上の場合は、必ず{}で囲んでください。\nまずは、単にN回繰り返すfor()文を書いてみましょう。任意の変数名はiとします。この場合、for (i in 1:N)と書きます。5回繰り返すならfor (i in 1:5)となります。1:5はc(1, 2, 3, 4, 5)と同じですので、for (i in c(1, 2, 3, 4, 5))でも構いませんが、効率はよくありません。\nとりあえず、以下のようにコードを作成し、走らせてみましょう。これは3行にわたるコードですので、Rコンソールで打ち込むのは非効率的です。必ずソースコード欄に書いて、forの行にカーソルを置いた状態でCmd + Enter (Windowsの場合、Contrl + Enter)を入力しましょう。むろん、この3行を全て選択してからCmd + Enterを押しても構いません。\n\nfor (i in 1:5) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n# 上記のコードはこのように書くことも可能\n# for(i in 1:5) print(i)\n\n1から5までの数字が表示されました。これは一体、どのような動きをしているのでしょうか。これから詳しく解説します。\n\niにベクトル1:5の最初の要素を代入 (i = 1)\nprint(i)を実行\n{}中身の処理が終わったらiにベクトル1:5内の次の要素を代入 (i = 2)\nprint(i)を実行\n{}中身の処理が終わったらiにベクトル1:5内の次の要素を代入 (i = 3)\nprint(i)を実行\n{}中身の処理が終わったらiにベクトル1:5内の次の要素を代入 (i = 4)\nprint(i)を実行\n{}中身の処理が終わったらiにベクトル1:5内の次の要素を代入 (i = 5)\nprint(i)を実行\n{}中身の処理が終わったらiにベクトル1:5内の次の要素を代入するが、5が最後なので反復終了\n\n以上のような手順でfor()文は作動します。この手順を要約すると以下のようになります。\n\n任意の変数 (ここではi)にベクトル (ここでは1:5)の最初の要素が格納され、{}内の処理を行う。\n{}内の処理が終わったら、ベクトル (ここでは1:5)の次の要素を任意の変数 (ここではi)に格納し、{}内の処理を行う。\n格納できる要素がなくなったら反復を終了する。\n\nしたがって、反復はベクトル (ここでは1:5)の長さだけ実行される (ここでは5回)。\n\n\n反復はベクトルの長さだけ実行されるので、1:5のような書き方でなく、普通のベクトルでも問題ありません。たとえば、長さ6のIter_Vec1というベクトルを作り、その要素を出力するコードを書いてみましょう。\n\nIter_Vec1 <- c(24, 64, 31, 46, 81, 102)\n\nfor (damage in Iter_Vec1) {\n    x <- paste0(\"トンヌラに\", damage, \"のダメージ!!\")\n    print(x)\n}\n\n[1] \"トンヌラに24のダメージ!!\"\n[1] \"トンヌラに64のダメージ!!\"\n[1] \"トンヌラに31のダメージ!!\"\n[1] \"トンヌラに46のダメージ!!\"\n[1] \"トンヌラに81のダメージ!!\"\n[1] \"トンヌラに102のダメージ!!\"\n\n\nスライムを倒すには十分な攻撃力を持つ勇者ですね。ちなみに、paste0()は引数を空白なし3で繋ぎ、文字列として返す関数です。詳細割愛しますが、簡単な例だけお見せします。\n\npaste0(\"私は\", \"Rが\", \"使えません。\")\n\n[1] \"私はRが使えません。\"\n\npaste0(\"私の\", \"英語成績は\", 0, \"点です。\")\n\n[1] \"私の英語成績は0点です。\"\n\n\nむろん、文字列のベクトルを使うことも可能です。10個の都市名が格納されたIter_Vec2の要素を一個ずつ出力するコードは以下のようになります。\n\nIter_Vec2 <- c(\"Sapporo\", \"Sendai\", \"Tokyo\", \"Yokohama\", \"Nagoya\",\n               \"Kyoto\", \"Osaka\", \"Kobe\", \"Hiroshima\", \"Fukuoka\")\n\nfor (city in Iter_Vec2) {\n    x <- paste0(\"現在、cityの値は\", city, \"です。\")\n    print(x)\n}\n\n[1] \"現在、cityの値はSapporoです。\"\n[1] \"現在、cityの値はSendaiです。\"\n[1] \"現在、cityの値はTokyoです。\"\n[1] \"現在、cityの値はYokohamaです。\"\n[1] \"現在、cityの値はNagoyaです。\"\n[1] \"現在、cityの値はKyotoです。\"\n[1] \"現在、cityの値はOsakaです。\"\n[1] \"現在、cityの値はKobeです。\"\n[1] \"現在、cityの値はHiroshimaです。\"\n[1] \"現在、cityの値はFukuokaです。\"\n\n\n次は、for()の外にあるオブジェクトを参照するfor()文について説明します。たとえば、\"1番目の都市名はSapporoです\"、\"2番目の都市名はSendaiです\"、…のように出力するにはどうすれば良いでしょうか。今までは一箇所だけ変えながら表示しましたが、今回は「i番目」と「cityです」の二箇所が変わります。この場合は、まずiを基準に反復を行いながら、Iter_Vec2のi番目要素を呼び出すことで解決できます。以下のコードを見てみましょう。\n\nfor (i in 1:length(Iter_Vec2)) {\n    msg <- paste0(i, \"番目の都市名は\", Iter_Vec2[i], \"です。\")\n    print(msg)\n}\n\n[1] \"1番目の都市名はSapporoです。\"\n[1] \"2番目の都市名はSendaiです。\"\n[1] \"3番目の都市名はTokyoです。\"\n[1] \"4番目の都市名はYokohamaです。\"\n[1] \"5番目の都市名はNagoyaです。\"\n[1] \"6番目の都市名はKyotoです。\"\n[1] \"7番目の都市名はOsakaです。\"\n[1] \"8番目の都市名はKobeです。\"\n[1] \"9番目の都市名はHiroshimaです。\"\n[1] \"10番目の都市名はFukuokaです。\"\n\n\nここではfor (i in 1:10)ではなく、for (i in 1:length(Iter_Vec2))と表記しましたが、前者よりも後者の方が望ましい書き方です。なぜなら、もし事後的にIter_Vec2に都市名を2つ追加した場合、前者だと1:12と修正する必要がありますが、後者はlength()関数でIter_Vec2の長さを計算してくれるからです。\n多重for()文について\nfor()文の中に、更にfor()文を使用することも可能です。最初はやや難しいかも知れませんが、多重反復はプログラミングでもよく使われるので4、この機会に勉強しておきましょう。\nこの多重for()文の定番の例は(掛け算)九九です。1から9まで、全ての組み合わせの掛け算を計算するためには、2つのfor()文が必要です。i * jでiとjそれぞれに1から9を代入しながら結果を出力するコードは以下のように書きます。\n\nfor (i in 1:9) {\n  for (j in 1:9) {\n    print(paste(i, \"*\", j, \"=\", i * j))\n  }\n}\n\n[1] \"1 * 1 = 1\"\n[1] \"1 * 2 = 2\"\n[1] \"1 * 3 = 3\"\n[1] \"1 * 4 = 4\"\n[1] \"1 * 5 = 5\"\n[1] \"1 * 6 = 6\"\n[1] \"1 * 7 = 7\"\n[1] \"1 * 8 = 8\"\n[1] \"1 * 9 = 9\"\n[1] \"2 * 1 = 2\"\n[1] \"2 * 2 = 4\"\n[1] \"2 * 3 = 6\"\n[1] \"2 * 4 = 8\"\n[1] \"2 * 5 = 10\"\n[1] \"2 * 6 = 12\"\n[1] \"2 * 7 = 14\"\n[1] \"2 * 8 = 16\"\n[1] \"2 * 9 = 18\"\n[1] \"3 * 1 = 3\"\n[1] \"3 * 2 = 6\"\n[1] \"3 * 3 = 9\"\n[1] \"3 * 4 = 12\"\n[1] \"3 * 5 = 15\"\n[1] \"3 * 6 = 18\"\n[1] \"3 * 7 = 21\"\n[1] \"3 * 8 = 24\"\n[1] \"3 * 9 = 27\"\n[1] \"4 * 1 = 4\"\n[1] \"4 * 2 = 8\"\n[1] \"4 * 3 = 12\"\n[1] \"4 * 4 = 16\"\n[1] \"4 * 5 = 20\"\n[1] \"4 * 6 = 24\"\n[1] \"4 * 7 = 28\"\n[1] \"4 * 8 = 32\"\n[1] \"4 * 9 = 36\"\n[1] \"5 * 1 = 5\"\n[1] \"5 * 2 = 10\"\n[1] \"5 * 3 = 15\"\n[1] \"5 * 4 = 20\"\n[1] \"5 * 5 = 25\"\n[1] \"5 * 6 = 30\"\n[1] \"5 * 7 = 35\"\n[1] \"5 * 8 = 40\"\n[1] \"5 * 9 = 45\"\n[1] \"6 * 1 = 6\"\n[1] \"6 * 2 = 12\"\n[1] \"6 * 3 = 18\"\n[1] \"6 * 4 = 24\"\n[1] \"6 * 5 = 30\"\n[1] \"6 * 6 = 36\"\n[1] \"6 * 7 = 42\"\n[1] \"6 * 8 = 48\"\n[1] \"6 * 9 = 54\"\n[1] \"7 * 1 = 7\"\n[1] \"7 * 2 = 14\"\n[1] \"7 * 3 = 21\"\n[1] \"7 * 4 = 28\"\n[1] \"7 * 5 = 35\"\n[1] \"7 * 6 = 42\"\n[1] \"7 * 7 = 49\"\n[1] \"7 * 8 = 56\"\n[1] \"7 * 9 = 63\"\n[1] \"8 * 1 = 8\"\n[1] \"8 * 2 = 16\"\n[1] \"8 * 3 = 24\"\n[1] \"8 * 4 = 32\"\n[1] \"8 * 5 = 40\"\n[1] \"8 * 6 = 48\"\n[1] \"8 * 7 = 56\"\n[1] \"8 * 8 = 64\"\n[1] \"8 * 9 = 72\"\n[1] \"9 * 1 = 9\"\n[1] \"9 * 2 = 18\"\n[1] \"9 * 3 = 27\"\n[1] \"9 * 4 = 36\"\n[1] \"9 * 5 = 45\"\n[1] \"9 * 6 = 54\"\n[1] \"9 * 7 = 63\"\n[1] \"9 * 8 = 72\"\n[1] \"9 * 9 = 81\"\n\n\nまずは、iに1が代入されます。そして、jに1から9が一つずつ入れ替わるので、1 * 1、1 * 2、1 * 3、…、1 * 9が計算されます。1 * 9の計算が終わったらiに2が代入され、再び内側のfor()文が実行され、2 * 1、2 * 2、2 * 3、…、2 * 9が計算されます。これを9の段まで繰り返します。もし、9の段から1の段の順にしたい場合は、i in 1:9をi in 9:1に変えるだけです。\n\nfor (i in 9:1) {\n  for (j in 1:9) {\n    print(paste(i, \"*\", j, \"=\", i * j))\n  }\n}\n\n[1] \"9 * 1 = 9\"\n[1] \"9 * 2 = 18\"\n[1] \"9 * 3 = 27\"\n[1] \"9 * 4 = 36\"\n[1] \"9 * 5 = 45\"\n[1] \"9 * 6 = 54\"\n[1] \"9 * 7 = 63\"\n[1] \"9 * 8 = 72\"\n[1] \"9 * 9 = 81\"\n[1] \"8 * 1 = 8\"\n[1] \"8 * 2 = 16\"\n[1] \"8 * 3 = 24\"\n[1] \"8 * 4 = 32\"\n[1] \"8 * 5 = 40\"\n[1] \"8 * 6 = 48\"\n[1] \"8 * 7 = 56\"\n[1] \"8 * 8 = 64\"\n[1] \"8 * 9 = 72\"\n[1] \"7 * 1 = 7\"\n[1] \"7 * 2 = 14\"\n[1] \"7 * 3 = 21\"\n[1] \"7 * 4 = 28\"\n[1] \"7 * 5 = 35\"\n[1] \"7 * 6 = 42\"\n[1] \"7 * 7 = 49\"\n[1] \"7 * 8 = 56\"\n[1] \"7 * 9 = 63\"\n[1] \"6 * 1 = 6\"\n[1] \"6 * 2 = 12\"\n[1] \"6 * 3 = 18\"\n[1] \"6 * 4 = 24\"\n[1] \"6 * 5 = 30\"\n[1] \"6 * 6 = 36\"\n[1] \"6 * 7 = 42\"\n[1] \"6 * 8 = 48\"\n[1] \"6 * 9 = 54\"\n[1] \"5 * 1 = 5\"\n[1] \"5 * 2 = 10\"\n[1] \"5 * 3 = 15\"\n[1] \"5 * 4 = 20\"\n[1] \"5 * 5 = 25\"\n[1] \"5 * 6 = 30\"\n[1] \"5 * 7 = 35\"\n[1] \"5 * 8 = 40\"\n[1] \"5 * 9 = 45\"\n[1] \"4 * 1 = 4\"\n[1] \"4 * 2 = 8\"\n[1] \"4 * 3 = 12\"\n[1] \"4 * 4 = 16\"\n[1] \"4 * 5 = 20\"\n[1] \"4 * 6 = 24\"\n[1] \"4 * 7 = 28\"\n[1] \"4 * 8 = 32\"\n[1] \"4 * 9 = 36\"\n[1] \"3 * 1 = 3\"\n[1] \"3 * 2 = 6\"\n[1] \"3 * 3 = 9\"\n[1] \"3 * 4 = 12\"\n[1] \"3 * 5 = 15\"\n[1] \"3 * 6 = 18\"\n[1] \"3 * 7 = 21\"\n[1] \"3 * 8 = 24\"\n[1] \"3 * 9 = 27\"\n[1] \"2 * 1 = 2\"\n[1] \"2 * 2 = 4\"\n[1] \"2 * 3 = 6\"\n[1] \"2 * 4 = 8\"\n[1] \"2 * 5 = 10\"\n[1] \"2 * 6 = 12\"\n[1] \"2 * 7 = 14\"\n[1] \"2 * 8 = 16\"\n[1] \"2 * 9 = 18\"\n[1] \"1 * 1 = 1\"\n[1] \"1 * 2 = 2\"\n[1] \"1 * 3 = 3\"\n[1] \"1 * 4 = 4\"\n[1] \"1 * 5 = 5\"\n[1] \"1 * 6 = 6\"\n[1] \"1 * 7 = 7\"\n[1] \"1 * 8 = 8\"\n[1] \"1 * 9 = 9\"\n\n\n九九を覚える際はこれで良いでしょうが、i * jとj * iは実質的に同じですから、片方だけ出力させても問題はないでしょう。これはどうすれば良いでしょうか。この場合、for (j in 1:9)をfor (j in 1:i)に変えるだけです。\n\nfor (i in 1:9) {\n  for (j in 1:i) {\n    print(paste(i, \"*\", j, \"=\", i * j))\n  }\n}\n\n[1] \"1 * 1 = 1\"\n[1] \"2 * 1 = 2\"\n[1] \"2 * 2 = 4\"\n[1] \"3 * 1 = 3\"\n[1] \"3 * 2 = 6\"\n[1] \"3 * 3 = 9\"\n[1] \"4 * 1 = 4\"\n[1] \"4 * 2 = 8\"\n[1] \"4 * 3 = 12\"\n[1] \"4 * 4 = 16\"\n[1] \"5 * 1 = 5\"\n[1] \"5 * 2 = 10\"\n[1] \"5 * 3 = 15\"\n[1] \"5 * 4 = 20\"\n[1] \"5 * 5 = 25\"\n[1] \"6 * 1 = 6\"\n[1] \"6 * 2 = 12\"\n[1] \"6 * 3 = 18\"\n[1] \"6 * 4 = 24\"\n[1] \"6 * 5 = 30\"\n[1] \"6 * 6 = 36\"\n[1] \"7 * 1 = 7\"\n[1] \"7 * 2 = 14\"\n[1] \"7 * 3 = 21\"\n[1] \"7 * 4 = 28\"\n[1] \"7 * 5 = 35\"\n[1] \"7 * 6 = 42\"\n[1] \"7 * 7 = 49\"\n[1] \"8 * 1 = 8\"\n[1] \"8 * 2 = 16\"\n[1] \"8 * 3 = 24\"\n[1] \"8 * 4 = 32\"\n[1] \"8 * 5 = 40\"\n[1] \"8 * 6 = 48\"\n[1] \"8 * 7 = 56\"\n[1] \"8 * 8 = 64\"\n[1] \"9 * 1 = 9\"\n[1] \"9 * 2 = 18\"\n[1] \"9 * 3 = 27\"\n[1] \"9 * 4 = 36\"\n[1] \"9 * 5 = 45\"\n[1] \"9 * 6 = 54\"\n[1] \"9 * 7 = 63\"\n[1] \"9 * 8 = 72\"\n[1] \"9 * 9 = 81\"\n\n\nこれは1の段は1 * 1まで、2の段は2 * 2まで、3の段は3 * 3まで計算し、出力するコードです。2の段の場合、2 * 3や2 * 9などが出力されませんが、これは3の段において3 * 2が、9の段で9 * 2が出力されるので問題ないでしょう。このように多重for()文は複数のベクトルの組み合わせ全てにおいて処理を行う場合、有効です。\n複数のベクトルでなく、データフレームなどの2次元以上データにも多重for()文は使われます。データフレームは複数のベクトルで構成されているため、実質的には複数のベクトルを扱うことになります。今回は例として、FIFA_Men.csvを利用し、それぞれの国のチーム名、FAFAランキング、ポイントをまとめて表示するコードを書いてみます。全てのチームを出すと結果が長くなるので、OFC (オセアニアサッカー連盟)所属チームだけに絞ります。\n\n# FIFA_Men.csvを読み込み、myDFという名で保存\n# ファイルのパスは適宜修正してください\nmyDF <- read.csv(\"Data/FIFA_Men.csv\")\n# myDFのConfederation列がOFCの行だけを抽出\nmyDF <- myDF[myDF$Confederation == \"OFC\", ]\n\nmyDF\n\n     ID             Team Rank Points Prev_Points Confederation\n4     4   American Samoa  192    900         900           OFC\n69   69             Fiji  163    996         996           OFC\n135 135    New Caledonia  156   1035        1035           OFC\n136 136      New Zealand  122   1149        1149           OFC\n147 147 Papua New Guinea  165    991         991           OFC\n159 159            Samoa  194    894         894           OFC\n171 171  Solomon Islands  141   1073        1073           OFC\n185 185           Tahiti  161   1014        1014           OFC\n191 191            Tonga  203    862         862           OFC\n204 204          Vanuatu  163    996         996           OFC\n\n\nちょうど10チームとなります。作りたいのは以下のような内容が表示されるコードです。\n=====1番目の国家情報=====\nTeam: American Samoa\nRank: 192\nPoints: 900\n=====2番目の国家情報=====\nTeam: Fiji\nRank: 163\nPoints: 996\n=====3番目の国家情報=====\nTeam: New Caledonia\n...\n実はこれ、一つのfor()でも作成できますが、勉強のために2つのfor()文を使いましょう。\n\nfor (i in 1:nrow(myDF)) {\n  print(paste0(\"=====\", i, \"番目の国家情報=====\"))\n  \n  for (j in c(\"Team\", \"Rank\", \"Points\")) {\n    print(paste0(j, \": \", myDF[i, j]))\n  }\n}\n\n以上のコードを実行すると以下のような結果が表示されます。ここでは全部掲載するのは長いので、最初の2チーム情報のみ掲載します。\n\n\n[1] \"=====1番目の国家情報=====\"\n[1] \"Team: American Samoa\"\n[1] \"Rank: 192\"\n[1] \"Points: 900\"\n[1] \"=====2番目の国家情報=====\"\n[1] \"Team: Fiji\"\n[1] \"Rank: 163\"\n[1] \"Points: 996\"\n\n\nそれではコードの詳細について説明します。まず、外側のfor()文には任意の変数としてはiを、内側のfor()文に対してはjを用います。Rの場合、コードは上からの順番で処理します（同じ行なら、カッコの中から処理します）。したがって、まず処理されるのは外側のfor()文です。iに割り当てられたベクトルは1:nrow(myDF)です。nrow()は行列またはデータフレームの行数を求める関数で、今回の場合、myDFは10行のデータであるため、1:10になります。つまり、外側のfor()文はiに1, 2, 3…の順で値を格納しながら10回処理を繰り返すことになります。それでは外側for()文の中身を見ましょう。ここではまず、\"=====i番目の国家情報=====\"というメッセージを出力します。最初はiが1ですので、\"=====1番目の国家情報=====\"が表示されます。\n次の行からが内側のfor()文になります。ここではjにc(\"Team\", \"Rank\", \"Points\")を格納しながら内側for()文内のコードを3回繰り返します。内側のコード内容は例えば、i = 1の状態で、j = \"Team\"なら、print(paste0(\"Team\", \": \", myDF[1, \"Team\"]))になります。つまり、myDF[1, \"Team\"]はmyDFのTeam列の1番目の要素を意味します。この処理が終わったら、次はjに\"Rank\"が代入され、同じコードを処理します。そして、j = \"Points\"まで処理が終わったら、内側のfor()文の役目はとりあえず終わりです。\n内側のfor()文が終わっても、外側のfor()文はまだ終わっておりません。次はiに2が格納され、\"=====2番目の国家情報=====\"を表示し、また内側のfor()文を最初から処理します。この作業はi = 10の状態で内側のfor()文が終わる時点で、外側のfor()文も止まります。これが多重for()文の動き方です。\nもし、チーム名の次に、所属連盟も表示したい場合はどう直せば良いでしょうか。正解はc(\"Team\", \"Rank\", \"Points\")のベクトルにおいて、\"Team\"と\"Rank\"の間に\"Confederation\"を追加するだけです。実際にやってみましょう (結果は紙幅の関係上、最初の2チームのみ掲載します)。\n\nfor (i in 1:nrow(myDF)) {\n  print(paste0(\"=====\", i, \"番目の国家情報=====\"))\n  \n  for (j in c(\"Team\", \"Confederation\", \"Rank\", \"Points\")) {\n    print(paste0(j, \": \", myDF[i, j]))\n  }\n}\n\n\n\n[1] \"=====1番目の国家情報=====\"\n[1] \"Team: American Samoa\"\n[1] \"Confederation: OFC\"\n[1] \"Rank: 192\"\n[1] \"Points: 900\"\n[1] \"=====2番目の国家情報=====\"\n[1] \"Team: Fiji\"\n[1] \"Confederation: OFC\"\n[1] \"Rank: 163\"\n[1] \"Points: 996\"\n\n\nちなみに、同じ動きをする別の書き方は以下のとおりです。結果は掲載しませんが、興味のある方は試してみて下さい。cat()関数の使い方については?catを参照してください。ちなみに\\nは改行コードを意味します。\n\nfor (i in 1:nrow(myDF)) {\n  cat(paste0(\"=====\", i, \"番目の国家情報=====\\n\"))\n  cat(paste0(\"Team:\",   myDF$Team[i], \"\\n\",\n             \"Rank:\",   myDF$Rank[i], \"\\n\",\n             \"Points:\", myDF$Points[i], \"\\n\"))\n}\n\nもう一つの例を見たいと思います。今回はリスト型を対象とした多重反復の例です。複数のベクトルを含むリストの場合、3番目のベクトルの5番目の要素を抽出するにはリスト名[[3]][5]のように2つの位置を指定する必要があります。たとえば、以下の3人で構成された3つのクラスがあるとし、そのグループ構成人の名前が出席番号順で入っているmyListを考えましょう。\n\nmyList <- list(Class_A = c(\"Yanai\", \"Song\", \"Hadley\"),\n               Class_B = c(\"Tanaka\", \"Sato\", \"Suzuki\"),\n               Class_C = c(\"Abe\", \"Moon\", \"Xi\"))\n\nここで、まず各クラスの出席番号1番の人の名字3つを出力し、次は2番の人、最後に3番の人を出すにはどうすれば良いでしょうか。以下のコードを見て下さい。\n\nfor (i in 1:3) {\n  \n  for (j in names(myList)) {\n    print(myList[[j]][i])\n  }\n  \n  print(paste0(\"===ここまでが出席番号\", i, \"番の人です===\"))\n}\n\n[1] \"Yanai\"\n[1] \"Tanaka\"\n[1] \"Abe\"\n[1] \"===ここまでが出席番号1番の人です===\"\n[1] \"Song\"\n[1] \"Sato\"\n[1] \"Moon\"\n[1] \"===ここまでが出席番号2番の人です===\"\n[1] \"Hadley\"\n[1] \"Suzuki\"\n[1] \"Xi\"\n[1] \"===ここまでが出席番号3番の人です===\"\n\n\n以上のコードは以下のように動きます。\n\n1行目: 任意の変数をiとし、1から3までの数字をiに格納しながら、3回反復作業を行います。\n3行目: 任意の変数をjとし、ここにはリストの要素名を格納しながら、リストの長さだけ、処理を繰り返します。names(myList)はc(\"Class_A\", \"Class_B\", \"Class_C\")と同じです。\n4行目: myList[[j]][i]の内容を出力します。最初はi = 1、j = \"Class_A\"なので、print(myList[[\"Class_A\"]][1])、つまり、myListから\"Class_A\"を取り出し、そこの1番目の要素を出力するという意味です。\n7行目: 各ベクトルからi番目の要素を出力したら、print(paste0(\"===ここまでが出席番号\", i, \"番の人です===\"))を実行し、iに次の要素を入れて、反復作業を続きます。iに格納する要素がなくなったら、反復を中止します。\n\n多重for()文は3重、4重も可能ですが、コードの可読性が低下するため、多くても3つ、できれば最大2つが良いです。3重以上にfor()文を重ねる場合は内側のfor()文を後ほど解説する関数でまとめるのがおすすめです。\n\n\nwhile()による反復\nfor()文は任意の変数にベクトルの要素を1つずつ入れ替えながら、ベクトルの要素を使い尽くすまで反復処理を行います。一方、「ある条件が満たされる限り、反復し続ける」ことも可能であり、この際使う関数がwhile()文です。while()文の書き方はfor()文に非常に似ています。\nwhile (条件) {\n  条件が満たされた場合の処理内容\n}\nforがwhileに変わり、()内の書き方が(任意の変数 in ベクトル)から(条件)に変わりました。この()内の条件が満たされる間は{}内の内容を処理します。1から5を表示するコードはfor()文を使う場合、以下のようになります。\n\n# for文を使う場合\nfor (i in 1:5) {\n    print(i)\n}\n\nこれをwhile()を使って再現すると以下のようなコードとなります。\n\n# while文を使う場合\ni <- 1\n\nwhile (i <= 5) {\n  print(i)\n  i <- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nwhile()内の条件はi <= 5、つまり「iが5以下なら以下の内容を処理する」ことを意味します。注意すべきところはiが5以下か否かの判断は最初から行われるため、予め変数iを指定しておく必要があります。最初に出てくるi <- 1がそれです。そして、{}内にはi <- i + 1を追加し、iを1を足していきます。i = 5の時点ではprint(i)が実行されますが、i = 6になった瞬間、反復は停止します。\nもう一つ注意すべきところはi <- i + 1の位置です。以下は先ほどと同じコードですが、i <- i + 1がprint(i)の前に位置するコードです。\n\ni <- 1\n\nwhile (i <= 5) {\n  i <- i + 1\n  print(i)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n\n\n今回は2から6まで出力されました。これはiを出力する前にiに1が足されるからです。そして、i = 5の状態で、iに1が足されることとなり、6が出力されます。もし、i <- i + 1をprint(i)より前に置いたまま、1から5を出力させる場合は、以下のようにコードを修正する必要があります。\n\ni <- 0\n\nwhile (i <= 4) {\n  i <- i + 1\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n変更点は1)iの初期値が0となった点と、2) ()内の条件が(i <= 5)から(i <= 4)に変わった点です。\nwhile()による書き方は慣れないとややこしいと感じる方も多いかと思います。また、for()文で代替できるケースも多いです。それでもwhile()文を使うケースはあります。それは先述した通り、「目標が決まっているが、その目標が達成されるまでは何回繰り返せば分からない時」においてwhile()文は効果的です。\nたとえば、6面サイコロ投げを考えてみましょう。投げる度に出た目を記録し、その和が30以上に達した瞬間、サイコロ投げを中止します。この場合、何回サイコロを投げる必要があるでしょうか。連続で6が出るなら5回で十分ですが、ずっと1のみが出るなら30回投げる必要があります。このように、「反復処理は行うが、何回行えばいいか分からない。ただし、いつやめるかは知っている」場合にwhile()文を使います。以下はwhile()文を使ったコードです。\n\nTotal <- 0\nTrial <- 1\n\nwhile (Total < 30) {\n  Dice  <- sample(1:6, 1)\n  Total <- Total + Dice\n  \n  print(paste0(Trial, \"回目のサイコロ投げの結果: \", Dice,\n               \"(これまでの総和: \", Total, \")\"))\n  \n  # print()文は以下のような書き方も可能\n  # Result <- sprintf(\"%d回目のサイコロ投げの結果: %d (これまでの総和: %d)\", \n  #                   Trial, Dice, Total)\n  # print(Result)\n  \n  Trial <- Trial + 1\n}\n\n[1] \"1回目のサイコロ投げの結果: 3(これまでの総和: 3)\"\n[1] \"2回目のサイコロ投げの結果: 3(これまでの総和: 6)\"\n[1] \"3回目のサイコロ投げの結果: 1(これまでの総和: 7)\"\n[1] \"4回目のサイコロ投げの結果: 3(これまでの総和: 10)\"\n[1] \"5回目のサイコロ投げの結果: 6(これまでの総和: 16)\"\n[1] \"6回目のサイコロ投げの結果: 2(これまでの総和: 18)\"\n[1] \"7回目のサイコロ投げの結果: 1(これまでの総和: 19)\"\n[1] \"8回目のサイコロ投げの結果: 6(これまでの総和: 25)\"\n[1] \"9回目のサイコロ投げの結果: 4(これまでの総和: 29)\"\n[1] \"10回目のサイコロ投げの結果: 1(これまでの総和: 30)\"\n\n\nそれでは、コードの説明をします。まず、これまで出た目の和を記録する変数Totalを用意し、初期値として0を格納します。また、何回目の投げかを記録するためのTrial変数を記録し、初期値として1を格納しておきます (コードの1、2行目)。\n続いてwhile()文を入れます。反復する条件はTotalが30より小さい場合と設定します。つまり、Totalが30以上になったら反復を終了することを意味します (コードの4行目)。\nつづいて、サイコロ投げをし、出た目をDiceという変数に格納します。サイコロ投げは1から6の間の整数から無作為に一つを値を抽出することでできます。そこで使われるのがsample()関数です (コードの5行目)。sample()関数は与えられたベクトル内の要素を無作為に抽出する関数です。sample(c(1, 2, 3, 4, 5, 6), 1)は「c(1, 2, 3, 4, 5, 6)から1つの要素を無作為に抽出せよ」という意味です。むろん、複数の要素を抽出することも可能ですし、数値型ベクトルでなく、文字型ベクトルから抽出することも可能です。サイコロ投げをしたら、その目をTotalの値に足します (コードの6行目)。\nつづいて、「1回目のサイコロ投げの結果: 5 (これまでの総和: 5)」のように、現在の処理結果を表示させます (コードの8行目)。最後にTrialの値を1増加させて反復を続けます (コードの14行目)。\nむろん、for()文を使って再現することも可能であり、以下はfor()を使ったコードになります。\n\nTotal <- 0\n\nfor (Trial in 1:30) {\n  Dice  <- sample(1:6, 1)\n  Total <- Total + Dice\n  \n  # print()の代わりにsprintf()を使うことも可能\n  Result <- sprintf(\"%d回目のサイコロ投げの結果: %d (これまでの総和: %d)\",\n                    Trial, Dice, Total)\n  print(Result)\n  \n  if (Total >= 30) {\n    break() # ()は省略可能\n  }\n}\n\n[1] \"1回目のサイコロ投げの結果: 5 (これまでの総和: 5)\"\n[1] \"2回目のサイコロ投げの結果: 6 (これまでの総和: 11)\"\n[1] \"3回目のサイコロ投げの結果: 6 (これまでの総和: 17)\"\n[1] \"4回目のサイコロ投げの結果: 6 (これまでの総和: 23)\"\n[1] \"5回目のサイコロ投げの結果: 3 (これまでの総和: 26)\"\n[1] \"6回目のサイコロ投げの結果: 5 (これまでの総和: 31)\"\n\n\nまず、サイコロを何回投げれば良いかは分かりませんが6面サイコロの場合、30回以内には必ず合計30になるので、まずTrial in 1:30とします。あとはwhile()文の書き方とほぼ同じですが、今回はiが自動的に更新されるのでi <- i + 1は不要です。ただし、一定の条件が満たされる場合、for()文を停止する必要があります。そこで登場するのが条件分岐とbreak()です。条件分岐は次節で説明しますが、これは「Totalが30以上になった場合、ループから脱出せよ」を意味します。このループからの脱出を指示する関数がbreak()です。ちなみに、break()関数は()を抜いてbreakと書いても問題ありません。また、if(){}内の処理内容は一行ですので、if (Total >= 30) breakのような書き方でも問題ありません。\nこれは余談ですが、結果の表示に今回はこれまで使ってきたprint()とpaste0() (またはpaste())を使わず、sprintf()を使いました。人によってはこっちの書き方を好む場合もあります。sprintf()内の%dはその位置に指定された変数の値を整数として代入することを意味します。sprintf()はまず、出力する文字列を指定し、続いて代入する変数を順番に引数として入れます。%sは文字型、%fは実数を意味します。%fの場合、少数点の桁数も指定可能であり、小数点2桁まで表示させる場合は%.2fのように表記します。以下のコードは3つの変数の値と文字列を結合する処理をprint(paste0())とsprintf()を用いて書いたものであり、同じ動きをします。\n\nName   <- \"Song\"\nScore  <- 50\nHeight <- 176.2\n\nprint(paste0(Name, \"の数学成績は\", Score, \"点で、身長は\", Height, \"cmです。\"))\n\n[1] \"Songの数学成績は50点で、身長は176.2cmです。\"\n\n# %sにNameを、%dにHeightを、%.1fにHeightを小数点1位まで格納し、出力\nsprintf(\"%sの数学成績は%d点で、身長は%.1fcmです。\", Name, Score, Height)\n\n[1] \"Songの数学成績は50点で、身長は176.2cmです。\"\n\n\nただし、{}内のsprintf()はそのまま出力されないため、一旦、オブジェクトとして保存し、それをprint()を使って出力させる必要があります。詳細は?sprintfで確認してください。\n今回は「多くても30回以内に終わる」ことが分かっていましたが、シミュレーションなどではそれが未知であるケースも多いです。目的に合わせてfor()文とwhile()文を使い分けましょう。"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#programming-condition",
    "href": "tutorial/R/rprogramming.html#programming-condition",
    "title": "Rプログラミング入門の入門",
    "section": "条件分岐",
    "text": "条件分岐\n\nif()、else if()、else()による条件分岐\n続いて、条件分岐について説明します。これは何らかの条件が満たされた場合、指定された処理を行うことを意味します。一般的な条件分岐は以下のように書きます。\nif (条件) {\n  条件が満たされた場合の処理内容\n}\nwhile()文と非常に書き方が似ていることが分かります。ただし、while()文は条件が満たされる限り{}の内容を繰り返して実行する意味を持つ一方、if()文は条件が満たされれば{}の内容を処理して終わるといった違いがあります。たとえば、名前が格納されているオブジェクトNameの中身が\"Song\"なら「アホ」と出力されるコードを考えてみましょう。\n\nName <- \"Song\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n}\n\n[1] \"アホ\"\n\n\nif()内の条件は「Nameが\"Song\"」の場合、{}内の内容を処理せよということを意味します。もし、Nameの中身が\"Yanai\"ならどうでしょうか。\n\nName <- \"Yanai\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n}\n\nなんの結果も表示されません。if()文単体だと、条件が満たされない場合の処理内容を指定することができません。ここでelse文の出番です。これはif()とセットで使われるものであり、if()内の条件が満たされなかった場合の処理内容を指定します。elseを加えたコードの書き方は以下の通りです。\nif (条件) {\n  条件が満たされた場合の処理内容\n} else {\n  条件が満たされなかった場合の処理内容\n}\nelse文は以下のように改行して入れることも可能です。\nif (条件) {\n  条件が満たされた場合の処理内容\n} \nelse {\n  条件が満たされなかった場合の処理内容\n}\nただし、一般的なRスクリプトの書き方だと、改行しない方が望ましいです。なぜなら、「このif()文とelseは一つのセットです」ということがより明確になるからです。コードの書き方は人にもよりますが、何らかのこだわりがないなら、改行せずに使いましょう。\nそれではNameが\"Song\"じゃない場合、「天才」と表示されるコードを書いてみましょう。\n\nName <- \"Song\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else {\n  print(\"天才\")\n}\n\n[1] \"アホ\"\n\n\nNameが\"Song\"の場合、前と変わらず「アホ」が出力されます。それではNameを\"Yanai\"に変えてみましょう。\n\nName <- \"Yanai\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else {\n  print(\"天才\")\n}\n\n[1] \"天才\"\n\n\nちゃんと「天才」が表示されます。しかし、世の中はアホと天才のみで構成されておりません。世界に真の天才はYanaiとShinadaのみ、アホはSongのみだとします。それ以外の人は凡才だとします。つまり、今回はパタンが3つあります。この場合はelse if()をif()とelseの間に挿入します。\nif (条件1) {\n  条件1が満たされた場合の処理内容\n} else if (条件2) {\n  条件1が満たされず、条件2が満たされたい場合の処理内容\n} else if (条件3) {\n  条件1, 2が満たされず、条件3が満たされたい場合の処理内容\n} else {\n  条件が全て満たされなかった場合の処理内容\n}\nこれはif()文の条件が満たされたら後の{}内容を処理し、満たされなかったら次のelse if()内の条件を判定する。そして、条件が満たされたら{}の内容が実行され、満たされなかったら次のelse if()へ移動する…といった構造となります。それでは実際のコードを書いてみましょう。\n\nName <- \"Song\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else if (Name == \"Yanai\" | Name == \"Shinada\") {\n  # 以上の文は } else if (Name %in% c(\"Yanai\", \"Shinada\")) { もOK\n  print(\"天才\")\n} else {\n  print(\"凡才\")\n}\n\n[1] \"アホ\"\n\n\nまず、Nameが\"Song\"か否かを判定し、TRUEなら「アホ」を表示します。もし、FALSEなら次のelse if()文へ移動します。ここではNameが\"Yanai\"もしくは\"Shinada\"かを判定します。|は「OR (または)」を意味する論理演算子です。このように条件の中には|や&などの論理演算子を使うことで、複数の条件を指定することができます。また、Name == \"Yanai\" | Name == \"Shinada\"はName %in% c(\"Yanai\", \"Shinada\")に書き換えることができます。x %in% yはxがyに含まれているか否かを判定する演算子です。たとえば、\"A\" %in% c(\"A\", \"B\", \"C\")の結果はTRUEですが、\"Z\" %in% c(\"A\", \"B\", \"C\")の結果はFALSEです。\nそれではNameを\"Yanai\"、\"Shinada\"、\"Shigemura\"、\"Hakiai\"に変えながら結果を確認してみましょう。\n\nName <- \"Yanai\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else if (Name %in% c(\"Yanai\", \"Shinada\")) {\n  print(\"天才\")\n} else {\n  print(\"凡才\")\n}\n\n[1] \"天才\"\n\n\n\nName <- \"Shinada\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else if (Name %in% c(\"Yanai\", \"Shinada\")) {\n  print(\"天才\")\n} else {\n  print(\"凡才\")\n}\n\n[1] \"天才\"\n\n\n\nName <- \"Shigemura\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else if (Name %in% c(\"Yanai\", \"Shinada\")) {\n  print(\"天才\")\n} else {\n  print(\"凡才\")\n}\n\n[1] \"凡才\"\n\n\n\nName <- \"Hakiai\"\n\nif (Name == \"Song\") {\n  print(\"アホ\")\n} else if (Name %in% c(\"Yanai\", \"Shinada\")) {\n  print(\"天才\")\n} else {\n  print(\"凡才\")\n}\n\n[1] \"凡才\"\n\n\nif()文が単独で使われることは滅多にありません。if()文は主に反復処理を行うfor()またはwhile()文内、もしくは次節で説明する自作関数内に使われる場合がほとんどです。実際、これまで名前からアホ・天才・凡才を判定する度に同じコードを書いてきました。これはあまりにも非効率的です。これを関数としてまとめることが出来るとより使いやすくなります。関数の作成については後ほどで説明します。\nここではfor()文と条件分岐の組み合わせについて考えてみましょう。生徒10人の成績が入っているベクトルScoresがあるとします。そして、成績が60点以上なら「可」を、未満なら「不可」を返すようなコードを書きます。この作業を効率的に行うためにはfor()文とif()文を組み合わせる方法が考えられます。つまり、for()文には任意の変数iに1から10までの数字を入れ替えながら反復を行います。ここでの10はベクトルScoresの長さですので、length(Scores)も問題ありません。そして、Scoresのi番目要素に対して60点以上か否かの判定を行い、「可」または「不可」の結果を返します。以下のコードはその例です。\n\nScores <- c(58, 100, 81, 97, 71, 61, 47, 60, 73, 85)\n\nfor (i in 1:length(Scores)) {\n  if (Scores[i] >= 60) {\n    print(paste0(\"生徒\", i, \"の判定結果: 可\"))\n  } else {\n    print(paste0(\"生徒\", i, \"の判定結果: 不可\"))\n  }\n}\n\n[1] \"生徒1の判定結果: 不可\"\n[1] \"生徒2の判定結果: 可\"\n[1] \"生徒3の判定結果: 可\"\n[1] \"生徒4の判定結果: 可\"\n[1] \"生徒5の判定結果: 可\"\n[1] \"生徒6の判定結果: 可\"\n[1] \"生徒7の判定結果: 不可\"\n[1] \"生徒8の判定結果: 可\"\n[1] \"生徒9の判定結果: 可\"\n[1] \"生徒10の判定結果: 可\"\n\n\n\n1行目: まず、ベクトルScoresを定義します。\n3行目: for()文を用います。任意の変数はiとし、ここには1から10を入れ替えながら反復を行います。したがって、i in 1:10でも問題ありませんが、ここではi in 1:length(Scores)にします。こうすることで、Scoresベクトルの長さが変わっても、for()の中身を修正する必要がなくなります。\n4, 5行目: Scoresのi番目要素が60点以上かを判定し、TRUEなら「生徒iの判定結果: 可」を出力します。\n6-8行目: Scoresのi番目要素が60点以上でない場合、「生徒iの判定結果: 不可」を出力します。\n\nprint()内にpaste0()を使うのはコードの可読性がやや落ちるので、sprintf()を使うことも可能です。\n\nScores <- c(58, 100, 81, 97, 71, 61, 47, 60, 73, 85)\n\nfor (i in 1:length(Scores)) {\n  if (Scores[i] >= 60) {\n    Result <- sprintf(\"生徒%dの判定結果: 可\", i)\n  } else {\n    Result <- sprintf(\"生徒%dの判定結果: 不可\", i)\n  }\n  \n  print(Result)\n}\n\n[1] \"生徒1の判定結果: 不可\"\n[1] \"生徒2の判定結果: 可\"\n[1] \"生徒3の判定結果: 可\"\n[1] \"生徒4の判定結果: 可\"\n[1] \"生徒5の判定結果: 可\"\n[1] \"生徒6の判定結果: 可\"\n[1] \"生徒7の判定結果: 不可\"\n[1] \"生徒8の判定結果: 可\"\n[1] \"生徒9の判定結果: 可\"\n[1] \"生徒10の判定結果: 可\"\n\n\n続いて、結果を出力するのではなく、結果を別途のベクトルに格納する例を考えてみましょう。予め長さ10の空ベクトルPFを用意します。ここに各生徒の判定結果を「可/不可」で格納する処理を行います。以下はその例です。\n\nScores <- c(58, 100, 81, 97, 71, 61, 47, 60, 73, 85)\nPF     <- rep(NA, length(Scores))\n\nfor (i in 1:length(Scores)) {\n  if (Scores[i] >= 60) {\n    PF[i] <- \"可\"\n  } else {\n    PF[i] <- \"不可\"\n  }\n}\n\nまず、中身がNAのみで構成されている長さ10の空ベクトルPFを生成します。rep(NA, length(Scores))はNAをlength(Scores)個 (ここでは10個)並べたベクトルを生成する関数です。たとえば、rep(5, 3)はc(5, 5, 5)と同じです。\n続いて、条件分岐ですが、今回はメッセージを出力するのではなく、PFのi番目に\"可\"か\"不可\"を入れます。以下はその結果です。\n\nPF\n\n [1] \"不可\" \"可\"   \"可\"   \"可\"   \"可\"   \"可\"   \"不可\" \"可\"   \"可\"   \"可\"  \n\n\nこのように、if()文は反復処理を行うfor()またはwhile()文と組み合わせることで本領発揮となります。実は今回の例ですが、後で説明するifelse()を使うと、一行で処理することができます。ただし、複雑な処理を伴う反復と条件分岐の組み合わせの場合は、今回のようにfor()とif()を組み合わせる必要が出てきます。\n\n\nifelse()による条件分岐\nifelse()は与えられたベクトル内の要素に対して条件分岐をし、それを全ての要素に対して繰り返す関数です。簡単な条件分岐と反復処理を同時に行う非常に便利な関数です。ifelse()の使い方は以下の通りです。\nifelse(条件, 条件がTRUEの場合の処理、条件がFALSEの場合の処理)\nたとえば、生徒10人の成績が入っているベクトルScoresに対して、60点以上の場合、合格 (\"Pass\")、未満の場合は不合格 (\"Fail\")の値を割り当て、PFというベクトルに格納するとします。\n\nScores <- c(58, 100, 81, 97, 71, 61, 47, 60, 73, 85)\nPF     <- ifelse(Scores >= 60, \"Pass\", \"Fail\")\nPF\n\n [1] \"Fail\" \"Pass\" \"Pass\" \"Pass\" \"Pass\" \"Pass\" \"Fail\" \"Pass\" \"Pass\" \"Pass\"\n\n\n条件はScoresが60以上か否かであり、60以上なら\"Pass\"を、それ以外なら\"Fail\"を返します。また、返す値は必ず新しい値である必要がありません。一定の条件が満たされたらそのままにし、それ以外なら指定した値を返すことも可能です。たとえば、世論調査では以下のように「答えたくない」または「わからない」を9や99などで記録することがよくあります。この値はこのまま使うのが難しい場合が多く、欠損値として扱う場合があります。\nQ. あなたはラーメンが好きですか。\n\n1.非常にそう思う\n2.そう思う\n3.どちらかと言えばそう思う\n9.答えたくない\n\n以上の選択肢に対する10人の回答が格納されているデータフレーム (Ramen.df)があるとします。データフレームは2つの列があり、ID列は回答者のID、Ramen列は以上の質問文に対する答えが入っています。\n\nRamen.df <- data.frame(\n  ID    = 1:10,\n  Ramen = c(1, 1, 2, 1, 3, 1, 9, 2, 1, 9)\n)\n\n\nRamen.df\n\n   ID Ramen\n1   1     1\n2   2     1\n3   3     2\n4   4     1\n5   5     3\n6   6     1\n7   7     9\n8   8     2\n9   9     1\n10 10     9\n\n\nここのRamen列に対して、Ramen == 9ならNAを割り当て、それ以外の場合は元の値を割り当てるとします。そしてその結果をRamen.dfのRamen列に上書きします。これは以下のように書くことができます。\n\nRamen.df$Ramen <- ifelse(Ramen.df$Ramen == 9, NA, Ramen.df$Ramen)\n\nRamen.df\n\n   ID Ramen\n1   1     1\n2   2     1\n3   3     2\n4   4     1\n5   5     3\n6   6     1\n7   7    NA\n8   8     2\n9   9     1\n10 10    NA\n\n\nこれはRamen.df$Ramenの値が9ならNAを、それ以外なら元の値を格納し、結果をRamen.df$Ramenに上書きすることを意味します。\nまた、ifelse()の中にifelse()を使うことも可能です。Scoresベクトルの例を考えてみましょう。ここで90点以上ならS、80点以上ならA、70点以上ならB、60点以上ならCを割り当て、それ以外はDを返すとします。この結果をGradeという変数に格納してみましょう。\n\nGrade <- ifelse(Scores >= 90, \"S\", \n                ifelse(Scores >= 80 & Scores < 90, \"A\",\n                       ifelse(Scores >= 70 & Scores < 80, \"B\",\n                              ifelse(Scores >= 60 & Scores < 70, \"C\", \"D\"))))\n\n\nGrade\n\n [1] \"D\" \"S\" \"A\" \"S\" \"B\" \"C\" \"D\" \"C\" \"B\" \"A\"\n\n\nこれはifelse()の条件がFALSEの場合、次のifelse()で判定する仕組みとなっています。しかし、ifelse()を使いすぎるとコードの可読性が非常に落ちてしまうので、あまり例ではありません。しかもifelse()の中にifelse()を書く場合、&や|を用いることが多く、更に可動性が落ちてしまいます。この場合はdplyrパッケージに含まれているcase_when()関数を推奨します。この関数に関しては「dplyr入門」で詳細に説明する予定ですが、ここでは上のコードと同じ処理を行うcase_when()関数を書く方を紹介します。\n\nGrade2 <- dplyr::case_when(Scores >= 90 ~ \"S\",\n                           Scores >= 80 & Scores < 90 ~ \"A\",\n                           Scores >= 70 & Scores < 80 ~ \"B\",\n                           Scores >= 60 & Scores < 70 ~ \"C\",\n                           Scores <  60 ~ \"D\")\n\n\nGrade2\n\n [1] \"D\" \"S\" \"A\" \"S\" \"B\" \"C\" \"D\" \"C\" \"B\" \"A\"\n\n\n\n\nswitch()による条件分岐\nswitch()は基本的に与えられた長さ1の文字列5を用いた条件分岐となります。これは単体で使われうことがほぼなく、自作関数内に使われる場合があります。したがって、この節はとりあえず飛ばし、関数の節を読んでから戻ってきてください。\nここでは2つの数値xとyの加減乗除を行う関数myCalcを作成します。この関数はx とy以外にもmethod引数があり、このmethodの値によって行う処理が異なります。たとえば、method = \"+\"なら足し算を、method = \"*\"なら掛け算を行う仕組みです。\n\nmyCalc <- function(x, y, method) {\n  switch(method,\n         \"+\" = x + y,\n         \"-\" = x - y,\n         \"*\" = x * y,\n         \"/\" = x / y,\n         stop(\"method引数は+, -, *, /のみ使用可能です。\")\n         )\n}\n\nこのコードの中身をみてみましょう。重要なのは2行目から8行目の部分です。switch()文の最初の引数は条件を判定する長さ1のベクトルです。ここではこれがmethod引数となります。そして、method引数によって異なる処理を行うわけです。\"+\" = x + yは「methodの値が\"+\"の場合、x + yを実行する」ことを意味します。これを他の演算子に対しても指定します。最後の引数は条件に合っていない場合の処理内容です。ここでは\"method引数は+, -, *, /のみ使用可能です。\"というメッセージを出力させ、関数を中止させるという意味で、stop(\"method引数は+, -, *, /のみ使用可能です。\")を入れました。\nむろん、以上のコードはif()文を使うことも可能です。\n\nmyCalc <- function(x, y, method) {\n  \n  if (method == \"+\") {\n    return(x + y)\n  } else if (method == \"-\") {\n    return(x - y)\n  } else if (method == \"*\") {\n    return(x * y)\n  } else if (method == \"/\") {\n    return(x / y)\n  } else {\n    stop(\"method引数は+, -, *, /のみ使用可能です。\")\n  }\n  \n}\n\nまた、match.arg()関数を使って事前に使える引数を指定しておき、該当しない場合は関数の処理を中止させることもできます。基本的には上の書き方より以下の書き方が推奨されます。\n\nmyCalc <- function(x, y, method = c(\"+\", \"-\", \"*\", \"/\")) {\n  \n  method <- match.arg(method)\n  \n  if (method == \"+\") {\n    return(x + y)\n  } else if (method == \"-\") {\n    return(x - y)\n  } else if (method == \"*\") {\n    return(x * y)\n  } else {\n    return(x / y)\n  } \n}\n\n異なる点はfunction()の中にmethodのデフォルト値をベクトルとしてした点です。これは「method引数はこれらの値以外は許さない」ということを意味します。むろん、普通のデフォルト値としてベクトルを指定することも可能ですが、このような使い方もあります。そしてmatch.arg()関数についてですが、これはmethod引数が予め指定されていた引数の値と一致するか否かを判断する関数です。もし、予め指定されていた引数と一致しない場合、エラーを表示させ、処理を中断します。ちなみに、match.arg()はswith()文と組み合わせて使うことも可能です。。\n今回の例の場合、switch()文の方が読みやすいコードですが、if()でも十分に使えます。また、条件によって行う処理が複雑な場合はswitch()よりもif()の方が使いやすいです。\n\nmyCalc(5, 3, \"+\")\n\n[1] 8\n\nmyCalc(5, 3, \"-\")\n\n[1] 2\n\nmyCalc(5, 3, \"*\")\n\n[1] 15\n\nmyCalc(5, 3, \"/\")\n\n[1] 1.666667\n\n\nそれでは\"+\"、\"-\"、\"*\"、\"/\"以外の値をmethodに与えたらどうなるでしょうか。ここでは\"^\"を入れてみましょう。\n\nmyCalc(5, 3, \"^\")\n\nError in myCalc(5, 3, \"^\"): method引数は+, -, *, /のみ使用可能です。\n\n\nstop()内に書いたエラーメッセージが表示され、処理は中止されます。"
  },
  {
    "objectID": "tutorial/R/rprogramming.html#programming-function",
    "href": "tutorial/R/rprogramming.html#programming-function",
    "title": "Rプログラミング入門の入門",
    "section": "関数の作成",
    "text": "関数の作成\nこれまでclass()や、sum()、print()など様々な関数を使ってきました。関数は関数名(引数)の書き方をします。関数とは()内の引数のデータを関数内部の手続きに沿って処理し、その結果を返す関数です。したがって、関数は「データだけ違って、同じ処理を行いたい」場合に非常に便利です。\nベクトルc(1, 2, 3, 4, 5)の総和を計算する方法について考えてみましょう。まず、1つ目は単純に足し算をする方法があります。\n\n1 + 2 + 3 + 4 + 5\n\n[1] 15\n\n\n他にも反復処理を使うことも可能です。とりわけ、1:100のようなベクトルを1つ目の方法で記述するのは時間の無駄でしょう。for()文を使った方法は以下のようになります。\n\nResult <- 0\n\nfor (i in 1:5) {\n  Result <- Result + i  \n}\n\n\nResult\n\n[1] 15\n\n\n数個の数字を足すだけなら方法1の方が楽でしょうし、数百個の数字の場合は方法2の方が効率的です。それでもやはりsum()関数の方が数倍は効率的です。また、関数を使うことで、スクリプトの量をへらすこともできます。1から100までの総和なら、方法2のfor (i in 1:5)をfor (i in 1:100)に変えることで対応可能ですが、それでも全体としては数行のコードで書かなくてもなりません。一方、sum(1:100)なら一行で済みます。\nsum()はまだマシな方です。たとえば、回帰分析をしたい場合、毎回回帰分析のコードを一から書くのはあまりにも非効率的です。lm()関数を使うと、データや回帰式などを指定するだけで、一連の作業を全て自動的に行い、その結果を返してくれます。中には回帰式の係数も計算してくれますが、他にも残差や決定係数なども計算してくれます。その意味で、lm()という関数は複数の機能を一つの関数としてまとめたものでもあります。\nこれらの関数は既にR開発チームが書いた関数ですが、ユーザー側から関数を作成することも可能です。長いコードを書く、同じ作業を繰り返す場合、関数の作成はほぼ必須とも言えます。ここではまず、簡単な関数を作成してみましょう。与えられた数字を二乗し、その結果を返すmyPower()関数を作ってみましょう。\n\nmyPower <- function(x) {\n  x^2\n}\n\n\n# 引数が一つしかないので、myPower(24)も可能\nmyPower(x = 24)\n\n[1] 576\n\n\nそれではコードを解説します。関数は以下のように定義されます。\n関数名 <- function (引数名) {\n  処理内容\n}\nまず、関数名をmyPowerとし、それが関数であることを宣言します。そして、この関数の引数の名前はxとします。それが\nmyPower <- function (x)\nの部分です。続いて、{}内に処理内容を書きます。今回はx^2であり、これはxの2乗を意味します。そして、関数の処理結果が返されますが、{}内の最後の行が結果として返されます。x^2の部分はreturn(x^2)と書き換えることも可能です。return()は「この結果を返せよ」という意味の関数ですが、返す結果が最後の行である場合、省略可能であり、Hadely先生もこのような書き方を推奨しています。\nそれでは、もうちょっと複雑な関数を作成してみましょう。ベクトルを引数とし、その和を計算するmySum()という関数です。要するにsum()関数を再現したものです。\n\n# mySum関数を定義し、引数はxのみとする\nmySum <- function(x) {\n  # 結果を格納するベクトルResultを生成し、0を入れておく\n  Result <- 0\n  \n  # xの要素をiに一つずつ入れながら反復処理\n  for (i in x) {\n    # Resultに既存のResultの値にiを足した結果を上書きする\n    Result <- Result + i\n  }\n  \n  # Resultを返す\n  Result\n}\n\n\nmySum(1:5)\n\n[1] 15\n\n\n普通のsum()関数と同じ動きをする関数が出来上がりました。よく見ると、上で説明した総和を計算する方法2のコードを丸ごと関数内に入っているだけです。変わったところがあるとすれば、for()文であり、for (i in 1:5)がfor (i in x)に変わっただけです。ここのxはmySum <- function (x)のxを意味します。このように関数を一回作成しておくと、これからは総和を出す作業を1行に短縮することができます。\nこのmySum()ですが、一つ問題があります。それはxに欠損値が含まれている場合、結果がNAになることです。\n\nmySum(c(1, 2, 3, NA, 5))\n\n[1] NA\n\n\n実際、R内蔵関数であるsum()も同じですが、sum()にはna.rm =というもう一つの引数があり、これをTRUEにすることで欠損値を除いた総和が計算できます。つまり、関数は複数の引数を持つことができます。それでは、mySum()を改良してみましょう。ここにもna.rmという関数を追加し、na.rm引数がTRUEの場合、xから欠損値を除いた上で総和を計算するようにしましょう。\n\nmySum <- function(x, na.rm = FALSE) {\n  if (na.rm == TRUE) {\n    x <- x[!is.na(x)]\n  }\n  \n  Result <- 0\n  \n  for (i in x) {\n      Result <- Result + i\n  }\n  \n  Result\n}\n\n\nmySum(c(1, 2, 3, NA, 5))\n\n[1] NA\n\nmySum(c(1, 2, 3, NA, 5), na.rm = FALSE)\n\n[1] NA\n\nmySum(c(1, 2, 3, NA, 5), na.rm = TRUE)\n\n[1] 11\n\n\n変わったところは、まずfunction (x)がfunction (x, na.rm = FALSE)になりました。これはxとna.rmの引数が必要であるが、na.rmのデフォルト値はFALSEであることを意味します。デフォルト値が指定されている場合、関数を使用する際、その引数は省略できます。実際、sum()関数のna.rm引数もFALSEがデフォルトとなっており、省略可能となっています。\n次は最初に条件分岐が追加されました。ここではna.rmがTRUEの場合、xから欠損値を抜いたベクトルをxに上書きするように指定しました。もし、FALSEならこの処理は行いません。\nこれでR開発チームが作成したsum()関数と同じものが出来上がりました。それでは引数の順番について簡単に解説し、もうちょっと複雑な関数を作ってみましょう。引数の順番は基本的にfunction()の()内で定義した順番であるなら、引数名を省略することも可能です。\n\nmySum(c(1, 2, 3, NA, 5), TRUE)\n\n[1] 11\n\n\nただし、順番を逆にすると、以下のようにわけのわからない結果が返されます。\n\nmySum(TRUE, c(1, 2, 3, NA, 5))\n\nError in if (na.rm == TRUE) {: the condition has length > 1\n\n\n任意の順番で引数を指定する場合、引数名を指定する必要があります。\n\nmySum(na.rm = TRUE, x = c(1, 2, 3, NA, 5))\n\n[1] 11\n\n\n自分で関数を作成し、他の人にも使ってもらう場合、引数名、順番、デフォルト値を適切に設定しておくことも大事です。\nちょっと複雑な関数を作ってみよう\nそれではちょっとした遊び心を込めた関数を作ってみましょう。その名もドラクエ戦闘シミュレーターです。\n以下はドラクエ11のダメージ公式です。\n\nダメージの基礎値 = (攻撃力 / 2) - (守備力 / 4)\n\n0未満の場合、基礎値は0とする\n\nダメージの幅 = (ダメージの基礎値 / 16) + 1\n\n端数は切り捨てます (floor()関数使用)\n\n\nダメージの最小値は「ダメージの基礎値 - ダメージの幅」、最大値は「ダメージの基礎値 - ダメージの幅」となります。この最小値が負になることもありますが、その場合は0扱いになります。実際のダメージはこの範囲内でランダムに決まります (runif()関数使用)。\n\n# DQ_Attack関数を定義\nDQ_Attack <- function(attack, defence, hp, enemy) {\n  ## 引数一覧\n  ## attack: 勇者の力 + 武器の攻撃力 (長さ1の数値型ベクトル)\n  ## defence: 敵の守備力 (長さ1の数値型ベクトル)\n  ## hp: 敵のHP (長さ1の数値型ベクトル)\n  ## enemy: 敵の名前 (長さ1の文字型ベクトル)\n  \n  # ダメージの基礎値\n  DefaultDamage <- (attack / 2) - (defence / 4)\n  # ダメージの基礎値が負の場合、0とする\n  DefaultDamage <- ifelse(DefaultDamage < 0, 0, DefaultDamage)\n  # ダメージの幅\n  DamageWidth   <- floor(DefaultDamage / 16) + 1\n  \n  # ダメージの最小値\n  DamageMin     <- DefaultDamage - DamageWidth\n  # ダメージの最小値が負の場合、0とする\n  DamageMin     <- ifelse(DamageMin < 0, 0, DamageMin)\n  # ダメージの最大値\n  DamageMax     <- DefaultDamage + DamageWidth\n  \n  # 敵の残りHPを格納する\n  CurrentHP     <- hp\n  \n  # 残りHPが0より大きい場合、以下の処理を繰り返す\n  while (CurrentHP > 0) {\n    # ダメージの最小値から最大値の間の数値を1つ無作為に抽出する\n    Damage <- runif(n = 1, min = DamageMin, max = DamageMax)\n    # 小数点1位で丸める\n    Damage <- round(Damage, 0)\n    # 残りのHPを更新する\n    CurrentHP <- CurrentHP - Damage\n    # メッセージを表示\n    print(paste0(enemy, \"に\", Damage, \"のダメージ!!\"))\n  }\n  \n  # 上記の反復処理が終わったら勝利メッセージを出力\n  paste0(enemy, \"をやっつけた！\")\n}\n\n初めて見る関数が3つありますね。まず、floor()関数は引数の端数は切り捨てる関数です。たとえば、floor(2.1)もfloor(2.6)も結果は2です。続いて、runif()関数は指定された範囲の一様分布から乱数を生成する関数です。引数は生成する乱数の個数 (n)、最小値 (min)、最大値 (max)の3つです。runif(5, 3, 10)なら最小値3、最大値10の乱数を5個生成するという意味です。正規分布は平均値周辺の値が生成されやすい一方、一様分布の場合、ある値が抽出される確率は同じです。最後にround()関数は四捨五入の関数です。引数は2つあり、1つ目の引数は数値型のベクトルです。2つ目は丸める小数点です。たとえば、round(3.127, 1)の結果は3.1であり、round(3.127, 2)の結果は3.13となります。\nそれでは「ひのきのぼう」を装備したレベル1の勇者を考えてみましょう。ドラクエ5の場合、Lv1勇者の力は11、「ひのきのぼう」の攻撃力は2ですので、攻撃力は13です。まずは定番のスライムから狩ってみましょう。スライムのHPと守備力は両方7です。\n\nDQ_Attack(13, defence = 7, hp = 7, \"スライム\")\n\n[1] \"スライムに6のダメージ!!\"\n[1] \"スライムに6のダメージ!!\"\n\n\n[1] \"スライムをやっつけた！\"\n\n\nまぁ、こんなもんでしょう。それではスライムナイト (=ピエール)はどうでしょう。スライムナイトのHPのは40、守備力は44です。\n\nDQ_Attack(13, defence = 44, hp = 40, \"スライムナイト\")\n\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに0のダメージ!!\"\n[1] \"スライムナイトに1のダメージ!!\"\n\n\n[1] \"スライムナイトをやっつけた！\"\n\n\nこれだと「なんと　スライムナイトが　おきあがりなかまに　なりたそうに　こちらをみている！」のメッセージを見る前に勇者ご一行が全滅しますね。エスターク (HP: 9000 / 守備力: 250)は計算するまでもないでしょう…\n以上の関数に条件分岐を追加することで「かいしんの　いちげき！」を入れることもできますし6、逆に敵からの攻撃を計算して誰が先に倒れるかをシミュレーションすることも可能でしょうね。色々遊んでみましょう。\n\n関数 in 関数\n当たり前かも知れまっせんが、自作関数を他の自作関数に含めることもできます。ここでは乱数を生成する関数を作ってみましょう。パソコンだけだと完全な乱数を作成することは不可能ですが7、乱数に近いものは作れます。このようにソフトウェアで生成された乱数は擬似乱数と呼ばれ、様々なアルゴリズムが提案されています。天才、フォン・ノイマン大先生も乱数生成では天才ではなかったらしく、彼が提案した平方採中法 (middle-square method)は使い物になりませんので、ここではもうちょっとマシな方法である線形合同法 (linear congruential generators; 以下、LCG)を採用します。平方採中法よりは式が複雑ですが、それでも非常に簡単な式で乱数が生成可能であり、Rによる実装に関しては平方採中法より簡単です。ちなみに、線形合同法にも様々な問題があり、Rのデフォルトは広島大学の松本眞先生と山形大学の西村拓士先生が開発しましたメルセンヌ・ツイスタ (Mersenne twister)を採用しています。それでは、LCGのアルゴリズムから見ましょう。\n乱数の列があるとし、\\(n\\)番目の乱数を\\(X_n\\)とします。この場合、\\(X_{n+1}\\)は以下のように生成されます。\n\\[\nX_{n+1} = (aX_n + c) \\text{ mod } m\n\\]\n\\(\\text{mod}\\)は余りを意味し、\\(5 \\text{mod} 3\\)は5を3で割った際の余りですので、2となります。\\(a\\)と\\(c\\)、\\(m\\)は以下の条件を満たす任意の数です。\n\\[\\begin{align}\n0 < &  m, \\\\\n0 < &  a < m, \\\\\n0 \\leq &  c < m.\n\\end{align}\\]\nベストな\\(a\\)、\\(c\\)、\\(m\\)も決め方はありませんが、ここではTurbo Cの設定を真似て\\(a = 22695477\\)、\\(c = 1\\)、\\(m = 2^{32}\\)をデフォルト値として設定します8。そして、もう一つ重要なのが最初の数、つまり\\(X_n\\)をどう決めるかですが、これは自由に決めて問題ありません。最初の数 (\\(X_0\\))はシード (seed)と呼ばれ、最終的には使わない数字となります。それではseedという引数からある乱数を生成する関数rng_number()を作ってみましょう。\n\nrng_number <- function(seed, a = 22695477, c = 1, m = 2^32) {\n  (a * seed + c) %% m\n}\n\n簡単な四則演算のみで構成された関数ですね。ちなみに%%は余りを計算する演算子です。とりあえず、seedを12345に設定し、一つの乱数を生成してみましょう。\n\nrng_number(12345)\n\n[1] 1002789326\n\n\nかなり大きい数字が出ました。ちなみに線形合同法で得られる乱数の最大値は\\(m\\)、最小値は0です。次は、今回得られた乱数1.0027893^{9}を新しいseedとし、新しい乱数を作ってみましょう。\n\nrng_number(1002789326)\n\n[1] 3785644968\n\n\nこの作業を繰り返すと、(疑似)乱数の数列が得られます。続いて、この作業をn回繰り返し、長さnの乱数ベクトルを返す関数LCGを作ってみましょう。いかがコードになります。\n\nLCG <- function(n, seed, a = 22695477, c = 1, m = 2^32) {\n  rng_vec    <- rep(NA, n + 1) # seedも入るので長さn+1の空ベクトルを生成\n  rng_vec[1] <- seed           # 1番目の要素にseedを入れる\n  \n  # iに2からn+1までの値を順次的に投入しながら、反復処理\n  for (i in 2:(n+1)) {\n    # rng_vecのi番目にi-1番目の要素をseedにした疑似乱数を格納\n    rng_vec[i] <- rng_number(rng_vec[i - 1], a, c, m)\n  }\n  \n  rng_vec <- rng_vec[-1] # 1番目の要素 (seed)を捨てる\n  rng_vec <- rng_vec / m # 最小値0、最大値1になるように、mで割る\n  \n  rng_vec # 結果を返す\n}\n\nそれでは、詳細に解説します。\n\n1行目: 関数LCGを定義し、必要な引数としてnとseedを設定する。\n2行目: 結果を格納する空ベクトルrng_vecを生成。ただし、1番目にはseedが入るので、長さをn+1とする。\n3行目: rng_vecの1番目にseedを格納する。\n6行目: 疑似乱数をn回生成し、格納するように反復作業を行う。任意の変数はiとし、iに代入される値は2からn+1までである。\n8行目: rng_vecのi-1番目要素をseedにした疑似乱数を生成し、rng_vecのi番目に格納する。1回目の処理だとi=2であるため、rng_vec[1] (= seed)をseedにした疑似乱数が生成され、rng_vec[2]に格納される。\n11行目: rng_vecの1番目の要素はseedであるため、捨てる。\n12行目: 乱数が最小値0、最大値1になるように、調整する。具体的には得られた乱数をm (デフォルトは\\(2^{32}\\))で割るだけである。\n14行目: 結果ベクトルを返す。\n\nそれでは、seedを19861008とした疑似乱数10000個を生成し、LCG_Numbersという名のベクトルに格納してみましょう。結果を全て表示させるのは無理があるので、最初の20個のみを確認してみます。\n\nLCG_Numbers <- LCG(10000, 19861008)\nhead(LCG_Numbers, 20)\n\n [1] 0.58848246 0.09900449 0.21707060 0.89462981 0.23421890 0.72341249\n [7] 0.55965400 0.59552685 0.96594972 0.69050965 0.98383344 0.20136551\n[13] 0.25856502 0.37569497 0.50086451 0.03986446 0.89291806 0.24760102\n[19] 0.27912510 0.24493750\n\n\n正直、これが乱数かどうかは見るだけでは分かりませんね。簡単な確認方法としては、これらの乱数列のヒストグラムを見れば分かります。得られた数列が本当に乱数(に近いもの)なら、その分布は一様分布に従っているからです。ただし、一様分布に従っていることが乱数を意味するものではありません。\n可視化については割愛しますが、ここでは簡単にhist()関数を使ってみましょう。必要な引数はnumeric型のベクトルのみです。以下のコードにあるxlab =などはラベルを指定する引数ですが、省略しても構いません。\n\nhist(LCG_Numbers, xlab = \"乱数\", ylab = \"度数\", \n     main = \"生成された乱数10000個のヒストグラム\")\n\n\n\n\n\n\n\n\nややギザギザしているように見えますが9、これなら一様分布だと考えて良いでしょう。"
  },
  {
    "objectID": "tutorial/R/opp_intro.html",
    "href": "tutorial/R/opp_intro.html",
    "title": "オブジェクト指向型プログラミング入門",
    "section": "",
    "text": "修正履歴\n\n2021/05/24: 公開\n2021/05/25: 一部修正\n\n以下の内容は現在執筆中の内容の一部となります。\n\nSong Jaehyun・矢内勇生『私たちのR: ベストプラクティスの探求』(E-book)\nいきなりオブジェクト、関数、引数といった馴染みのない概念が出てきます。これらの概念に馴染みのない方は、予め「Rプログラミング入門の入門」をご一読ください。"
  },
  {
    "objectID": "tutorial/R/opp_intro.html#まずは例から",
    "href": "tutorial/R/opp_intro.html#まずは例から",
    "title": "オブジェクト指向型プログラミング入門",
    "section": "まずは例から",
    "text": "まずは例から\n\nVector1 <- c(1, 5, 3, 7, 9, 12, 5, 4, 10, 1)\nVector2 <- c(\"A\", \"B\", \"D\", \"B\", \"E\", \"A\", \"A\", \"D\", \"C\", \"C\")\n\n\nsummary(Vector1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.00    5.70    8.50   12.00 \n\nsummary(Vector2)\n\n   Length     Class      Mode \n       10 character character \n\n\n　同じsummary()関数ですが、中のデータのタイプによって動きが異なります。これがオブジェクト指向プログラミングにおいて「多態性 (polymorphism)」と呼ばれる概念です。同じ関数でもデータ型、またはデータ構造に応じて異なる動きをすることです。ここでのデータ型やデータ構造を、OOPでは「クラス (class)」と呼びます。クラスはclass()関数で確認することができます。\n\nclass(Vector1)\n\n[1] \"numeric\"\n\nclass(Vector2)\n\n[1] \"character\"\n\n\n　Vector1はnumeric、Vector2はcharacterです。もし、無理矢理にVector1のクラスをcharacterに変えればどうなるでしょうか。クラスの変更はclass(オブジェクト名) <- \"クラス名\"でできます。一つのオブジェクトは複数のクラスが持てますが、これはOOPの「継承 (inheritance)」概念に関係するので後で解説します。ここではまず、Vector1のクラスをcharacterにし、もう一回summary()を使ってみましょう。\n\nclass(Vector1) <- \"character\"\nsummary(Vector1)\n\n   Length     Class      Mode \n       10 character character \n\n\n　データの中身は変わっていませんが、summary()関数の動き方が変わりました。このように、Rで頻繁に使うsummary()、print()、plot()などの関数は様々なクラスの対応しております。lm()関数を使った回帰分析の結果オブジェクトのクラス名はlmであり、その結果を見るためにもsummary()関数を使います。他にもplot(lmオブジェクト名)をすると回帰診断の図が表示されます。これができないと、各クラスに応じた関数を作成する必要がありますね。numeric型専用のnumeric_print()、character型専用のcharacter_print()、lm型専用のlm_plot()など…、覚えなきゃいけない関数が増えてきます。ユーザー側でも大変ですが、コードを作成する側も大変です。実際、図 11を見ると、プログラマーにとって最も大変な仕事は「名付け」であることが分かります。\n\n\n\n\n\n図 1: Programmers’ Hardest Tasks\n\n\n\n\n　OOPの多態性にはこのような煩わしい仕事を軽減する機能があります。OOPにはここで紹介した多態性以外にも、「継承 (inheritance)」、「カプセル化 (encapsulation)」のような特徴があります。他にも人によっては「メッセージパッシング (message passing)」、「動的バインディング (dynamic binding)」などの特徴を述べたりしますが、詳しい話は専門書に譲りたいと思います。また、ここではRのS3クラスについて解説しますが、S3はカプセル化に対応しておりません。したがって、ここでは以下の概念について例と一緒に解説していきたいと思います。\n\nオブジェクト (object)\nクラス (class)\nメソッド (method)\n多態性 (polymorphism)　\n継承 (inheritance)"
  },
  {
    "objectID": "tutorial/R/opp_intro.html#oopとは",
    "href": "tutorial/R/opp_intro.html#oopとは",
    "title": "オブジェクト指向型プログラミング入門",
    "section": "OOPとは",
    "text": "OOPとは\n\nオブジェクト\n　ここは『私たちのR』の「Rプログラミングの基礎」内容の繰り返しですが、オブジェクト (object) とはメモリに割り当てられた「何か」です。「何か」に該当するのは、ベクトル (vector)、行列 (matrix)、データフレーム (data frame)、リスト (list)、関数 (function) などがあります。一般的に、オブジェクトにはそれぞれ固有の（つまり、他のオブジェクトと重複しない）名前が付いています。\n　たとえば、1から5までの自然数の数列を\n\nmy_vec1 <- c(1, 2, 3, 4, 5)  # my_vec1 <- 1:5 でも同じ\n\n　のようにmy_vec1という名前のオブジェクトに格納します。オブジェクトに名前をつけてメモリに割り当てると、その後 my_vec1 と入力するだけでそのオブジェクトの中身を読み込むことができるようになります。\n　ここで、次のように my_vec1の要素を2倍にする操作を考えてみましょう。\n\nmy_vec1 * 2\n\n[1]  2  4  6  8 10\n\n\n　my_vec1は、先ほど定義したオブジェクトです。では2はどうでしょうか。2はメモリに割り当てられていないので、オブジェクトではないでしょうか。実は、この数字 2 もオブジェクトです。計算する瞬間のみ2がメモリに割り当てられ、計算が終わったらメモリから消されると考えれば良いでしょう。むろん、* のような演算子でさえもオブジェクトです。\n\n\nクラス\n　クラス (class) とはオブジェクトを特徴づける属性のことです。既に何度か class() 関数を使ってデータ型やデータ構造を確認しましたが、class()関数でオブジェクトのクラスを確認することができます。先ほど、my_vec1も*も2もオブジェクトであると説明しました。これらがすべてオブジェクトであるということは、何らかのクラス属性を持っているというこです。また、class()関数そのものもオブジェクトなので、何らかのクラスを持ちます。確認してみましょう。\n\nclass(my_vec1)\n\n[1] \"numeric\"\n\nclass(`*`)\n\n[1] \"function\"\n\nclass(2)\n\n[1] \"numeric\"\n\nclass(class)\n\n[1] \"function\"\n\n\n　統計分析をする際に、Rのクラスを意識することはあまりありません。しかし、Rでオブジェクト指向プログラミングを行う際は、オブジェクトのクラスを厳密に定義する必要があります。\n　Rにおける全てはオブジェクトであり、全てのオブジェクトは一つ以上クラスが付与されています。このクラスの考え方はプログラミング言語によって異なります。たとえば、Pythonの場合、一つのクラスの内部にはオブジェクトのデータ構造が定義され、そのクラスで使用可能な関数も含んでいます。また、データを含む場合もあります。このようにクラス内部にデータ、データ構造、専用関数などを格納することをカプセル化（encapsulation）と呼びます。\n　一方、Rの（S3）クラスにはクラス専用関数がクラス内で定義されておらず、データのみが格納されています。\n\n\nメソッドと多態性\n　各クラス専用の関数をメソッド（method）と呼びます。たとえば、summary()関数を考えてみましょう。lm()関数を用いた回帰分析から得られたオブジェクトのクラスはlmであり、c()で作られた数値型ベクトルのクラスはnumericです。しかし、同じsummary()関数ですが、引数のクラスがlmかnumericかによって異なる動きを見せます。その例を見ましょう。\n\nX <- c(1, 3, 5, 7, 9, 11)\nY <- c(1, 2, 3, 7, 11, 13)\nlm_class <- lm(Y ~ X)\nclass(X)\n\n[1] \"numeric\"\n\nclass(lm_class)\n\n[1] \"lm\"\n\nsummary(X)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0     3.5     6.0     6.0     8.5    11.0 \n\nsummary(lm_class)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n      1       2       3       4       5       6 \n 1.3333 -0.2667 -1.8667 -0.4667  0.9333  0.3333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -1.6333     1.0546  -1.549  0.19637   \nX             1.3000     0.1528   8.510  0.00105 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.278 on 4 degrees of freedom\nMultiple R-squared:  0.9477,    Adjusted R-squared:  0.9346 \nF-statistic: 72.43 on 1 and 4 DF,  p-value: 0.001046\n\n\n　このように同じ関数でもクラスによって異なる動作をすることを多態性 (polymorphism)と呼びます。しかし、実はRにおいてこれらの関数は別途作られた関数です。つまり、summary()という関数がクラスごとに定義されていることを意味します。summary()関数がどのクラスで使用可能かを確認するためにはmethods()関数を使います。\n\nmethods(\"summary\")\n\n [1] summary,ANY-method                  summary,DBIObject-method           \n [3] summary.aov                         summary.aovlist*                   \n [5] summary.aspell*                     summary.check_packages_in_dir*     \n [7] summary.connection                  summary.data.frame                 \n [9] summary.Date                        summary.default                    \n[11] summary.Duration*                   summary.ecdf*                      \n[13] summary.factor                      summary.ggplot*                    \n[15] summary.glm                         summary.haven_labelled*            \n[17] summary.hcl_palettes*               summary.infl*                      \n[19] summary.Interval*                   summary.lm                         \n[21] summary.loess*                      summary.manova                     \n[23] summary.matrix                      summary.mlm*                       \n[25] summary.nls*                        summary.packageStatus*             \n[27] summary.Period*                     summary.POSIXct                    \n[29] summary.POSIXlt                     summary.ppr*                       \n[31] summary.prcomp*                     summary.princomp*                  \n[33] summary.proc_time                   summary.rlang_error*               \n[35] summary.rlang_message*              summary.rlang_trace*               \n[37] summary.rlang_warning*              summary.rlang:::list_of_conditions*\n[39] summary.srcfile                     summary.srcref                     \n[41] summary.stepfun                     summary.stl*                       \n[43] summary.table                       summary.tukeysmooth*               \n[45] summary.vctrs_sclr*                 summary.vctrs_vctr*                \n[47] summary.warnings                   \nsee '?methods' for accessing help and source code\n\n\n　このように47種類のクラスに対してsummary()関数が定義されています2。この関数の内部を確認するにはどうすれば良いでしょうか。関数のコードを見るときにはコンソール上に関数名を入力するだけです（()は不要）。\n\nsummary\n\nfunction (object, ...) \nUseMethod(\"summary\")\n<bytecode: 0x7fe77b0c93b0>\n<environment: namespace:base>\n\n\n　しかし、多態性を持つ関数の内部を見ることはできません。そもそもsummary()関数はクラスごとに異なるコードを持っているため、summaryだけでは「どのクラスのsummary()か」が分かりません。それでもRにはsummary()関数が存在し、それをジェネリック関数（generic function）と呼びます。内部にはUseMethod(\"summary\")のみが書かれており、これは「このsummary()関数は様々なクラスのメソッドとして機能するぞ」と宣言しているだけです。各クラスに対応したメソッドの内部を見るにはgetS3method(\"メソッド名\", \"クラス名\")を使います。summary()メソッドはnumeric型が別途指定されていないため、\"defualt\"となります。\n\ngetS3method(\"summary\", \"default\")\n\nfunction (object, ..., digits, quantile.type = 7) \n{\n    if (is.factor(object)) \n        return(summary.factor(object, ...))\n    else if (is.matrix(object)) {\n        if (missing(digits)) \n            return(summary.matrix(object, quantile.type = quantile.type, \n                ...))\n        else return(summary.matrix(object, digits = digits, quantile.type = quantile.type, \n            ...))\n    }\n    value <- if (is.logical(object)) \n        c(Mode = \"logical\", {\n            tb <- table(object, exclude = NULL, useNA = \"ifany\")\n            if (!is.null(n <- dimnames(tb)[[1L]]) && any(iN <- is.na(n))) dimnames(tb)[[1L]][iN] <- \"NA's\"\n            tb\n        })\n    else if (is.numeric(object)) {\n        nas <- is.na(object)\n        object <- object[!nas]\n        qq <- stats::quantile(object, names = FALSE, type = quantile.type)\n        qq <- c(qq[1L:3L], mean(object), qq[4L:5L])\n        if (!missing(digits)) \n            qq <- signif(qq, digits)\n        names(qq) <- c(\"Min.\", \"1st Qu.\", \"Median\", \"Mean\", \"3rd Qu.\", \n            \"Max.\")\n        if (any(nas)) \n            c(qq, `NA's` = sum(nas))\n        else qq\n    }\n    else if (is.recursive(object) && !is.language(object) && \n        (n <- length(object))) {\n        sumry <- array(\"\", c(n, 3L), list(names(object), c(\"Length\", \n            \"Class\", \"Mode\")))\n        ll <- numeric(n)\n        for (i in 1L:n) {\n            ii <- object[[i]]\n            ll[i] <- length(ii)\n            cls <- oldClass(ii)\n            sumry[i, 2L] <- if (length(cls)) \n                cls[1L]\n            else \"-none-\"\n            sumry[i, 3L] <- mode(ii)\n        }\n        sumry[, 1L] <- format(as.integer(ll))\n        sumry\n    }\n    else c(Length = length(object), Class = class(object), Mode = mode(object))\n    class(value) <- c(\"summaryDefault\", \"table\")\n    value\n}\n<bytecode: 0x7fe71b849778>\n<environment: namespace:base>\n\n\n　Rにはメソッドがクラス内部で定義されず、別途のメソッド名.クラス名()といった関数として作成されています。そしてジェネリック関数によって一つの関数の「ように」まとまっています。このように、ジェネリック関数経由でメソッドを呼び出すことをメソッド・ディスパッチ（method dispatch）と呼びます。\n\n\n継承\n　クラスの継承 (inheritance)は一つのオブジェクトが2つ以上のクラスを持つ場合、子クラスが親クラスの特徴を継承することを意味します。たとえば、データフレームの拡張版とも言えるtibbleの場合、複数のクラスを持っています。\n\nlibrary(tidyverse)\nmy_tibble <- tibble(X = 1:5, Y = 1:5)\nclass(my_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n　このmy_tibbleはtlb_dfとtbl、data.frameといった3つのクラスを持っており、先に出てきたものが子クラス、後に出てくるものが親クラスです。tblクラスとdata.frameクラス両方に同じメソッドが定義されている場合、まず子クラスであるメソッド.tbl()が実行されます。もし、子クラスにメソッドが定義されていない場合はtblの親クラスであるdata.frameのメソッドが実行されます。tibbleはデータフレームとは異なるクラスのオブジェクトですが、データフレームと（ほぼ）同じ操作ができるのは、クラスが継承されるからです。クラスの継承ができないと、tibbleで使える全ての関数（列や行の抽出に使う[や$なども！）を全て一から定義する必要がありますが3、継承を使うことによってこのような手間を省くことが出来ます。"
  },
  {
    "objectID": "tutorial/R/opp_intro.html#rにおけるoop",
    "href": "tutorial/R/opp_intro.html#rにおけるoop",
    "title": "オブジェクト指向型プログラミング入門",
    "section": "RにおけるOOP",
    "text": "RにおけるOOP\n\nオブジェクトに任意のクラスを付ける\n　クラスを変えるのは簡単です。class(オブジェクト) <- \"新しいクラス名\"だけです。つまり、関数から何かの結果を返す直前にクラスを変更すれば良いです。\n# 方法1\n関数名 <- function(...) {\n  \n  ...\n  \n  class(返すオブジェクト名) <- \"任意のクラス名\"\n  \n  返すオブジェクト名 # return(オブジェクト名) でもOK\n}\n　たとえば、入力された2つのベクトル（xとy）をリスト構造とし、クラス名をScoreにするにはどうすれば良いでしょうか。\n\nMake_Score1 <- function(x, y) {\n  \n  # resultリストにxとyを格納\n  result <- list(Score1 = x, Score2 = y)\n  \n  # 以下は attr(result, \"class\") <- \"Score\" も可\n  class(result) <- \"Score\" # resultのクラスを\"Score\"とする\n  \n  result                   # resultを返す\n}\n\nMy_Score1 <- Make_Score1(x = rnorm(10, 50, 10),\n                         y = rnorm(10, 50, 10))\n\nMy_Score1 # My_Score1の内部を見る\n\n$Score1\n [1] 59.69953 43.85329 55.02336 45.61001 47.16679 50.98648 36.21005 54.12351\n [9] 57.61261 48.66611\n\n$Score2\n [1] 55.10768 40.38849 66.65466 44.25542 39.61994 47.82450 50.79352 36.29206\n [9] 42.78539 52.44530\n\nattr(,\"class\")\n[1] \"Score\"\n\nclass(My_Score1) # My_Score1のクラスを表示\n\n[1] \"Score\"\n\n\n　もう一つの方法はstructure()関数を使う方法です。sturcture()の第1引数に返すオブジェクト名を指定し、class = \"クラス名\"引数でクラスを指定します。\n\nMake_Score2 <- function(x, y) {\n  \n  # resultリストにxとyを格納\n  result <- list(Score1 = x, Score2 = y)\n  \n  structure(result, class = \"Score\") # resultを返す\n}\n\nMy_Score2 <- Make_Score2(x = rnorm(10, 50, 10),\n                         y = rnorm(10, 50, 10))\n\nMy_Score2 # My_Score2の内部を見る\n\n$Score1\n [1] 39.18877 30.29135 58.31219 47.29806 37.80779 40.90283 44.22559 45.58376\n [9] 45.11062 43.46584\n\n$Score2\n [1] 44.46645 55.65166 59.24438 47.41489 66.02514 38.53756 45.58103 65.23995\n [9] 54.74014 63.42466\n\nattr(,\"class\")\n[1] \"Score\"\n\nclass(My_Score2) # My_Score2のクラスを表示\n\n[1] \"Score\"\n\n\n　どれも同じ結果が得られます。\n\n\nメソッドの作り方\n\n既に存在する関数名を使う\n　先ほど作成しましたScoreクラスのオブジェクトは長さ2のリスト構造をしています。これらの要素それぞれの平均値を求める場合は、mean(My_Score1[[1]])とmean(My_Score1[[2]])を実行する必要があります。なぜなら、mean()はベクトルしか計算できないからです。ここではScoreクラスのオブジェクト要素それぞれの平均値を求める関数mean()を作成します。\n　しかし、問題があります。それはRにmean()関数が既に存在することです。ここで勝手に上書きするのは良くないでしょう。ここで出てくるのがメソッドです。Scoreクラスのメソッドは「Scoreクラス専用の関数」であり、通常のベクトルならR内蔵のmean()関数を、ScoreクラスのオブジェクトならScoreのメソッドであるmean()を実行します。\n　メソッドの作り方は自作関数と同じです。相違点としては関数名を関数名.クラス名にすることです。Scoreクラスのメソッドしてのmean()関数を定義する場合、関数名をmean.Scoreとします。\n\nmean.Score <- function(x) {\n  print(mean(x$Score1))\n  print(mean(x$Score2))\n}\n\nmean(c(1, 3, 5, 7, 9, 11)) # R内蔵関数のmean()を使う\n\n[1] 6\n\nmean(My_Score1) # Scoreクラスのメソッドであるmean()を使う\n\n[1] 49.89517\n[1] 47.6167\n\n\n　mean(c(1, 3, 5, 7, 9, 11))は引数がnumeric型ベクトルであるため、既存のmean()関数が使用されます。一方、mean(My_Score1)は引数がScoreクラスであるため、mean.Score()が使用されます。このようにmean_Score()のような別途の関数を作る必要なく、既存の関数名が利用できます。実際、methods(mean)を実行すると、Scoreクラスのメソッドとしてmean()関数が用意されたことを確認できます。\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.Score       mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\n\n新しい関数を作る\n　もし、新しい関数名を使用し、その関数が様々なクラスに対応するとしましょう。今回はCatというクラスを作ってみましょう。Catクラスの内部は長さ1のリストで、要素の名前はNameとし、ここには長さ1のcharacter型ベクトルが入ります。このCatクラスを作成する関数をMake_Cat()とします。\n\nMake_Cat <- function(name) {\n  \n  # resultリストにxを格納\n  result <- list(Name = name)\n  \n  structure(result, class = \"Cat\") # resultを返す\n}\n\nMy_Cat <- Make_Cat(name = \"矢内\")\nMy_Cat\n\n$Name\n[1] \"矢内\"\n\nattr(,\"class\")\n[1] \"Cat\"\n\nclass(My_Cat)\n\n[1] \"Cat\"\n\n\n　続いて、Catクラスに使うmy_func()を作成します。my_func()はそもそも存在しない関数ですので、普通にmy_func <- function()で作成可能です。この関数はCatのNameの後ろに\": にゃーにゃー\"を付けて出力する関数です。実際にやってみましょう。\n\nmy_func <- function(name) {\n  print(paste0(name$Name, \": にゃーにゃー\"))\n}\n\nmy_func(My_Cat)\n\n[1] \"矢内: にゃーにゃー\"\n\n\n　しかし、my_func()をCatクラス以外にも使いたい場合はどうすればいいでしょうか。普通にmy_func.クラス名()で良いでしょうか。確かにそうですが、その前に一つの手順が必要です。それは、my_func()をジェネリック関数として定義することです。この関数そのものは関数として機能はしませんが、「これからmy_func()がいろんなクラスのメソッドとして使われるぞ」と予め決めてくれます。ジェネリック関数を作成しないと関数名.クラス名は定義できません。そこで使うのがUseMethod()です。第一引数はメソッド名、第二引数は任意の引数ですが、通常、xが使われます。また、第二の引数は省略可能で、UseMethod(\"メソッド名\")でも動きます。\n\nmy_func <- function(x) {\n  UseMethod(\"my_func\", x)\n}\n\n　これからはmy_func.クラス名()の関数を作るだけです。まず、Score型オブジェクトに対してはそれぞれの要素の平均値を出力するとします。\n\nmy_func.Score <- function(x) {\n  print(mean(x$Score1))\n  print(mean(x$Score2))\n}\n\nmy_func.Cat <- function(cat) {\n  print(paste0(cat$Name, \": にゃーにゃー\"))\n}\n\n\nmethods(my_func)\n\n[1] my_func.Cat   my_func.Score\nsee '?methods' for accessing help and source code\n\n\n　my_func()関数はScoreとCatといった2種類のクラスで使われることが確認できます。それでは問題なく作動するかを確認してみましょう。My_Score1とMy_Catを、それぞれmy_func()に渡します。\n\nmy_func(My_Score1)\n\n[1] 49.89517\n[1] 47.6167\n\nmy_func(My_Cat)\n\n[1] \"矢内: にゃーにゃー\"\n\n\n　同じ関数名でも、オブジェクトのクラスによって異なる処理が行われることが分かります。\n\n\n\n検証用関数を作る\n　この作業は必須ではありませんが、今後、自分でパッケージ等を作ることになったら重要になるかも知れません。\n　最初の例でもお見せしましたが、Rでは事後的にクラスを変更することができます。強制的にクラスを変更した場合、そのクラスに属するメソッドを使うことができますが、エラーが生じてしまうでしょう。例えば、任意のcharacter型ベクトルMy_Cat2を作成し、Catクラスを付与してみましょう。\n\nMy_Cat2 <- \"宋\"\nclass(My_Cat2) <- \"Cat\"\nclass(My_Cat2)\n\n[1] \"Cat\"\n\n\n　My_Cat2のクラスはCatであるため、my_func.Cat()メソッドが使えます。しかし、my_func.Cat()仕組みを見る限り、うまく作動しないでしょう。\n\nmy_func(My_Cat2)\n\nError in cat$Name: $ operator is invalid for atomic vectors\n\n\n　間違った動作をするよりは、エラーが出て中断される方が良いですし、これで問題ないかも知れません。しかし、可能であれば、引数として使われたオブジェクトが、Catクラスか否かを厳密にチェックする機能があれば良いでしょう。カプセル化されている場合、クラスの定義時にデータの構造が厳密に定義されているため、このような手続きの必要性はあまりありませんが、カプセル化ができないRのS3クラスでは検証用関数（Validator）が必要です。\n　それではCatクラスの特徴をいくつか考えてみましょう。\n\nオブジェクトの中にはNameという要素のみがある。\nNameは長さ1のCharacter型ベクトルである。\n\n　以上の条件を全て満たしていればメソッドを実行し、一つでも満たさない場合はメソッドの実行を中止します。それでは検証用関数Validation_Cat()を作ってみましょう。\n\nValidation_Cat <- function(x) {\n  Message <- \"正しいCatクラスではありません。\"\n  \n  if (length(x) != 1) {\n    stop(Message)\n  } else if (is.null(names(x))) {\n    stop(Message)\n  } else if (names(x) != \"Name\"){\n    stop(Message)\n  } else if (length(x$Name) != 1 | class(x$Name) != \"character\") {\n    stop(Message)\n  }\n}\n\n　この検証用関数をmy_func.Cat()の最初に入れておきましょう。\n\nmy_func.Cat <- function(cat) {\n  Validation_Cat(cat)\n  \n  print(paste0(cat$Name, \": にゃーにゃー\"))\n}\n\n　それではMy_CatとMy_Cat2に対してmy_func()メソッドを実行してみます。\n\nmy_func(My_Cat)\n\n[1] \"矢内: にゃーにゃー\"\n\nmy_func(My_Cat2)\n\nError in Validation_Cat(cat): 正しいCatクラスではありません。\n\n\n　関数を実行する前に与えられたオブジェクトが正しいCatクラスか否かが判断され、パスされた場合のみ、メソッドが実行されることが分かります。もし、あるクラスで使用可能なメソッドが一つだけでしたら、検証用関数はメソッド内に直接書き込んでも良いですが、2つ以上のメソッドを持つ場合は別途の検証用関数を作成しておきましょう。"
  },
  {
    "objectID": "tutorial/R/opp_intro.html#例題",
    "href": "tutorial/R/opp_intro.html#例題",
    "title": "オブジェクト指向型プログラミング入門",
    "section": "例題",
    "text": "例題\n　ここでは2つのnumeric型ベクトルとそのベクトル名入力し、相関係数を求めるMy_Cor()関数を作ってみます。単に相関係数を求めるだけならcor()やcor.test()があるので、いくつかの機能も追加してみましょう。\n\n\n\n　たとえば、「1日当たりゲーム時間」と「身長」といった2つのnumeric型ベクトルをそれぞれxとyで入力し、x.nameとy.nameで各ベクトルの名前も指定します。また、入力されたデータを用いて相関係数とその信頼区間を求めます。これらのデータはリスト型として格納されますが、クラスを\"My_Cor_Object\"とします。以下はその例です。\n\nlibrary(tidyverse)\n\nCor_Obj <- My_Cor(x      = rnorm(20, 2, 0.5), \n                  y      = rnorm(20, 165, 6), \n                  x.name = \"1日当たりゲーム時間\", \n                  y.name = \"身長\")\n\nclass(Cor_Obj)\n\n[1] \"My_Cor_Object\"\n\n\n　このCor_Objの構造をstr()で確認してみます。\n\nstr(Cor_Obj)\n\nList of 4\n $ data    :'data.frame':   20 obs. of  2 variables:\n  ..$ x: num [1:20] 1.799 1.264 0.553 1.434 1.539 ...\n  ..$ y: num [1:20] 162 179 159 169 167 ...\n $ var_name: chr [1:2] \"1日当たりゲーム時間\" \"身長\"\n $ cor     : Named num -0.0794\n  ..- attr(*, \"names\")= chr \"cor\"\n $ cor_ci  : num [1:2] -0.504 0.376\n  ..- attr(*, \"conf.level\")= num 0.95\n - attr(*, \"class\")= chr \"My_Cor_Object\"\n\n\n　Cor_Objには元のデータがデータフレームとして格納され（$data）、それぞれの変数名（$var_name）、相関係数（$cor）、相関係数の95%信頼区間（$cor_ci）がCor_Objの中に入っています。本質的にはリスト型のデータ構造ですが、クラス名がMy_Cor_Objectになっているだけです。\n　このMy_Cor_Objectクラスには3つのメソッド（専用関数）が用意されており、print()、summary()、plot()です。print()とsummary()は同じ関数で、xとyの平均値、そして相関係数と信頼区間を出力します。plot()は散布図と相関係数を出力します。実際の例を見てみましょう。\n\nprint(Cor_Obj)\n\n1日当たりゲーム時間の平均値: 1.634\n身長の平均値: 164.547\n相関係数: -0.079 [-0.504, 0.376]\n\nsummary(Cor_Obj) # summary()はprint()と同じ\n\n1日当たりゲーム時間の平均値: 1.634\n身長の平均値: 164.547\n相関係数: -0.079 [-0.504, 0.376]\n\nplot(Cor_Obj)\n\n\n\n\n\n\n\n\n　既存のcor.test()で作成される\"htest\"クラスに比べ、\"My_Cor_Object\"クラスは各変数の平均値が名前と一緒に表示され、plot()で簡単に散布図が作成できる大変便利なクラスです。このMy_Cor_Objectクラスとそのメソッドの構造を図示したものが以下の図です。\n\n　それでは一つずつ作っていきましょう。まずは、\"My_Cor_Object\"クラスのオブジェクトを作成するMy_Cor()関数からです。\n\nMy_Cor <- function(x, y, x.name, y.name) {\n    if (!is.numeric(x) | !is.numeric(y)) {\n        stop(\"xまたはyがnumeric型ではありません。\")\n    }\n    if (length(x) != length(y)) {\n        stop(\"xとyは同じ長さでなかればなりません。\")\n    }\n    if (!is.character(x.name) | !is.character(y.name)) {\n        stop(\"x.nameまたはy.nameがcharacter型ではありません。\")\n    }\n    \n    data     <- data.frame(x = x, y = y)\n    var_name <- c(x.name, y.name)\n    cor      <- cor.test(x, y)$estimate\n    cor_ci   <- cor.test(x, y)$conf.int\n    \n    result   <- structure(list(data     = data, \n                               var_name = var_name, \n                               cor      = cor,\n                               cor_ci   = cor_ci),\n                          class = \"My_Cor_Object\")\n    \n    result\n}\n\n　最初の部分は入力されたデータがMy_Cor_Objectクラスに適した構造か否かを判断します。これは最初から想定外のMy_Cor_Objectクラスのオブジェクトが作成されることを防ぐことが目的です。むろん、R（S3）の性質上、事後的にクラスを変更することが可能ですから、検証用関数も作っておきます。ここでは以下の条件を検証します。\n\ndataという要素が存在し、2列である。\nvar_nameという要素が存在し、長さ2のcharacter型ベクトルである。\ncorという要素が存在し、長さ1のnumeric型ベクトルである。\ncor_ciという要素が存在し、長さ2のnumeric型ベクトルである。\n\n\nValidation <- function (x) {\n  UseMethod(\"Validation\", x)\n}\n\nValidation.My_Cor_Object <- function(x) {\n  Message <- \"正しいMy_Cor_Objectクラスではございません。\"\n  \n  if (is.null(x$data) | ncol(x$data) != 2) {\n    stop(Message)\n  }\n  if (is.null(x$var_name) | length(x$var_name) != 2 | class(x$var_name) != \"character\") {\n    stop(Message)\n  }\n  if (is.null(x$cor) | length(x$cor) != 1 | class(x$cor) != \"numeric\") {\n    stop(Message)\n  }\n  if (is.null(x$cor_ci) | length(x$cor_ci) != 2 | class(x$cor_ci) != \"numeric\") {\n    stop(Message)\n  }\n}\n\n　ここではValidation()をジェネリック関数として使用しました。自分が開発するパッケージで複数のクラスを提供する予定でしたら、このようなやり方が良いでしょう。\n　検証用関数は細かく書いた方が良いです。以上のValidation()もより細かくことが出来ます。たとえば、dataが2列か否かを判定するだけでなく、numeric型であるかなども判定した方が良いでしょう。\n　つづいて、My_Cor_Objectクラス用のprint()関数（メソッド）を作成します。\n\nprint.My_Cor_Object <- function(data) {\n    Valdation(data)\n    \n    cat(sprintf(\"%sの平均値: %.3f\\n\", \n                data$var_name[1],\n                mean(data$data$x)))\n    cat(sprintf(\"%sの平均値: %.3f\\n\", \n                data$var_name[2],\n                mean(data$data$y)))\n    cat(sprintf(\"相関係数: %.3f [%.3f, %.3f]\\n\", \n                data$cor, \n                data$cor_ci[1],\n                data$cor_ci[2]))\n}\n\n　次は、summary()メソッドですが、これはprint()と同じ機能をする関数です。この場合、UseMethod(\"メソッド名\")を使うと、指定したメソッドを使うことになります。\n\nsummary.My_Cor_Object <- function(data) {\n  UseMethod(\"print\")\n}\n\n　最後はplot()メソッドです。\n\nplot.My_Cor_Object <- function(data) {\n    Valdation(data)\n    \n    data$data %>%\n      ggplot(aes(x = x, y = y)) +\n      geom_point() +\n      labs(x = data$var_name[1], y = data$var_name[2]) +\n      ggtitle(sprintf(\"相関係数 = %.3f\", data[[\"cor\"]])) +\n      theme_minimal(base_family = \"HiraKakuProN-W3\")\n}\n\n　これで相関係数の計算および可視化が便利になる関数群が完成しました。Rパッケージの開発はこれよりも数倍も複雑ですが、本記事の内容はプログラミングをより効率的に行うための入り口となります。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html",
    "href": "tutorial/R/ggplot_intro4.html",
    "title": "可視化[発展]",
    "section": "",
    "text": "理論編では{ggplot2}の仕組みについて、基礎編ではよく使われる5種類のプロット（棒グラフ、散布図、折れ線グラフ、箱ひげ図、ヒストグラム）の作り方を、応用編ではスケール、座標系などの操作を通じたグラフの見た目調整について解説しました。本記事では基礎編の延長線上に位置づけることができ、紹介しきれなかった様々なグラフの作り方について簡単に解説します。本記事で紹介するグラフは以下の通りです。\n\nバイオリンプロット\nラグプロット\nリッジプロット\nエラーバー付き散布図\nロリーポップチャート\n平滑化ライン\n文字列の出力\nヒートマップ\n等高線図\n地図\n非巡回有向グラフ\nバンプチャート\n沖積図\nツリーマップ\nモザイクプロット\n\n\npacman::p_load(tidyverse)\n\nCountry_df <- read_csv(\"Data/Countries.csv\")\nCOVID19_df <- read_csv(\"Data/COVID19_Worldwide.csv\", guess_max = 10000)"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-violin",
    "href": "tutorial/R/ggplot_intro4.html#visual4-violin",
    "title": "可視化[発展]",
    "section": "バイオリンプロット",
    "text": "バイオリンプロット\n　バイオリンプロットは連続変数の分布を可視化する際に使用するプロットの一つです。理論編で紹介しましたヒストグラムや箱ひげ図と目的は同じです。それではバイオリンプロットとは何かについて例を見ながら解説します。\n　以下の図は対数化した一人当たり購買力平価GDP（PPP_per_capita）のヒストグラムです。\n\n\n\n\n\n\n\n\n\n　このヒストグラムをなめらかにすると以下のような図になります。\n\n\n\n\n\n\n\n\n\n　この密度曲線を上下対称にすると以下のような図となり、これがバイオリンプロットです。ヒストグラムのようにデータの分布が分かりやすくなります。\n\n\n\n\n\n\n\n\n\n　しかし、この図の場合、ヒストグラムと同様、中央値や四分位数などの情報が含まれておりません。これらの箱ひげ図を使用した方が良いでしょう。バイオリンプロットの良い点はバイオリンの中に箱ひげ図を入れ、ヒストグラムと箱ひげ図両方の長所を取ることができる点です。たとえば、バイオリンプロットを90度回転させ、中にバイオリン図を入れると以下のようになります。\n\n\n\n\n\n\n\n\n\n　それでは実際にバイオリンプロットを作ってみましょう。使い方は箱ひげ図（geom_boxplot()）と同じです。たとえば、横軸は大陸（Continent）に、縦軸は対数化した一人当たり購買力平価GDP（PPP_per_capita）にしたバイオリンプロットを作るには作るにはgeom_violin()幾何オブジェクトの中にマッピングするだけです。大陸ごとに色分けしたい場合はfill引数にContinentをマッピングします。\n\nCountry_df %>%\n    ggplot() +\n    geom_violin(aes(x = Continent, y = PPP_per_capita, fill = Continent)) +\n    labs(x = \"大陸\", y = \"一人当たり購買力平価GDP (対数)\") +\n    scale_y_continuous(breaks = c(0, 1000, 10000, 100000),\n                       labels = c(0, 1000, 10000, 100000),\n                       trans  = \"log10\") + # y軸を対数化\n    guides(fill = \"none\") + # fillのマッピング情報の凡例を隠す\n    theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n　ここに箱ひげ図も載せたい場合は、geom_violin()オブジェクトの後にgeom_boxplot()オブジェクトを入れるだけで十分です。\n\nCountry_df %>%\n    ggplot() +\n    geom_violin(aes(x = Continent, y = PPP_per_capita, fill = Continent)) +\n    geom_boxplot(aes(x = Continent, y = PPP_per_capita),\n                 width = 0.2) +\n    labs(x = \"大陸\", y = \"一人当たり購買力平価GDP (対数)\") +\n    scale_y_continuous(breaks = c(0, 1000, 10000, 100000),\n                       labels = c(0, 1000, 10000, 100000),\n                       trans = \"log10\") +\n    guides(fill = \"none\") +\n    theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n　箱ひげ図は四分位範囲、四分位数、最小値、最大値などの情報を素早く読み取れますが、どの値当たりが分厚いかなどの情報が欠けています。これをバイオリンプロットで補うことで、よりデータの分布を的確に把握することができます。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-rug",
    "href": "tutorial/R/ggplot_intro4.html#visual4-rug",
    "title": "可視化[発展]",
    "section": "ラグプロット",
    "text": "ラグプロット\n　ラグプロット（rug plot）は変数の分布を示す点ではヒストグラム、箱ひげ図、バイオリンプロットと同じ目的を持ちますが、大きな違いとしてはラグプロット単体で使われるケースがない（または、非常に稀）という点です。ラグプロットは上述しましたヒストグラムや箱ひげ図、または散布図などと組み合わせて使うのが一般的です。\n　以下はCountry_dfのPPP_per_capita（常用対数変換）のヒストグラムです。\n\n\n\n\n\n\n\n\n\n　一変数の分布を確認する場合、ヒストグラムは情報量の損失が少ない方です。それでも値一つ一つの情報は失われますね。例えば、上記のヒストグラムで左端の度数は1です。左端の棒の区間はおおよそ500から780であり、一人当たりPPPがこの区間に属する国は1カ国ということです。ちなみに、その国はブルンジ共和国ですが、ブルンジ共和国の具体的な一人当たりPPPはヒストグラムから分かりません。情報量をより豊富に持たせるためには区間を細かく刻むことも出来ますが、逆に分布の全体像が読みにくくなります。\n　ここで登場するのがラグプロットです。これは座標平面の端を使ってデータを一時現状に並べたものです。多くの場合、点ではなく、垂直線（｜）を使います。ラグプロットの幾何オブジェクトは{ggplot2}でデフォルトで提供されており、geom_rug()を使います。マッピングはxまたはyに対して行いますが、座標平面の下段にラグプロットを出力する場合はxに変数（ここではPPP_per_capita）をマッピングします。\n\n\n\n\n\n\n\n\n\nラグプロットを使うと本来のヒストグラムの外見にほぼ影響を与えず、更に情報を付け加えることが可能です。点（｜）の密度でデータの分布を確認することもできますが、その密度の相対的な比較に関してはヒストグラムの方が良いでしょう。\nラグプロットは散布図に使うことも可能です。散布図は一つ一つの点が具体的な値がマッピングされるため、情報量の損失はほぼないでしょう。それでも散布図にラグプロットを加える意味はあります。まず、Country_dfのフリーダムハウス指数（FH_Total）と一人当たりPPP（PPP_per_capita）の散布図を作ってみましょう。\n\n\n\n\n\n\n\n\n\n散布図の目的は二変量間の関係を確認することであって、それぞれの変数の分布を確認することではありません。もし、FH_TotalとPPP_per_capitaの分布が確認したいなら、それぞれのヒストグラムや箱ひげ図を作成した方が良いでしょう。しかし、ラグプロットを使えば、点（｜）の密度で大まかな分布は確認出来ますし、図の見た目にもほぼ影響を与えません。\n横軸と縦軸両方のラグプロットは、geom_rug()にxとy両方マッピングするだけです。\n\n\n\n\n\n\n\n\n\n　これでFH_Totalはほぼ均等に分布していて、PPP_per_capitaは2万ドル以下に多く密集していることが確認できます。\n　{ggExtra}のggMarginal()を使えば、ラグプロットでなく、箱ひげ図やヒストグラムを付けることも可能です。{ggplot2}で作図した図をオブジェクトとして格納し、ggMarginal()の第一引数として指定します。第一引数のみだと密度のだけ出力されるため、箱ひげ図を付けるためにはtype = \"boxplot\"を指定します（既定値は\"density\"）。ヒストグラムを出力する場合は\"histogram\"と指定します。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-ridge",
    "href": "tutorial/R/ggplot_intro4.html#visual4-ridge",
    "title": "可視化[発展]",
    "section": "リッジプロット",
    "text": "リッジプロット\n　リッジプロット（ridge plot）はある変数の分布をグループごとに出力する図です。大陸ごとの人間開発指数の分布を示したり、時系列データなら分布の変化を示す時にも使えます。ここでは大陸ごとの人間開発指数の分布をリッジプロットで示してみましょう。\n　リッジプロットを作成する前に、geom_density()幾何オブジェクトを用い、変数の密度曲線（density curve）を作ってみます。マッピングはxに対し、分布を出力する変数名を指定します。また、密度曲線内部に色塗り（fill）をし、曲線を計算する際のバンド幅（bw）は0.054にします。bwが大きいほど、なめらかな曲線になります。\n\nCountry_df %>%\n  ggplot() +\n  geom_density(aes(x = HDI_2018), fill = \"gray70\", bw = 0.054) +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\nWarning: Removed 6 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\n\n\n　これを大陸ごとに出力する場合、ファセット分割を行います。今回は大陸ごとに1列（ncol = 1）でファセットを分割します。\n\nCountry_df %>%\n  ggplot() +\n  geom_density(aes(x = HDI_2018), fill = \"gray70\", bw = 0.054) +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  facet_wrap(~Continent, ncol = 1) +\n  theme_minimal(base_size = 12)\n\nWarning: Removed 6 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\n\n\n　それでは上のグラフをリッジプロットとして作図してみましょう。今回は{ggridges}パッケージを使います。\n\npacman::p_load(ggridges)\n\n　使用する幾何オブジェクトはgeom_density_ridges()です。似たような幾何オブジェクトとしてgeom_ridgeline()がありますが、こちらは予め密度曲線の高さを計算しておく必要があります。一方、geom_density_ridges()は変数だけ指定すれば密度を自動的に計算してくれます。マッピングはxとyに対し、それぞれ分布を出力する変数名とグループ変数名を指定します。また、密度曲線が重なるケースもあるため、透明度（alpha）も0.5にしておきましょう。ここでは別途指定しませんが、ハンド幅も指定可能であり、aes()の外側にbandwidthを指定するだけです。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges(aes(x = HDI_2018, y = Continent), \n                      alpha = 0.5) +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\nPicking joint bandwidth of 0.054\n\n\nWarning: Removed 6 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\n\n\n\n\n先ほど作図した図と非常に似た図が出来上がりました。ファセット分割に比べ、空間を最大限に活用していることが分かります。ファセットラベルがなく、グループ名が縦軸上に位置するからです。また、リッジプロットの特徴は密度曲線がオーバラップする点ですが、以下のようにscale = 1を指定すると、オーバラップなしで作成することも可能です。もし、scale = 3にすると最大2つの密度曲線が重なることになります。たとえば最下段のアフリカはアメリカの行と若干オーバラップしていますが、scale = 3の場合、アジアの行までオーバーラップされうることになります。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges(aes(x = HDI_2018, y = Continent), \n                      scale = 1, alpha = 0.5) +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\nPicking joint bandwidth of 0.054\n\n\nWarning: Removed 6 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\n\n\n\n\n　また、横軸の値に応じて背景の色をグラデーションで表現することも可能です。この場合、geom_density_ridges()幾何オブジェクトでなく、geom_density_ridges_gradient()を使い、fillにもマッピングをする必要があります。横軸（x）の値に応じて色塗りをする場合、fill = stat(x)とします。デフォルトでは横軸の値が高いほど空色、低いほど黒になります。ここでは高いほど黄色、低いほど紫ににするため、色弱にも優しいscale_fill_viridis_c()を使い1、カラーオプションはplasmaにします（option = \"c\"）。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges_gradient(aes(x = HDI_2018, y = Continent, fill = stat(x)), \n                               alpha = 0.5) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\", fill = \"2018年人間開発指数\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\nPicking joint bandwidth of 0.054\n\n\n\n\n\n\n\n\n\n密度曲線は基本的にはなめらかな曲線であるため、データが存在しない箇所にも密度が高く見積もられるケースがあります。全体的な分布を俯瞰するには良いですが、情報の損失は避けられません。そこで出てくるのが点付きのリッジプロットです。HDI_2018の個々の値を点で出力するにはjittered_points = TRUEを指定するだけです。これだけで密度曲線の内側に点が若干のズレ付き（jitter）で出力されます。ただし、密度曲線がオーバーラップされるリッジプロットの特徴を考えると、グループごとに点の色分けをする必要があります（同じ色になると、どのグループの点かが分からなくなるので）。この場合、point_colorに対し、グループ変数（Continent）をマッピングします。また、密度曲線の色と合わせるために密度曲線の色塗りもfillで指定します。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges(aes(x = HDI_2018, y = Continent, fill = Continent,\n                          point_color = Continent), \n                      alpha = 0.5, jittered_points = TRUE) +\n  guides(fill = \"none\", point_color = \"none\") + # 凡例を削除\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\nPicking joint bandwidth of 0.054\n\n\nWarning: Removed 6 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\n\n\n\n\n　他にも密度曲線の下側にラグプロットを付けることも可能です。こうすれば点ごとに色訳をする必要もなくなります。ラグプロットを付けるためには点の形（point_shape）を「|」にする必要があります。ただ、これだけだと「|」が密度曲線内部に散らばる（jittered）だけです。散らばりをなくす、つまり密度曲線の下段に固定する必要があり、これはaes()その外側にposition = position_points_jitter(width = 0, height = 0)を指定することで出来ます。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges(aes(x = HDI_2018, y = Continent, fill = Continent), \n                      alpha = 0.5, jittered_points = TRUE,\n                      position = position_points_jitter(width = 0, height = 0),\n                      point_shape = \"|\", point_size = 3) +\n  guides(fill = \"none\") +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\nPicking joint bandwidth of 0.054\n\n\nWarning: Removed 6 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\n\n\n\n\n　最後に密度曲線でなく、ヒストグラムで示す方法を紹介します。これはgeom_density_ridges()の内部にstat = \"binline\"を指定するだけです。\n\nCountry_df %>%\n  ggplot() +\n  geom_density_ridges(aes(x = HDI_2018, y = Continent), alpha = 0.5,\n                      stat = \"binline\") +\n  labs(x = \"2018年人間開発指数\", y = \"大陸\") +\n  theme_minimal(base_size = 12)\n\n`stat_binline()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 6 rows containing non-finite values (stat_binline)."
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-pointrange",
    "href": "tutorial/R/ggplot_intro4.html#visual4-pointrange",
    "title": "可視化[発展]",
    "section": "エラーバー付き散布図",
    "text": "エラーバー付き散布図\n　エラーバー付きの散布図は推定結果の点推定値とその不確実性（信頼区間など）を示す際によく使われる図です。以下の表はCountry_dfを用い、大陸（オセアニアを除く）ごとにフリーダムハウス・スコア（FH_Total）を一人当たりPPP GDP（PPP_per_capita）に回帰させた分析から得られたフリーダムハウス・スコア（FH_Total）の係数（以下の式の\\(\\beta_1\\)）の点推定値と95%信頼区間です。\n\\[\n\\text{PPP per capita} = \\beta_0 + \\beta_1 \\cdot \\text{FH}\\_\\text{Total} + \\varepsilon\n\\]\n\n\n\n\nPointrange_df <- tibble(\n    Continent = c(\"Asia\", \"Europe\", \"Africa\", \"America\"),\n    Coef      = c(65.3, 588.0, 53.4, 316.0),\n    Conf_lwr  = c(-250.0, 376.0, -14.5, 128.0),\n    Conf_upr  = c(380.0, 801.0, 121.0, 504.0)\n)\n\n\nPointrange_df\n\n# A tibble: 4 × 4\n# Groups:   Continent [4]\n  Continent  Coef Conf_lwr Conf_upr\n  <chr>     <dbl>    <dbl>    <dbl>\n1 Asia       65.3   -250.      380.\n2 Europe    588.     376.      801.\n3 Africa     53.4    -14.5     121.\n4 America   316.     128.      504.\n\n\n　実は以上のデータは以下のようなコードで作成されています。{purrr}パッケージの使い方に慣れる必要があるので、purrr入門を参照してください。\n\nPointrange_df <- Country_df %>%\n    filter(Continent != \"Oceania\") %>%\n    group_by(Continent) %>%\n    nest() %>%\n    mutate(Fit = map(data, ~lm(PPP_per_capita ~ FH_Total, data = .)),\n           Est = map(Fit, broom::tidy, conf.int = TRUE)) %>%\n    unnest(Est) %>%\n    filter(term == \"FH_Total\") %>%\n    select(Continent, Coef = estimate, \n           Conf_lwr = conf.low, Conf_upr = conf.high) \n\nこのPointrange_dfを用いて横軸は大陸（Continent）、縦軸には点推定値（Coef）と95%信頼区間（Conf_lwrとConf_upr）を出力します。ここで使う幾何オブジェクトはgeom_pointrange()です。横軸xと点推定値y、95%信頼区間の下限のymin、上限のymaxにマッピングします。エラーバー付き散布図を立てに並べたい場合はyとx、xmin、xmaxにマッピングします。\n\nPointrange_df %>%\n    ggplot() +\n    geom_hline(yintercept = 0, linetype = 2) +\n    geom_pointrange(aes(x = Continent, y = Coef, \n                        ymin = Conf_lwr, ymax = Conf_upr),\n                    size = 0.75) +\n    labs(y = expression(paste(beta[1], \" with 95% CI\"))) +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nここでもう一つの次元を追加することもあるでしょう。たとえば、複数のモデルを比較した場合がそうかもしれません。以下のPointrange_df2について考えてみましょう。\n\n\n\n\nPointrange_df2 <- tibble(\n    Continent = rep(c(\"Asia\", \"Europe\", \"Africa\", \"America\"), each = 2),\n    Term      = rep(c(\"Civic Liverty\", \"Political Right\"), 4),\n    Coef      = c(207.747, 29.188, 1050.164, 1284.101,\n                  110.025, 93.537, 581.4593, 646.9211),\n    Conf_lwr  = c(-385.221, -609.771, 692.204, 768.209,\n                  -12.648, -53.982, 262.056, 201.511),\n    Conf_upr  = c(800.716, 668.147, 1408.125, 1801.994,\n                  232.697, 241.057, 900.863, 1092.331))\n\n\nPointrange_df2\n\n# A tibble: 8 × 5\n# Groups:   Continent [4]\n  Continent Term              Coef Conf_lwr Conf_upr\n  <chr>     <chr>            <dbl>    <dbl>    <dbl>\n1 Asia      Civic Liberty    208.    -385.      801.\n2 Asia      Political Right   29.2   -610.      668.\n3 Europe    Civic Liberty   1050.     692.     1408.\n4 Europe    Political Right 1285.     768.     1802.\n5 Africa    Civic Liberty    110.     -12.6     233.\n6 Africa    Political Right   93.5    -54.0     241.\n7 America   Civic Liberty    581.     262.      901.\n8 America   Political Right  647.     202.     1092.\n\n\nこのデータは以下の2つのモデルを大陸ごとに推定した\\(\\beta_1\\)と\\(\\gamma_1\\)の点推定値と95%信頼区間です。\n\\[\n\\begin{aligned}\n\\text{PPP per capita} & = \\beta_0 + \\beta_1 \\cdot \\text{FH}\\_\\text{CL} + \\varepsilon \\\\\n\\text{PPP per capita} & = \\gamma_0 + \\gamma_1 \\cdot \\text{FH}\\_\\text{PR} + \\upsilon\n\\end{aligned}\n\\]\n　どの説明変数を用いたかでエラーバーと点の色分けを行う場合、colorに対してTermをマッピングします。\n\nPointrange_df2 %>%\n    ggplot() +\n    geom_hline(yintercept = 0, linetype = 2) +\n    geom_pointrange(aes(x = Continent, y = Coef, \n                        ymin = Conf_lwr, ymax = Conf_upr,\n                        color = Term), size = 0.75) +\n    labs(y = expression(paste(beta[1], \" and \", gamma[1], \" with 95% CI\")),\n         color = \"\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　何か違いますね。この2つのエラーバーと点の位置をずらす必要があるようです。これは3次元以上の棒グラフで使ったposition引数で調整可能です。今回は実引数としてposition_dodge(0.5)を指定してみましょう。\n\nPointrange_df2 %>%\n    ggplot() +\n    geom_hline(yintercept = 0, linetype = 2) +\n    geom_pointrange(aes(x = Continent, y = Coef, \n                        ymin = Conf_lwr, ymax = Conf_upr,\n                        color = Term),\n                    size = 0.75, position = position_dodge(0.5)) +\n    labs(y = expression(paste(beta[1], \" and \", gamma[1], \" with 95% CI\")),\n         color = \"\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　これで完成です。更に、\\(\\alpha = 0.05\\)水準で統計的に有意か否かを透明度で示し、透明度の凡例を非表示にしてみましょう。\\(\\alpha = 0.05\\)水準で統計的に有意か否かは95%信頼区間の上限と下限の積が0より大きいか否かで判定できます。ggplot()にデータを渡す前に統計的有意か否かを意味するSig変数を作成し、geom_pointrage()の内部ではalphaにSigをマッピングします。\n\nPointrange_df2 %>%\n    mutate(Sig = if_else(Conf_lwr * Conf_upr > 0, \n                         \"Significant\", \"Insignificant\")) %>%\n    ggplot() +\n    geom_hline(yintercept = 0, linetype = 2) +\n    geom_pointrange(aes(x = Continent, y = Coef, \n                        ymin = Conf_lwr, ymax = Conf_upr,\n                        color = Term, alpha = Sig),\n                    size = 0.75, position = position_dodge(0.5)) +\n    labs(y = expression(paste(beta[1], \" and \", gamma[1], \" with 95% CI\")),\n         color = \"\") +\n    scale_alpha_manual(values = c(\"Significant\" = 1, \"Insignificant\" = 0.35)) +\n    guides(alpha = FALSE) +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n\"none\")` instead."
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-lollipop",
    "href": "tutorial/R/ggplot_intro4.html#visual4-lollipop",
    "title": "可視化[発展]",
    "section": "ロリーポップチャート",
    "text": "ロリーポップチャート\n　ロリーポップチャートは棒グラフの特殊な形態であり、棒がロリーポップ（チュッパチャップス）の形をしているものを指します。したがって、2つの図は本質的に同じですが、棒が多い場合はロリーポップチャートを使うケースがあります。棒が非常に多い棒グラフの場合、図を不適切に縮小するとモアレが生じるケースがあるからです。\n　まず、Country_dfを用い、ヨーロッパ諸国の一人当たりPPP GDP（PPP_per_capita）の棒グラフを作るとします。PPP_per_capitaが欠損していないヨーロッパの国は46行であり、非常に棒が多い棒グラフになります。\n\nCountry_df %>%\n  filter(Continent == \"Europe\") %>%\n  drop_na(PPP_per_capita) %>%\n  ggplot() +\n  geom_bar(aes(y = Country, x = PPP_per_capita), stat = \"identity\") +\n  labs(x = \"一人あたり購買力平価GDP\", y = \"国\") +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nここで登場するのがロリーポップチャートです。ロリーポップチャートの構成要素は棒とキャンディーの部分です。棒は線になるためgeom_segement()を、キャンディーは散布図geom_point()を使います。散布図については既に基礎編で説明しましたので、ここではgeom_segment()について説明します。\n　geom_segment()は直線を引く幾何オブジェクトであり、線の起点（xとy）と終点（xendとyend）に対してマッピングをする必要があります。横軸上の起点は0、縦軸上の起点はCountryです。そして横軸上の終点はPPP_per_capita、縦軸上のそれはCountryです。縦軸上の起点と終点が同じということは水平線を引くことになります。\n　geom_segment()で水平線を描いたら、次は散布図をオーバーラップさせます。点の横軸上の位置はPPP_per_capita、縦軸上の位置はCountryです。\n\nCountry_df %>%\n  filter(Continent == \"Europe\") %>%\n  drop_na(PPP_per_capita) %>%\n  ggplot() +\n  geom_segment(aes(y = Country, yend = Country,\n                   x = 0, xend = PPP_per_capita)) +\n  geom_point(aes(y = Country, x = PPP_per_capita), color = \"orange\") +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid.major.y = element_blank(),\n        panel.border = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n　これで完成です。もし一人当たりPPP GDP順で並べ替えたい場合はfct_reorder()を使います。CountryをPPP_per_capitaの低い方を先にくるようにするなら、fct_reorder(Country, PPP_per_capita)です。縦に並ぶの棒グラフなら最初に来る水準が下に位置されます。もし、順番を逆にしたいなら、更にfct_rev()で水準の順番を逆転させます。\n\nCountry_df %>%\n  filter(Continent == \"Europe\") %>%\n  drop_na(PPP_per_capita) %>%\n  mutate(Country = fct_reorder(Country, PPP_per_capita)) %>% \n  ggplot() +\n  geom_segment(aes(y = Country, yend = Country,\n                   x = 0, xend = PPP_per_capita)) +\n  geom_point(aes(y = Country, x = PPP_per_capita), color = \"orange\") +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid.major.y = element_blank(),\n        panel.border = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n　ロリーポップロリーポップチャートで次元を追加するには点（キャンディー）の色分けが考えられます。たとえば、OECD加盟国か否かの次元を追加する場合、geom_point()においてcolorをマッピングするだけです。\n\nCountry_df %>%\n  filter(Continent == \"Europe\") %>%\n  drop_na(PPP_per_capita) %>%\n  mutate(Country = fct_reorder(Country, PPP_per_capita),\n         OECD    = if_else(OECD == 1, \"OECD\", \"non-OECD\"),\n         OECD    = factor(OECD, levels = c(\"OECD\", \"non-OECD\"))) %>% \n  ggplot() +\n  geom_segment(aes(y = Country, yend = Country,\n                   x = 0, xend = PPP_per_capita)) +\n  geom_point(aes(y = Country, x = PPP_per_capita, color = OECD)) +\n  scale_color_manual(values = c(\"OECD\" = \"orange\", \"non-OECD\" = \"royalblue\")) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"国\", color = \"\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid.major.y = element_blank(),\n        panel.border       = element_blank(),\n        axis.ticks.y       = element_blank(),\n        legend.position    = \"bottom\")\n\n\n\n\n\n\n\n\n　ファセット分割ももちろんできますが、この場合、OECD加盟国の一人当たりPPP GDPが相対的に高いことを示すなら、一つのファセットにまとめた方が良いでしょう。\n　以下のようにロリーポップを横に並べることもできますが、棒の数が多いケースがほとんどであるロリーポップチャートではラベルの回転が必要になるため、読みにくくなるかも知れません。\n\nCountry_df %>%\n  filter(Continent == \"Europe\") %>%\n  drop_na(PPP_per_capita) %>%\n  mutate(Country = fct_reorder(Country, PPP_per_capita),\n         Country = fct_rev(Country)) %>% \n  ggplot() +\n  geom_segment(aes(x = Country, xend = Country,\n                   y = 0, yend = PPP_per_capita)) +\n  geom_point(aes(x = Country, y = PPP_per_capita), color = \"orange\") +\n  labs(x = \"国\", y = \"一人あたり購買力平価GDP (USD)\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid.major.y = element_blank(),\n        panel.border = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-smooth",
    "href": "tutorial/R/ggplot_intro4.html#visual4-smooth",
    "title": "可視化[発展]",
    "section": "平滑化ライン",
    "text": "平滑化ライン\n2次元平面上に散布図をプロットし、二変数間の関係を一本の線で要約するのは平滑化ラインです。{ggplot2}ではgeom_smooth()幾何オブジェクトを重ねることで簡単に平滑化ラインをプロットすることができます。まずは、横軸をフリーダムハウス・スコア（FH_Total）、縦軸を一人当たり購買力平価GDP（PPP_per_capita）にした散布図を出力し、その上に平滑化ラインを追加してみましょう。geom_smooth()にもマッピングが必要で、aes()の内部にxとyをマッピングします。今回はgeom_point()とgeom_smooth()が同じマッピング情報を共有するため、ggplot()内部でマッピングします。\n\nCountry_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth()  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　青い線が平滑化ライン、網掛けの領域が95%信頼区間です。この線はLOESS (LOcal Estimated Scatterplot Smoothing)と呼ばれる非線形平滑化ラインです。どのようなラインを引くかはmethod引数で指定しますが、このmethod既定値が\"loess\"です。これを見るとフリーダムハウス・スコアが75以下の国では国の自由度と所得間の関係があまり見られませんが、75からは正の関係が確認できます。\n　LOESS平滑化の場合、span引数を使って滑らかさを調整することができます。spanの既定値は0.75ですが、これが小さいほど散布図によりフィットしたラインが引かれ、よりギザギザな線になります。たとえば、spanを0.25にすると以下のようなグラフが得られます。\n\nCountry_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", span = 0.25)  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　他にも定番の回帰直線を引くこともできます。methodの実引数を\"lm\"に変えるだけです。\n\nCountry_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　信頼区間は既定値だと95%信頼区間が表示されますが、level引数で調整することができます。たとえば、99.9%信頼区間を表示したい場合、level = 0.999を指定します。\n\nCountry_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", level = 0.999)  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　信頼区間を消したい場合はse = FALSEを指定します（既定値はTRUE）。\n\nCountry_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　最後にデータのサブセットごとに回帰直線を引く方法について説明します。散布図で色分けを行う場合、aes()内でcolor引数を指定しますが、これだけで十分です。今回はこれまでの散布図をOECD加盟有無ごとに色分けし、それぞれ別の回帰直線を重ねてみましょう。回帰直線も色分けしたいのでcolor引数で次元を増やす必要があり、これはgeom_point()と共通であるため、ggplot()内でマッピングします。\n\nCountry_df %>%\n    mutate(OECD = if_else(OECD == 1, \"加盟国\", \"非加盟国\")) %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita, color = OECD)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    coord_cartesian(ylim = c(0, 120000)) +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　これを見ると、国の自由度と所得の間に関係が見られるのはOECD加盟国で、非加盟国では非常に関係が弱いことが分かります。\n　あまりいい方法ではないと思いますが、散布図は色（color）で分け、回帰直線は線の種類（linetype）で分けるならどうすれば良いでしょうか。この場合はcolorはgeom_point()内部で、linetypeはgeom_smooth()でマッピングします。\n\nCountry_df %>%\n    mutate(OECD = if_else(OECD == 1, \"加盟国\", \"非加盟国\")) %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point(aes(color = OECD)) +\n    geom_smooth(aes(linetype = OECD), method = \"lm\", color = \"black\")  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    coord_cartesian(ylim = c(0, 120000)) +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　{ggplot2}が提供する平滑化ラインにはLOESSと回帰直線以外にも\"glm\"や\"gam\"などがります。詳細はRコンソール上で?geom_smoothを入力し、ヘルプを参照してください。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-text",
    "href": "tutorial/R/ggplot_intro4.html#visual4-text",
    "title": "可視化[発展]",
    "section": "文字列の出力",
    "text": "文字列の出力\ngeom_text(): マッピングされたxとyの箇所に文字列を付ける\n\nbar_plot <- Country_df %>%\n  group_by(Continent) %>%\n  summarise(HDI = mean(HDI_2018, na.rm = TRUE)) %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = HDI), stat = \"identity\") +\n  labs(x = \"Continent\", y = \"Mean of\\nHuman Development Index (2018)\") +\n  theme_minimal(base_size = 12)\n\nbar_plot +\n  geom_text(aes(x = Continent, y = HDI, label = HDI),\n            size = 4)\n\n\n\n\n\n\n\n\n文字の位置をやや高めにし、平均値の小数点を3桁に丸める。\nただし、round()を使う場合、round(1.1298, 3)は1.13と出力される。もし、1.130のように出力したい場合はround()の代わりにsprintf()を使用する。今回の例の場合、sprintf(\"%.3f\", 1.1298)のように書く。\n\nbar_plot +\n  geom_text(aes(x = Continent, y = HDI + 0.03, label = round(HDI, 3)),\n            size = 4)\n\n\n\n\n\n\n\n\ngeom_label(): マッピングされたxとyの箇所にラベルを付ける\n\nbar_plot +\n  geom_label(aes(x = Continent, y = HDI, label = round(HDI, 3)),\n             size = 4)\n\n\n\n\n\n\n\n\nannotate(): 任意の箇所に文字列 or ラベルを追加\n\"text\"の代わりに\"label\"をすると、ラベル形式になる。\n\nlm_plot <- Country_df %>%\n    ggplot(aes(x = FH_Total, y = PPP_per_capita)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", level = 0.999)  +\n    labs(x = \"Freedom House Score\", y = \"PPP per capita (USD)\") +\n    scale_y_continuous(breaks = seq(0, 120000, by = 10000),\n                       labels = seq(0, 120000, by = 10000)) +\n    theme_minimal(base_size = 12)\n\nlm_plot +\n  annotate(\"text\", x = 0, y = 110000, label = \"注: 青い線は回帰直線を表す。\", \n           hjust = 0, size = 5)\n\n\n\n\n\n\n\n\n数式: {latex2exp}\nLaTeXの数式表記をほぼそのまま使えるが、\\は\\\\に置換すること。\n\npacman::p_load(latex2exp)\n\nlm_plot +\n  annotate(\"text\", x = 0, y = 110000, \n           label = TeX(\"$PPP\\\\_per\\\\_capita = \\\\alpha + \\\\beta \\\\cdot Freedom\\\\_House\\\\_Score + \\\\epsilon \\\\ where \\\\ \\\\epsilon \\\\sim Normal(0, \\\\sigma).$\"), \n           hjust = 0, size = 5)"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-heatmap",
    "href": "tutorial/R/ggplot_intro4.html#visual4-heatmap",
    "title": "可視化[発展]",
    "section": "ヒートマップ",
    "text": "ヒートマップ\n\n2つの離散変数の分布を表すヒートマップ\n　ヒートマップ（heat map）には2つの使い方があります。まずは、離散変数\\(\\times\\)離散変数の同時分布を示す時です。これは後ほど紹介するモザイク・プロットと目的は同じですが、モザイク・プロットはセルの面積で密度や度数を表すに対し、ヒートマップは主に色で密度や度数を表します。\n　ここでは一人当たり購買力平価GDP（PPP_per_capita）を「1万ドル未満」、「1万ドル以上・2万ドル未満」、「2万ドル以上、3万ドル未満」、「3万ドル以上」の離散変数に変換し、大陸ごとの国家数をヒートマップとして示してみたいと思います。まずは、変数のリコーディングをし、全てfactor化します。最後に国家名（Country）、大陸（Continent）、所得（Income）、フリーダム・ハウス・スコア（FH_Total）、人間開発指数（HDI_2018）列のみ抽出し、Heatmap_dfという名のオブジェクトとして格納しておきます。\n\nHeatmap_df <- Country_df %>%\n  filter(!is.na(PPP_per_capita)) %>%\n  mutate(Continent = recode(Continent,\n                            \"Africa\"  = \"アフリカ\",\n                            \"America\" = \"アメリカ\",\n                            \"Asia\"    = \"アジア\",\n                            \"Europe\"  = \"ヨーロッパ\",\n                            .default  = \"オセアニア\"),\n         Continent = factor(Continent, levels = c(\"アフリカ\", \"アメリカ\", \"アジア\", \n                                                  \"ヨーロッパ\", \"オセアニア\")),\n         Income    = case_when(PPP_per_capita < 10000 ~ \"1万ドル未満\",\n                               PPP_per_capita < 20000 ~ \"1万ドル以上\\n2万ドル未満\",\n                               PPP_per_capita < 30000 ~ \"2万ドル以上\\n3万ドル未満\",\n                               TRUE                   ~ \"3万ドル以上\"),\n         Income    = factor(Income, levels = c(\"1万ドル未満\", \"1万ドル以上\\n2万ドル未満\",\n                                               \"2万ドル以上\\n3万ドル未満\", \"3万ドル以上\"))) %>%\n  select(Country, Continent, Income, FH_Total, HDI_2018)\n\nHeatmap_df\n\n# A tibble: 178 × 5\n   Country             Continent  Income                     FH_Total HDI_2018\n   <chr>               <fct>      <fct>                         <dbl>    <dbl>\n 1 Afghanistan         アジア     \"1万ドル未満\"                    27    0.496\n 2 Albania             ヨーロッパ \"1万ドル以上\\n2万ドル未満\"       67    0.791\n 3 Algeria             アフリカ   \"1万ドル以上\\n2万ドル未満\"       34    0.759\n 4 Angola              アフリカ   \"1万ドル未満\"                    32    0.574\n 5 Antigua and Barbuda アメリカ   \"2万ドル以上\\n3万ドル未満\"       85    0.776\n 6 Argentina           アメリカ   \"2万ドル以上\\n3万ドル未満\"       85    0.83 \n 7 Armenia             ヨーロッパ \"1万ドル以上\\n2万ドル未満\"       53    0.76 \n 8 Australia           オセアニア \"3万ドル以上\"                    97    0.938\n 9 Austria             ヨーロッパ \"3万ドル以上\"                    93    0.914\n10 Azerbaijan          ヨーロッパ \"1万ドル以上\\n2万ドル未満\"       10    0.754\n# … with 168 more rows\n\n\n　次はgroup_by()とsummarise()を使って、各カテゴリーに属するケース数を計算し、Nという名の列として追加します。\n\nHeatmap_df1 <- Heatmap_df %>%\n  group_by(Continent, Income) %>%\n  summarise(N       = n(),\n            .groups = \"drop\")\n\nHeatmap_df1\n\n# A tibble: 18 × 3\n   Continent  Income                         N\n   <fct>      <fct>                      <int>\n 1 アフリカ   \"1万ドル未満\"                 41\n 2 アフリカ   \"1万ドル以上\\n2万ドル未満\"     9\n 3 アフリカ   \"2万ドル以上\\n3万ドル未満\"     2\n 4 アメリカ   \"1万ドル未満\"                 10\n 5 アメリカ   \"1万ドル以上\\n2万ドル未満\"    14\n 6 アメリカ   \"2万ドル以上\\n3万ドル未満\"     6\n 7 アメリカ   \"3万ドル以上\"                  5\n 8 アジア     \"1万ドル未満\"                 17\n 9 アジア     \"1万ドル以上\\n2万ドル未満\"    10\n10 アジア     \"2万ドル以上\\n3万ドル未満\"     3\n11 アジア     \"3万ドル以上\"                 11\n12 ヨーロッパ \"1万ドル未満\"                  1\n13 ヨーロッパ \"1万ドル以上\\n2万ドル未満\"    10\n14 ヨーロッパ \"2万ドル以上\\n3万ドル未満\"     7\n15 ヨーロッパ \"3万ドル以上\"                 28\n16 オセアニア \"1万ドル未満\"                  1\n17 オセアニア \"1万ドル以上\\n2万ドル未満\"     1\n18 オセアニア \"3万ドル以上\"                  2\n\n\n　これでデータの準備は終わりました。ヒートマップを作成する幾何オブジェクトはgeom_tile()です。同時分布を示したい変数を、それぞれxとyにマッピングし、密度、または度数を表す変数をfillにマッピングします。ここでは横軸を大陸（Continent）、縦軸を一人当たり購買力平価GDP（Income）とし、fillにはN変数をマッピングします。\n\nHeatmap_df1 %>%\n  ggplot() +\n  geom_tile(aes(x = Continent, y = Income, fill = N)) +\n  labs(x = \"大陸\", y = \"一人当たり購買力平価GDP（ドル）\", fill = \"国家数\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid = element_blank()) # グリッドラインを消す\n\n\n\n\n\n\n\n\n　明るいほどカテゴリーに属するケースが多く、暗いほど少ないことを意味します。これを見ると世界で最も多くの割合を占めているのは、一人当たり購買力平価GDPが1万ドル未満のアフリカの国で、次は一人当たり購買力平価GDPが3万ドル以上のヨーロッパの国であることが分かります。欠損している（ケース数が0）セルは白の空白となります。\n　色をカスタマイズするにはscale_fill_gradient()です。これは応用編で紹介しましたscale_color_gradient()と使い方は同じです。scale_fill_gradient()は中間点なし、scale_fill_gradient2()は中間点ありの場合に使いますが、ここでは度数が小さい場合はcornsilk色を、大きい場合はbrown3色を使います。それぞれlowとhighに色を指定するだけです。\n\nHeatmap_df1 %>%\n  ggplot() +\n  geom_tile(aes(x = Continent, y = Income, fill = N)) +\n  labs(x = \"大陸\", y = \"一人当たり購買力平価GDP（ドル）\", fill = \"国家数\") +\n  scale_fill_gradient(low  = \"cornsilk\", \n                      high = \"brown3\") +\n  theme_bw(base_size = 12) +\n  theme(panel.grid = element_blank()) # グリッドラインを消す\n\n\n\n\n\n\n\n\n　気のせいかも知れませんが、先ほどよりは読みやすくなったような気がしますね。\n\n\n離散変数\\(\\times\\)離散変数における連続変数の値を示すヒートマップ\n　次は、離散変数\\(\\times\\)離散変数における連続変数の値を示すヒートマップを作ってみましょう。ヒートマップにおけるそれぞれのタイル（tile）は横軸上の位置と縦軸上の位置情報を持ち、これは前回と同様、離散変数でマッピングされます。そして、タイルの色は何らかの連続変数にマッピングされます。前回作成しましたヒートマップは度数、または密度であり、これも実は連続変数だったので、図の作り方は本質的には同じです。\n　ここでは大陸と所得ごとに人間開発指数の平均値を表すヒートマップを作ってみましょう。大陸（Continent）と所得（Income）でグループ化し、人間開発指数（HDI_2018）の平均値を計算したものをHeatmap_df2という名のオブジェクトとして格納します。\n\nHeatmap_df2 <- Heatmap_df %>%\n  group_by(Continent, Income) %>%\n  summarise(HDI     = mean(HDI_2018, na.rm = TRUE),\n            .groups = \"drop\")\n\nHeatmap_df2\n\n# A tibble: 18 × 3\n   Continent  Income                       HDI\n   <fct>      <fct>                      <dbl>\n 1 アフリカ   \"1万ドル未満\"              0.510\n 2 アフリカ   \"1万ドル以上\\n2万ドル未満\" 0.697\n 3 アフリカ   \"2万ドル以上\\n3万ドル未満\" 0.798\n 4 アメリカ   \"1万ドル未満\"              0.638\n 5 アメリカ   \"1万ドル以上\\n2万ドル未満\" 0.752\n 6 アメリカ   \"2万ドル以上\\n3万ドル未満\" 0.806\n 7 アメリカ   \"3万ドル以上\"              0.841\n 8 アジア     \"1万ドル未満\"              0.624\n 9 アジア     \"1万ドル以上\\n2万ドル未満\" 0.730\n10 アジア     \"2万ドル以上\\n3万ドル未満\" 0.818\n11 アジア     \"3万ドル以上\"              0.872\n12 ヨーロッパ \"1万ドル未満\"              0.711\n13 ヨーロッパ \"1万ドル以上\\n2万ドル未満\" 0.776\n14 ヨーロッパ \"2万ドル以上\\n3万ドル未満\" 0.827\n15 ヨーロッパ \"3万ドル以上\"              0.902\n16 オセアニア \"1万ドル未満\"              0.543\n17 オセアニア \"1万ドル以上\\n2万ドル未満\" 0.724\n18 オセアニア \"3万ドル以上\"              0.930\n\n\n　作図の方法は前回と同じですが、今回はタイルの色塗り（fill）を人間開発指数の平均値（HDI）でマッピングする必要があります。他の箇所は同じコードでも良いですが、ここでは色塗りの際、中間点を指定してみましょう。たとえば人間開発指数が0.75なら色をcornsilk色とし、これより低いっほどcornflowerblue色に、高いほどbrown3色になるように指定します。中間点を持つグラデーション色塗りはscale_fill_gradient2()で調整することができます。使い方はscale_fill_gradient()とほぼ同じですが、中間点の色（mid）と中間点の値（midpoint）をさらに指定する必要があります。\n\nHeatmap_df2 %>%\n  ggplot() +\n  geom_tile(aes(x = Continent, y = Income, fill = HDI)) +\n  labs(x = \"大陸\", y = \"一人当たり購買力平価GDP（ドル）\", \n       fill = \"人間開発指数の平均値 (2018)\") +\n  scale_fill_gradient2(low  = \"cornflowerblue\", \n                       mid  = \"cornsilk\",\n                       high = \"brown3\",\n                       midpoint = 0.75) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\",\n        panel.grid      = element_blank())"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-contour",
    "href": "tutorial/R/ggplot_intro4.html#visual4-contour",
    "title": "可視化[発展]",
    "section": "等高線図",
    "text": "等高線図\n　ヒートマップを使えば、離散変数\\(\\times\\)離散変数の同時分布を可視化することは出来ますが、連続変数\\(\\times\\)連続変数の同時分布を可視化するには限界があります。むろん、一つ一つのタイルを小さくすることも出来ますが、効率的な方法ではないでしょう。ここで活躍するのが等高線図 (contour plot) です。\n　たとえば、Country_dfのFH_TotalとHDI_2018の分布は散布図を通じて可視化することができます。\n\nCountry_df %>%\n  ggplot() +\n  geom_point(aes(x = FH_Total, y = HDI_2018)) +\n  labs(x = \"フリーダム・ハウス・スコア\", y = \"人間開発指数 (2018)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　右上に点が集まっていることから、密度の高い箇所だと考えられます。この密度を示す等高線図の幾何オブジェクトはgeom_density_2d()です。マッピング要素はgeom_point()と同じなので、マッピングはggplot()内で行い、geom_density_2d()レイヤーを使いしてみましょう。\n\nCountry_df %>%\n  ggplot(aes(x = FH_Total, y = HDI_2018)) +\n  geom_point() +\n  geom_density_2d() +\n  labs(x = \"フリーダム・ハウス・スコア\", y = \"人間開発指数 (2018)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　密度に応じて色塗りをする場合はgeom_density_2d()の代わりにgeom_density_2d_filled()を使います。\n\nCountry_df %>%\n  ggplot(aes(x = FH_Total, y = HDI_2018)) +\n  geom_density_2d_filled() +\n  labs(x = \"フリーダム・ハウス・スコア\", y = \"人間開発指数 (2018)\",\n       fill = \"密度\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　geom_density_2d_filled()オブジェクトの後にgeom_density_2d()オブジェクトを重ねると、区間の区画線を追加することもできます。\n\nCountry_df %>%\n  ggplot(aes(x = FH_Total, y = HDI_2018)) +\n  geom_density_2d_filled() +\n  geom_density_2d(color = \"black\") +\n  labs(x = \"フリーダム・ハウス・スコア\", y = \"人間開発指数 (2018)\",\n       fill = \"密度\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　色が気に入らない場合、自分で調整することも可能です。scale_fill_manual()で各区間ごとの色を指定することもできませんが、あまり効率的ではありません。ここではscale_fill_brewer()関数を使って、ColorBrewerのパレットを使ってみましょう。引数なしでも使えますが、既定値のパレットは区間が9つまで対応します。今回の等高線図は全部で10区間ですので、あまり適切ではありません。ここでは11区間まで対応可能な\"Spectral\"パレットを使いますが、これはpalette引数で指定できます。\n\nCountry_df %>%\n  ggplot(aes(x = FH_Total, y = HDI_2018)) +\n  geom_density_2d_filled() +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(x = \"フリーダム・ハウス・スコア\", y = \"人間開発指数 (2018)\",\n       fill = \"密度\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　paletteで指定可能なカラーパレットの一覧は{RColorBrewer}のdisplay.brewer.all()関数で確認することが出来ます。各パレットが何区間まで対応できるかを見てから自分でパレットを作成することも可能ですが、詳細はネット上の各種記事を参照してください。\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n{RColorBrewer}が提供するパレート一覧"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-map",
    "href": "tutorial/R/ggplot_intro4.html#visual4-map",
    "title": "可視化[発展]",
    "section": "地図",
    "text": "地図\n\n世界地図\n{ggplot2}で地図をプロットする方法は色々あります。理想としては各国政府が提供する地図データをダウンロードし、それを読み込み・加工してプロットすることでしょうが、ここではパッケージを使ったマッピングについて紹介します。\n　今回使用するパッケージは{rnaturalearth}、{rnaturalearthdata}、{rgeos}です。他にも使うパッケージはありますが、世界地図ならとりあえずこれで十分です。\n\npacman::p_load(rnaturalearth, rnaturalearthdata, rgeos)\n\n　世界地図を読み込む関数は{rnaturalearth}が提供するne_countries()です。とりあえず指定する引数はscaleとretunrclassです。scaleは地図の解像度であり、世界地図なら\"small\"で十分です。もう少し拡大される大陸地図なら\"medium\"が、一国だけの地図なら\"large\"が良いかも知れません。reutrnclassは\"sf\"と指定します。今回は低解像度の世界地図をsfクラスで読み込んでみましょう。\n\nworld_map <- ne_countries(scale = \"small\", returnclass = \"sf\")\n\nclass(world_map)\n\n[1] \"sf\"         \"data.frame\"\n\n\n　クラスはdata.frameとsfであり、実際、world_mapを出力してみると、見た目がデータフレームであることが分かります。地図の出力はgeom_sf()幾何オブジェクトを使用します。とりあえず、やってみましょう。\n\nworld_map %>% \n  ggplot() +\n  geom_sf() +\n  theme_void() # 何もないテーマを指定する。ここはお好みで\n\n\n\n\n\n\n\n\n　もし、各国の人口に応じて色塗りをする場合はどうすれば良いでしょうか。実は、今回使用するデータがデータフレーム形式であることを考えると、これまでの{ggplot2}の使い方とあまり変わりません。{rnaturalearth}から読み込んだデータには既にpop_estという各国の人口データが含まれて負います。この値に応じて色塗りを行うため、geom_sf()内にfill = pop_estでマッピングするだけです。\n　他にも自分で構築したデータがあるなら、データを結合して使用すれば良いでしょう。これについては後述します。\n\nworld_map %>% \n  ggplot() +\n  geom_sf(aes(fill = pop_est)) +\n  # 人口が少ない国はcornflowerblue色に、多い国はbrown3色とする\n  scale_fill_gradient(low = \"cornflowerblue\", high = \"brown3\") +\n  labs(fill = \"人口\") +\n  theme_void()\n\n\n\n\n\n\n\n\n　もし、世界でなく一部の地域だけを出力するなら、coord_sf()で座標系を調整します。東アジアと東南アジアの一部を出力したいとします。この場合、経度は90度から150度まで、緯度は10度から50度に絞ることになります。経度はxlimで、緯度はylimで調整します。\n\nworld_map %>% \n  ggplot() +\n  geom_sf(aes(fill = pop_est)) +\n  scale_fill_gradient(low = \"cornflowerblue\", high = \"brown3\") +\n  labs(fill = \"人口\") +\n  coord_sf(xlim = c(90, 150), ylim = c(10, 50)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　他にもne_countries()内にcontinent引数を指定し、特定の大陸だけを読み込むことで可能です。ここではアジアの国のみを抽出し、asia_mapという名のオブジェクトとして格納します。解像度は中程度とします。\n\nasia_map <- ne_countries(scale = \"medium\", continent = \"Asia\", \n                         returnclass = \"sf\")\n\nasia_map %>%\n    ggplot() +\n    # 所得グループで色塗り\n    geom_sf(aes(fill = income_grp)) +\n    theme_void() +\n    labs(fill = \"Income Group\")\n\n\n\n\n\n\n\n\n　アジアの中から更に東アジアに絞りたい場合はfilter()を使用し、subregion列を基準に抽出することも可能です。\n\nasia_map %>%\n    filter(subregion == \"Eastern Asia\") %>%\n    ggplot() +\n    geom_sf(aes(fill = income_grp)) +\n    theme_void() +\n    labs(fill = \"Income Group\")\n\n\n\n\n\n\n\n\n　subregionの値は以下のように確認可能です。\n\nunique(asia_map$subregion)\n\n[1] \"Southern Asia\"           \"Western Asia\"           \n[3] \"South-Eastern Asia\"      \"Eastern Asia\"           \n[5] \"Seven seas (open ocean)\" \"Central Asia\"           \n\n\n　これまで使用してきたデータがデータフレームと同じ見た目をしているため、{dplyr}を用いたデータハンドリングも可能です。たとえば、人口を連続変数としてでなく、factor型に変換してからマッピングをしてみましょう。\n\nasia_map %>% \n  mutate(Population = case_when(pop_est < 10000000  ~ \"1千万未満\",\n                                pop_est < 50000000  ~ \"5千万未満\",\n                                pop_est < 100000000 ~ \"1億未満\",\n                                pop_est < 500000000 ~ \"5億未満\",\n                                TRUE                ~ \"5億以上\"),\n         Population = factor(Population, \n                             levels = c(\"1千万未満\", \"5千万未満\", \"1億未満\",\n                                        \"5億未満\", \"5億以上\"))) %>%\n  ggplot() +\n  geom_sf(aes(fill = Population)) +\n  scale_fill_brewer(palette = \"Blues\", drop = FALSE) +\n  labs(fill = \"人口\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　scale_fill_brewer()のpalette引数は等高線図のときに紹介しましたパレート一覧を参照してください。\n\n\n日本地図（全体）\n　次は日本地図の出力についてです。日本全土だけを出力するなら、これまで使いましたne_countriesにcountry引数を指定するだけで使えます。たとえば、日本の地図だけなら、country = \"Japan\"を指定します。\n\nne_countries(scale = \"small\", country = \"Japan\", returnclass = \"sf\") %>%\n    ggplot() +\n    geom_sf() +\n    theme_void() # 空っぽのテーマ\n\n\n\n\n\n\n\n\n　これだと、物足りない感があるので、もう少し高解像度の地図にしてみましょう。高解像度の地図データを読み込む際はscale = \"large\"を指定します。\n\nne_countries(scale = \"large\", country = \"Japan\", returnclass = \"sf\") %>%\n    ggplot() +\n    geom_sf() +\n    theme_void() # 空っぽのテーマ\n\n\n\n\n\n\n\n\n　ただ、日本地図を出すという場合、多くは都道府県レベルでマッピングが目的でしょう。世界地図のマッピングならこれで問題ありませんが、一国だけなら、その下の自治体の境界線も必要です。したがって、先ほど使用しましたパッケージのより高解像度の地図が含まれている{rnaturalearthhires}をインストールし、読み込みましょう。2022年Jul月07日現在、{rnaturalearthhires}はCRANに登録されておらず、GitHubのropensciレポジトリーのみで公開されているため、今回は{pacman}のp_load()でなく、p_load_gh()を使用します。\n\npacman::p_load_gh(\"ropensci/rnaturalearthhires\")\n\n　地図データの抽出にはne_states()関数を使用します。第一引数として国家名を指定し、地図データのクラスはsfとします。抽出したデータの使い方は世界地図の時と同じです。\n\nJapan_Map <- ne_states(\"Japan\", returnclass = \"sf\")\n\nJapan_Map %>%\n  ggplot() +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\n\n\n　今回は各都道府県を人口密度ごとに色塗りをしてみましょう。ne_states()で読み込んだデータに人口密度のデータはないため、別途のデータと結合する必要があります。筆者が予め作成しておいたデータを読み込み、中身を確認してみます。\n\nJapan_Density <- read_csv(\"Data/Japan_Density.csv\")\n\nJapan_Density\n\n# A tibble: 47 × 3\n    Code Name   Density\n   <dbl> <chr>    <dbl>\n 1     1 北海道    66.6\n 2     2 青森県   128. \n 3     3 岩手県    79.2\n 4     4 宮城県   316. \n 5     5 秋田県    82.4\n 6     6 山形県   115. \n 7     7 福島県   133  \n 8     8 茨城県   470. \n 9     9 栃木県   302. \n10    10 群馬県   305. \n# … with 37 more rows\n\n\n　各都道府県の人口密度がついております、左側のCodeは何でしょうか。これは各都道府県のISOコードであり、このコードをキー変数としてデータを結合することとなります。各都道府県のコードは国土交通省のホームページから確認可能です。\n　それではデータを結合してみましょう。ne_states()で読み込んだデータの場合、地域のコードはiso_3166_2という列に格納されています。\n\nJapan_Map$iso_3166_2\n\n [1] \"JP-46\" \"JP-44\" \"JP-40\" \"JP-41\" \"JP-42\" \"JP-43\" \"JP-45\" \"JP-36\" \"JP-37\"\n[10] \"JP-38\" \"JP-39\" \"JP-32\" \"JP-35\" \"JP-31\" \"JP-28\" \"JP-26\" \"JP-18\" \"JP-17\"\n[19] \"JP-16\" \"JP-15\" \"JP-06\" \"JP-05\" \"JP-02\" \"JP-03\" \"JP-04\" \"JP-07\" \"JP-08\"\n[28] \"JP-12\" \"JP-13\" \"JP-14\" \"JP-22\" \"JP-23\" \"JP-24\" \"JP-30\" \"JP-27\" \"JP-33\"\n[37] \"JP-34\" \"JP-01\" \"JP-47\" \"JP-10\" \"JP-20\" \"JP-09\" \"JP-21\" \"JP-25\" \"JP-11\"\n[46] \"JP-19\" \"JP-29\"\n\n\n　こちらは文字列となっていますね。これを左から4番目の文字から切り取り、数値型に変換します。変換したコードは結合のためにCodeという名の列として追加しましょう。\n\nJapan_Map <- Japan_Map %>%\n    mutate(Code = str_sub(iso_3166_2, 4),\n           Code = as.numeric(Code))\n\nJapan_Map$Code\n\n [1] 46 44 40 41 42 43 45 36 37 38 39 32 35 31 28 26 18 17 16 15  6  5  2  3  4\n[26]  7  8 12 13 14 22 23 24 30 27 33 34  1 47 10 20  9 21 25 11 19 29\n\n\n　続いて、Japan_MapとJapan_DensityをCode列をキー変数として結合します。データの中身を確認すると、Density列が最後(の直前)の列に追加されたことが分かります。\n\nJapan_Map <- left_join(Japan_Map, Japan_Density, by = \"Code\")\n\nJapan_Map\n\nSimple feature collection with 47 features and 86 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 122.9382 ymin: 24.2121 xmax: 153.9856 ymax: 45.52041\nCRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\nFirst 10 features:\n           featurecla scalerank adm1_code diss_me iso_3166_2 wikipedia iso_a2\n1  Admin-1 scale rank         2  JPN-3501    3501      JP-46      <NA>     JP\n2  Admin-1 scale rank         6  JPN-1835    1835      JP-44      <NA>     JP\n3  Admin-1 scale rank         6  JPN-1829    1829      JP-40      <NA>     JP\n4  Admin-1 scale rank         6  JPN-1827    1827      JP-41      <NA>     JP\n5  Admin-1 scale rank         2  JPN-3500    3500      JP-42      <NA>     JP\n6  Admin-1 scale rank         6  JPN-1830    1830      JP-43      <NA>     JP\n7  Admin-1 scale rank         6  JPN-1831    1831      JP-45      <NA>     JP\n8  Admin-1 scale rank         6  JPN-1836    1836      JP-36      <NA>     JP\n9  Admin-1 scale rank         6  JPN-1833    1833      JP-37      <NA>     JP\n10 Admin-1 scale rank         6  JPN-1832    1832      JP-38      <NA>     JP\n   adm0_sr      name name_alt name_local type    type_en code_local code_hasc\n1        5 Kagoshima     <NA>       <NA>  Ken Prefecture       <NA>     JP.KS\n2        1      Oita     <NA>       <NA>  Ken Prefecture       <NA>     JP.OT\n3        1   Fukuoka  Hukuoka       <NA>  Ken Prefecture       <NA>     JP.FO\n4        1      Saga     <NA>       <NA>  Ken Prefecture       <NA>     JP.SG\n5        3  Nagasaki     <NA>       <NA>  Ken Prefecture       <NA>     JP.NS\n6        1  Kumamoto     <NA>       <NA>  Ken Prefecture       <NA>     JP.KM\n7        1  Miyazaki     <NA>       <NA>  Ken Prefecture       <NA>     JP.MZ\n8        1 Tokushima Tokusima       <NA>  Ken Prefecture       <NA>     JP.TS\n9        1    Kagawa     <NA>       <NA>  Ken Prefecture       <NA>     JP.KG\n10       4     Ehime     <NA>       <NA>  Ken Prefecture       <NA>     JP.EH\n   note hasc_maybe  region region_cod provnum_ne gadm_level check_me datarank\n1  <NA>      JP.NR  Kyushu    JPN-KYS          3          1       20        9\n2  <NA>      JP.ON  Kyushu    JPN-SHK         48          1       20        2\n3  <NA>      JP.NS  Kyushu    JPN-KYS         46          1       20        2\n4  <NA>      JP.OS    <NA>       <NA>         47          1       20        2\n5  <NA>      JP.OY    <NA>       <NA>          5          1       20        9\n6  <NA>      JP.NI  Kyushu    JPN-KYS          6          1       20        2\n7  <NA>      JP.OT  Kyushu    JPN-KYS         49          1       20        2\n8  <NA>      JP.SZ Shikoku    JPN-SHK         45          1       20        2\n9  <NA>      JP.SH Shikoku    JPN-SHK          4          1       20        2\n10 <NA>      JP.ST Shikoku    JPN-SHK         14          1       20        2\n   abbrev postal area_sqkm sameascity labelrank name_len mapcolor9 mapcolor13\n1    <NA>   <NA>         0         NA         2        9         5          4\n2    <NA>     OT         0          7         7        4         5          4\n3    <NA>     FO         0          7         7        7         5          4\n4    <NA>     SG         0         NA         6        4         5          4\n5    <NA>   <NA>         0         NA         2        8         5          4\n6    <NA>     KM         0         NA         6        8         5          4\n7    <NA>     MZ         0         NA         6        8         5          4\n8    <NA>     TS         0         NA         6        9         5          4\n9    <NA>     KG         0         NA         6        6         5          4\n10   <NA>     EH         0         NA         6        5         5          4\n   fips fips_alt   woe_id                       woe_label  woe_name latitude\n1  JA18     JA28  2345867 Kagoshima Prefecture, JP, Japan Kagoshima  29.4572\n2  JA30     JA47  2345879      Oita Prefecture, JP, Japan      Oita  33.2006\n3  JA07     JA27 58646425   Fukuoka Prefecture, JP, Japan   Fukuoka  33.4906\n4  JA33     JA32  2345882      Saga Prefecture, JP, Japan      Saga  33.0097\n5  JA27     JA31  2345876  Nagasaki Prefecture, JP, Japan  Nagasaki  32.6745\n6  JA21     JA29  2345870  Kumamoto Prefecture, JP, Japan  Kumamoto  32.5880\n7  JA25     JA30  2345874  Miyazaki Prefecture, JP, Japan  Miyazaki  32.0981\n8  JA39     JA37  2345888 Tokushima Prefecture, JP, Japan Tokushima  33.8546\n9  JA17     JA35  2345866    Kagawa Prefecture, JP, Japan    Kagawa  34.2162\n10 JA05     JA34  2345855     Ehime Prefecture, JP, Japan     Ehime  33.8141\n   longitude sov_a3 adm0_a3 adm0_label admin geonunit gu_a3   gn_id\n1    129.601    JPN     JPN          4 Japan    Japan   JPN 1860825\n2    131.449    JPN     JPN          4 Japan    Japan   JPN 1854484\n3    130.616    JPN     JPN          4 Japan    Japan   JPN 1863958\n4    130.147    JPN     JPN          4 Japan    Japan   JPN 1853299\n5    128.755    JPN     JPN          4 Japan    Japan   JPN 1856156\n6    130.834    JPN     JPN          4 Japan    Japan   JPN 1858419\n7    131.286    JPN     JPN          4 Japan    Japan   JPN 1856710\n8    134.200    JPN     JPN          4 Japan    Japan   JPN 1850157\n9    134.001    JPN     JPN          4 Japan    Japan   JPN 1860834\n10   132.916    JPN     JPN          4 Japan    Japan   JPN 1864226\n         gn_name  gns_id      gns_name gn_level gn_region gn_a1_code region_sub\n1  Kagoshima-ken -231556 Kagoshima-ken        1      <NA>      JP.18       <NA>\n2       Oita-ken -240089      Oita-ken        1      <NA>      JP.30       <NA>\n3    Fukuoka-ken -227382   Fukuoka-ken        1      <NA>      JP.07       <NA>\n4       Saga-ken -241905      Saga-ken        1      <NA>      JP.33       <NA>\n5   Nagasaki-ken -237758  Nagasaki-ken        1      <NA>      JP.27       <NA>\n6   Kumamoto-ken -234759  Kumamoto-ken        1      <NA>      JP.21       <NA>\n7   Miyazaki-ken -236958  Miyazaki-ken        1      <NA>      JP.25       <NA>\n8  Tokushima-ken -246216 Tokushima-ken        1      <NA>      JP.39       <NA>\n9     Kagawa-ken -231546    Kagawa-ken        1      <NA>      JP.17       <NA>\n10     Ehime-ken -227007     Ehime-ken        1      <NA>      JP.05       <NA>\n   sub_code gns_level gns_lang gns_adm1 gns_region min_label max_label min_zoom\n1      <NA>         1      jpn     JA18       <NA>         7        11        3\n2      <NA>         1      jpn     JA30       <NA>         7        11        3\n3      <NA>         1      jpn     JA07       <NA>         7        11        3\n4      <NA>         1      jpn     JA33       <NA>         7        11        3\n5      <NA>         1      jpn     JA27       <NA>         7        11        3\n6      <NA>         1      jpn     JA21       <NA>         7        11        3\n7      <NA>         1      jpn     JA25       <NA>         7        11        3\n8      <NA>         1      jpn     JA39       <NA>         7        11        3\n9      <NA>         1      jpn     JA17       <NA>         7        11        3\n10     <NA>         1      jpn     JA05       <NA>         7        11        3\n   wikidataid name_ar name_bn             name_de              name_en\n1      Q15701    <NA>    <NA> Präfektur Kagoshima Kagoshima Prefecture\n2     Q133924    <NA>    <NA>      Präfektur Oita      Oita Prefecture\n3     Q123258    <NA>    <NA>   Präfektur Fukuoka   Fukuoka Prefecture\n4     Q160420    <NA>    <NA>      Präfektur Saga      Saga Prefecture\n5     Q169376    <NA>    <NA>  Präfektur Nagasaki  Nagasaki Prefecture\n6     Q130308    <NA>    <NA>  Präfektur Kumamoto  Kumamoto Prefecture\n7     Q130300    <NA>    <NA>  Präfektur Miyazaki  Miyazaki Prefecture\n8     Q160734    <NA>    <NA> Präfektur Tokushima Tokushima Prefecture\n9     Q161454    <NA>    <NA>    Präfektur Kagawa    Kagawa Prefecture\n10    Q123376    <NA>    <NA>     Präfektur Ehime     Ehime Prefecture\n                   name_es                 name_fr name_el name_hi\n1  Prefectura de Kagoshima Préfecture de Kagoshima    <NA>    <NA>\n2       Prefectura de Oita       Préfecture d'Oita    <NA>    <NA>\n3    Prefectura de Fukuoka   Préfecture de Fukuoka    <NA>    <NA>\n4       Prefectura de Saga      Préfecture de Saga    <NA>    <NA>\n5   Prefectura de Nagasaki  Préfecture de Nagasaki    <NA>    <NA>\n6   Prefectura de Kumamoto  Préfecture de Kumamoto    <NA>    <NA>\n7   Prefectura de Miyazaki  Préfecture de Miyazaki    <NA>    <NA>\n8  Prefectura de Tokushima Préfecture de Tokushima    <NA>    <NA>\n9     Prefectura de Kagawa    Préfecture de Kagawa    <NA>    <NA>\n10     Prefectura de Ehime      Préfecture d'Ehime    <NA>    <NA>\n                name_hu             name_id                 name_it name_ja\n1   Kagosima prefektúra Prefektur Kagoshima prefettura di Kagoshima    <NA>\n2       Óita prefektúra      Prefektur Oita      prefettura di Oita    <NA>\n3    Fukuoka prefektúra   Prefektur Fukuoka   prefettura di Fukuoka    <NA>\n4      Szaga prefektúra      Prefektur Saga      Prefettura di Saga    <NA>\n5  Nagaszaki prefektúra  Prefektur Nagasaki  prefettura di Nagasaki    <NA>\n6   Kumamoto prefektúra  Prefektur Kumamoto  prefettura di Kumamoto    <NA>\n7   Mijazaki prefektúra  Prefektur Miyazaki  prefettura di Miyazaki    <NA>\n8   Tokusima prefektúra Prefektur Tokushima prefettura di Tokushima    <NA>\n9     Kagava prefektúra    Prefektur Kagawa    prefettura di Kagawa    <NA>\n10     Ehime prefektúra     Prefektur Ehime     prefettura di Ehime    <NA>\n   name_ko   name_nl              name_pl   name_pt name_ru             name_sv\n1     <NA> Kagoshima Prefektura Kagoshima Kagoshima    <NA> Kagoshima prefektur\n2     <NA>      Oita      Prefektura Oita      Oita    <NA>      Oita prefektur\n3     <NA>   Fukuoka   Prefektura Fukuoka   Fukuoka    <NA>   Fukuoka prefektur\n4     <NA>      Saga      Prefektura Saga      Saga    <NA>      Saga prefektur\n5     <NA>  Nagasaki  Prefektura Nagasaki  Nagasaki    <NA>  Nagasaki prefektur\n6     <NA>  Kumamoto  Prefektura Kumamoto  Kumamoto    <NA>  Kumamoto prefektur\n7     <NA>  Miyazaki  Prefektura Miyazaki  Miyazaki    <NA>  Miyazaki prefektur\n8     <NA> Tokushima Prefektura Tokushima Tokushima    <NA> Tokushima prefektur\n9     <NA>    Kagawa    Prefektura Kagawa    Kagawa    <NA>    Kagawa prefektur\n10    <NA>     Ehime     Prefektura Ehime     Ehime    <NA>     Ehime prefektur\n        name_tr   name_vi name_zh      ne_id Code     Name Density\n1  Kagosima ili Kagoshima    <NA> 1159315225   46 鹿児島県   172.9\n2          Oita      Oita    <NA> 1159311905   44   大分県   177.2\n3       Fukuoka   Fukuoka    <NA> 1159311899   40   福岡県  1029.8\n4          Saga      Saga    <NA> 1159311895   41   佐賀県   332.5\n5      Nagasaki  Nagasaki    <NA> 1159315235   42   長崎県   317.7\n6      Kumamoto  Kumamoto    <NA> 1159311901   43   熊本県   234.6\n7      Miyazaki  Miyazaki    <NA> 1159311903   45   宮崎県   138.3\n8     Tokushima Tokushima    <NA> 1159311909   36   徳島県   173.5\n9        Kagawa    Kagawa    <NA> 1159311907   37   香川県   506.3\n10        Ehime     Ehime    <NA> 1159311139   38   愛媛県   235.2\n                         geometry\n1  MULTIPOLYGON (((129.7832 31...\n2  MULTIPOLYGON (((131.2009 33...\n3  MULTIPOLYGON (((130.0363 33...\n4  MULTIPOLYGON (((129.8145 33...\n5  MULTIPOLYGON (((130.2041 32...\n6  MULTIPOLYGON (((130.3446 32...\n7  MULTIPOLYGON (((131.8723 32...\n8  MULTIPOLYGON (((134.4424 34...\n9  MULTIPOLYGON (((133.5919 34...\n10 MULTIPOLYGON (((132.6399 32...\n\n\n　それではマッピングをしてみましょう。人口密度を5つのカテゴリーに順序付きfactor化してから、そのカテゴリーに応じて色塗りをします。\n\nJapan_Map %>%\n    mutate(Density2 = case_when(Density >= 3000 ~ \"3000人以上\",\n                                Density >= 1000 ~ \"1000人以上\",\n                                Density >=  500 ~ \"500人以上\",\n                                Density >=  100 ~ \"100人以上\",\n                                TRUE            ~ \"100人未満\"),\n           Density2 = factor(Density2, ordered = TRUE,\n                             levels = c(\"3000人以上\", \"1000人以上\", \"500人以上\",\n                                        \"100人以上\", \"100人未満\"))) %>%\n    ggplot() +\n    geom_sf(aes(fill = Density2)) +\n    labs(fill = \"人口密度 (km^2)\") +\n    theme_void()\n\n\n\n\n\n\n\n\n　世界地図でも同じやり方でデータの結合が可能です。この場合はISO3コードかISO2コードがキー変数となります。ISO3コードはiso_a3、ISO2コードはiso_a2列に格納されています。他に使用可能なキー変数はiso_n3であり、こちらは各国を識別する3桁の数字となります。\n\n\n日本地図（特定の都道府県）\n　また日本地図のマッピングですが、今回は市区町村レベルまで見てみましょう。ne_states()では市区町村までマッピングすることはできませんので、今回は徳島大学の瓜生真也先生が公開しました{jpndistrict}を使います。\n\npacman::p_load_gh(\"uribo/jpndistrict\")\n\n　今回は大阪府の地図を出力してみましょう。特定の都道府県の地図を読み込むためにはjpn_pref()関数を使用します。都道府県はpref_codeまたはadmin_nameで指定します。大阪のコードは27であるため、pref_code = 27でも良いですし、admin_name = \"大阪府\"でも同じです。\n\n# Osaka_map <- jpn_pref(admin_name = \"大阪府\") でも同じ\nOsaka_map <- jpn_pref(pref_code = 27)\n\nclass(Osaka_map)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n　プロットの方法は同じです。\n\nOsaka_map %>%\n  ggplot() +\n  geom_sf() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　ここでもデータの結合&マッピングが可能です。大阪府内自治体の人口と学生数が格納されたデータを読み込んでみましょう。こちらは2015年国勢調査の結果から取得したデータです。\n\nOsaka_Student <- read_csv(\"data/Osaka_Student.csv\")\n\nOsaka_Student\n\n# A tibble: 75 × 4\n    Code Name                Pop Student\n   <dbl> <chr>             <dbl>   <dbl>\n 1 27000 大阪府          8839469  438901\n 2 27100 大阪市          2691185  104208\n 3 27102 大阪市 都島区    104727    3889\n 4 27103 大阪市 福島区     72484    2448\n 5 27104 大阪市 此花区     66656    2478\n 6 27106 大阪市 西区       92430    2633\n 7 27107 大阪市 港区       82035    3072\n 8 27108 大阪市 大正区     65141    2627\n 9 27109 大阪市 天王寺区   75729    3480\n10 27111 大阪市 浪速区     69766    1409\n# … with 65 more rows\n\n\n　各市区町村にもコードが指定されており、Osaka_StudentではCode列、Osaka_mapではciti_code列となります。Osaka_mapのcity_codeは文字列であるため、こちらを数値型に変換しCodeという名の列として追加しておきましょう。続いて、Code列をキー変数とし、2つのデータセットを結合します。\n\nOsaka_map <- Osaka_map %>%\n    mutate(Code = as.numeric(city_code))\n\nOsaka_map <- left_join(Osaka_map, Osaka_Student, by = \"Code\")\n\n　最後にマッピングです。ここでは人口1万人当たり学生数をStudent_Ratioという列として追加し、こちらの値に合わせて色塗りをしてみましょう。scale_fill_gradient()を使用し、人口1万人当たり学生数が少ないほど白、多いほど黒塗りします。\n\nOsaka_map %>%\n  mutate(Student_Ratio = Student / Pop * 10000) %>%\n  ggplot() +\n  geom_sf(aes(fill = Student_Ratio)) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  labs(fill = \"1万人当たり学生数 (人)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-dag",
    "href": "tutorial/R/ggplot_intro4.html#visual4-dag",
    "title": "可視化[発展]",
    "section": "非巡回有向グラフ",
    "text": "非巡回有向グラフ\n　近年、因果推論の界隈でよく登場する非巡回有向グラフ（DAG）ですが、「グラフ」からも分かるように、DAGの考え方に基づく因果推論の研究には多くの図が登場します。DAGを作図するには{ggplot2}のみでも可能ですが、{dagitty}パッケージでDAGの情報を含むオブジェクトを生成し、{ggdag}で作図した方が簡単です。以下の図はDAGの一例です。\n\n\n\n\n\n\n\n\n\n　ここでX、Y、Zはノード（node）と呼ばれ、それぞれのノードをつなぐ線のことをエッジ（edge）と呼びます。また、これらのエッジには方向があります（有向）。簡単に言うと原因と結果といった関係ですが、DAGを描く際は、各ノード間の関係を記述する必要があります。\n　それではまず、以上の図を作ってみましょう。最初のステップとして{dagitty}と{ggdag}をインストールし、読み込みましょう。\n\npacman::p_load(dagitty, ggdag)\n\n　つづいて、DAGの情報を含むオブジェクトを作成します。使用する関数はdagify()であり、ここには結果 ~ 原因の形式で各ノード間の関係を記述します。先ほどの図ではXはYの原因（X ~ Y）、ZはXとYの原因（X ~ ZとY ~ Z）です。これらの情報をdagify()内で指定します。\n\nDAG_data1 <- dagify(X ~ Z,\n                    Y ~ Z,\n                    Y ~ X,\n                    exposure = \"X\",\n                    outcome  = \"Y\")\n\nDAG_data1\n\ndag {\nX [exposure]\nY [outcome]\nZ\nX -> Y\nZ -> X\nZ -> Y\n}\n\n\n　Y ~ XとY ~ ZはY ~ X + Zとまとめることも可能です。これは「Yの原因はXとZである」という意味であり、「Yの原因はXであり、Yの原因はZである」と同じ意味です。また、DAGを作図する際、dagify()内にexposureとoutcomeは不要ですが、もしadjustmentSets()関数などを使って統制変数を特定したい場合は処置変数（exposure）と応答変数（outcome）にそれぞれ変数名を指定します。ちなみに、以上のコードは以下のように書くことも可能です。\n\nDAG_data1 <- dagitty(\n  \"dag{\n  X -> Y\n  X <- Z -> Y\n  X [exposure]\n  Y [outcome]\n  }\")\n\nDAG_data1\n\ndag {\nX [exposure]\nY [outcome]\nZ\nX -> Y\nZ -> X\nZ -> Y\n}\n\n\n　格納されたDAG_data1オブジェクトのクラスは\"dagitty\"です。\"dagitty\"の可視化には{ggdag}のggdag()を使用します。\n\nDAG_data1 %>%\n  ggdag()\n\n\n\n\n\n\n\n\n　DAGにおいて背景、軸の目盛り、ラベルは不要ですので、theme_dag_blank()テーマを指定して全て除去します。\n\nDAG_data1 %>%\n  ggdag() +\n  theme_dag_blank()\n\n\n\n\n\n\n\n\n\nノードの位置を指定する\n　読者の多くは以上のグラフと異なるものが得られたかも知れません。ノード間の関係は同じはずですが、ノードの位置が異なるでしょう。また、同じコードを実行する度にノードの位置は変わります。以下ではノードの位置を固定する方法について紹介します。位置を指定するにはdagify()内でcoords引数に各ノードの情報が格納されたリスト型オブジェクトを指定する必要があります。リストの長さは2であり、それぞれの名前はxとyです。そしてリストの各要素にはベクトルが入ります。たとえば、ノードXの位置を (1, 1)、Yの位置を (3, 1)、Zの位置を (2, 2)に指定してみましょう。dagify()内で直接リストを書くことも可能ですが、コードの可読性が落ちるため、別途のオブジェクト（DAG_Pos2）として格納しておきます。\n\nDAG_Pos2  <- list(x = c(X = 1, Y = 3, Z = 2),\n                  y = c(X = 1, Y = 1, Z = 2))\n\n　続いて、dagify()内でcoords引数を追加し、ノードの位置情報が格納されているDAG_Pos2を指定します。\n\nDAG_data2 <- dagify(X ~ Z,\n                    Y ~ X + Z,\n                    exposure = \"X\",\n                    outcome  = \"Y\",\n                    coords   = DAG_Pos2)\n\nDAG_data2\n\ndag {\nX [exposure,pos=\"1.000,1.000\"]\nY [outcome,pos=\"3.000,1.000\"]\nZ [pos=\"2.000,2.000\"]\nX -> Y\nZ -> X\nZ -> Y\n}\n\n\n　可視化の方法は同じです。\n\nDAG_data2 %>%\n  ggdag() +\n  theme_dag_blank()\n\n\n\n\n\n\n\n\n　以上の使い方だけでも、ほとんどのDAGは描けるでしょう。また、ノードを若干オシャレ（?）にするには、ggdag()内でstylized = TRUEを指定します。\n\nDAG_Pos3  <- list(x = c(X1 = 3, X2 = 3, X3 = 1, T = 2, Y = 4),\n                  y = c(X1 = 1, X2 = 2, X3 = 2, T = 3, Y = 3))\n\nDAG_data3 <- dagify(Y  ~ T + X1 + X2,\n                    T  ~ X3,\n                    X2 ~ T +X1 + X3,\n                    exposure = \"T\",\n                    outcome  = \"Y\",\n                    coords   = DAG_Pos3)\n\nDAG_data3 %>%\n  ggdag(stylized = TRUE) +\n  theme_dag_blank()\n\n\n\n\n\n\n\n\n　可視化の話ではありませんが、adjustmentSets()関数を用いると、処置変数Tの総効果（total effect）を推定するためにはどの変数を統制（調整）する必要があるかを調べることも可能です。\n\nadjustmentSets(DAG_data3, effect = \"total\")\n\n{ X3 }\n\n\n　X3変数のみ統制すれば良いという結果が得られました。また、TからYへの直接効果（direct effect）の場合、effect = \"direct\"を指定します。\n\nadjustmentSets(DAG_data3, effect = \"direct\")\n\n{ X1, X2 }\n\n\n　X1とX2を統制する必要があることが分かりますね。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-bump",
    "href": "tutorial/R/ggplot_intro4.html#visual4-bump",
    "title": "可視化[発展]",
    "section": "バンプチャート",
    "text": "バンプチャート\n　バンプチャート (bump chart)は順位の変化などを示す時に有効なグラフです。たとえば、G7構成国の新型コロナ感染者数の順位の変化を示すにはどうすれば良いでしょうか。そもそもどのような形式のデータが必要でしょうか。まずは必要なデータ形式を紹介したいと思います。\n　まず、{ggplot2}によるバンプチャートの作成を支援する{ggbump}パッケージをインストールし、読み込みましょう2。\n\npacman::p_load(ggbump)\n\n　ここではG7構成国の100万人当り新型コロナ感染者数の順位がどのように変化したのかを2020年4月から7月まで1ヶ月単位で表したデータが必要です。データは以下のようなコードで作成しますが、本書のサポートページからもダウンロード可能です。\n\nBump_df <- left_join(COVID19_df, Country_df, by = \"Country\") %>%\n  select(Country, Date, Population, Confirmed_Total, G7) %>%\n  separate(Date, into = c(\"Year\", \"Month\", \"Day\"), sep = \"/\") %>%\n  mutate(Month = as.numeric(Month)) %>%\n  filter(Month >= 4, G7 == 1) %>%\n  group_by(Country, Month) %>%\n  summarise(Population            = mean(Population),\n            New_Cases             = sum(Confirmed_Total, na.rm = TRUE),\n            New_Cases_per_million = New_Cases / Population * 1000000,\n            .groups               = \"drop\") %>%\n  select(Country, Month, New_Cases_per_million)\n\n\nBump_df <- Bump_df %>%\n  group_by(Month) %>%\n  mutate(Rank = rank(New_Cases_per_million, ties.method = \"random\")) %>%\n  ungroup() %>%\n  select(Country, Month, Rank, New_Cases_per_million)\n\n　必要なデータは以下のような形式です。ちなみにバンプチャートを作成するためには最後のNew_Cases_per_million列 (100万人当り新型コロナ感染者数)は不要です。つまり、国名、月、順位のみで十分です。\n\n# 以上のコードを省略し、加工済みのデータを読み込んでもOK\n# Bump_df <- read_csv(\"Data/Bumpchart.csv\")\nBump_df\n\n　それでは{ggbump}が提供するgeom_bump()幾何オブジェクトを使用し、簡単なバンプチャートを作成してみましょう。必要なマッピング要素はxとy、colorです。xには時間を表す変数であるMonthを、yには順位を表すRankをマッピングします。また、7本の線が出るため、月と順位、それぞれの値がどの国の値かを特定する必要があります。groupsに国名であるCountryをマッピングしても線は引けますが、どの線がどの国かが分からなくなるため、colorにCountryをマッピングし、線の色分けをします。\n\nBump_df %>%\n  ggplot(aes(x = Month, y = Rank, color = Country)) +\n  geom_bump()\n\n\n\n\n\n\n\n\n　これで最低限のバンプチャートはできましたが、もう少し見やすく、可愛くしてみましょう。今は線が細いのでややぶ厚めにします。これはgeom_bump()レイヤーのsize引数で指定可能です。また、各月に点を付けることによって、同時期における順位の比較をよりしやすくしてみましょう。これは散布図と同じであるため、geom_point()幾何オブジェクトを使用します。\n\nBump_df %>%\n  ggplot(aes(x = Month, y = Rank, color = Country)) +\n  geom_point(size = 7) +\n  geom_bump(size = 2) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n　これでだいぶ見やすくなりましたが、凡例における国名の順番がやや気になりますね。7月の時点において順位が最も高い国はアメリカ、最も低い国は日本ですが、凡例の順番はアルファベット順となっています。この凡例の順番を7月時点におけるRankの値に合わせた方がより見やすいでしょう。ここでdplyr入門で紹介しましたfct_reorder2()を使ってCountry変数の水準 (level)を7月時点におけるRankの順位に合わせます。この場合、Country変数 (.f = Country)の水準をMonthが (.x = Month)最も大きい (.fun = last2)時点におけるRankの順番に合わせる (.y = Rank)こととなります。fct_reorder2()内の引数の順番の既定値は.f、.x、.y、.funとなります。\n\nBump_df %>%\n  mutate(Country = fct_reorder2(Country, Month, Rank, last2)) %>%\n  ggplot(aes(x = Month, y = Rank, color = Country)) +\n  geom_point(size = 7) +\n  geom_bump(size = 2) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n　最後に縦軸の目盛りラベルを付けます。上に行くほど順位が高くなりますので、1を7に、2を6に、…、7を1に変更します。また、図内のグリッドも不要ですので、theme()を使用し、グリッドを削除します (panel.grid = element_blank())。\n\nBump_df %>%\n  mutate(Country = fct_reorder2(Country, Month, Rank, last2)) %>%\n  ggplot(aes(x = Month, y = Rank, color = Country)) +\n  geom_point(size = 7) +\n  geom_bump(size = 2) +\n  scale_y_continuous(breaks = 1:7, labels = 7:1) +\n  theme_minimal(base_size = 14) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n　これでバンプチャートの完成です。このままでの良いかも知れませんが、もう少し手間を掛けることでより読みやすいグラフが作れます。たとえば、今のままだと「日本のトレンド」を確認したい場合、まず凡例から日本の色を確認し、その色に該当する点と線を見つけてトレンドを見る必要がありますね。もし、ここで図の左端と右端の点の横に国名を出力すると、凡例がなくても良いですし、4月の時点から日本のトレンドを確認することも、7月の時点から遡る形で日本のトレンドを確認することも可能かも知れません。\n　図に文字列を追加するためにはgeom_text()幾何オブジェクトを使用します。マッピング要素は文字列の横軸上の位置 (x)、縦軸上の位置 (y)、そして出力する文字列 (label)です。左端に文字列を出力するのであれば、横軸上の位置は4 (= 4月)よりも若干左側が良いでしょう。ぴったり4になると、点と文字列が重なって読みにくくなりますね。縦軸上の位置は4月の時点での順位 (Rank)で、出力する文字列は国名 (Country)です。現在、使用しているデータは4月から7月までのデータですが、4月に限定したデータを使いたいので、geom_text()内にdata引数を追加し、Bump_dfからMonthの値が4の行のみを抽出したデータを割り当てます (data = filter(Bump_df, Month == 4)、またはdata = Bump_df %>% filter(Month == 4))。右端についても同じです。横軸上の位置は7から右方向へずらし、使用するデータは7月のデータとなります。\n　最後にもう一点調整が必要ですが、それは座標系です。図の座標系はggplot()関数で使用するデータに基づきます。Bump_dfの場合、横軸 (Month)は4から7です。しかし、文字列を追加した場合、文字列がすべて出力されないかも知れません。したがって、座標系を横方向に広める必要があります。今回は3から8までに調整します。座標系や文字列の位置調整は出力結果を見ながら、少しずつ調整していきましょう。\n\nBump_df %>%\n  ggplot(aes(x = Month, y = Rank, color = Country)) +\n  geom_point(size = 7) +\n  geom_bump(size = 2) +\n  # 4月の時点での行のみ抽出し、xはMonthより0.15分左方向、\n  # yはRankの値の位置に国名を出力する。揃える方向は右揃え (hjust = 1)\n  geom_text(data = filter(Bump_df, Month == 4),\n            aes(x = Month - 0.15, y = Rank, label = Country), hjust = 1) +\n  # 7月の時点での行のみ抽出し、xはMonthより0.15分右方向、\n  # yはRankの値の位置に国名を出力する。揃える方向は左揃え (hjust = 0)\n  geom_text(data = filter(Bump_df, Month == 7),\n            aes(x = Month + 0.15, y = Rank, label = Country), hjust = 0) +\n  # 座標系の調整\n  coord_cartesian(xlim = c(3, 8)) +\n  scale_x_continuous(breaks = 4:7, labels = 4:7) +\n  scale_y_continuous(breaks = 1:7, labels = 7:1) +\n  labs(y = \"Rank\", x = \"Month\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid      = element_blank())"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-alluvial",
    "href": "tutorial/R/ggplot_intro4.html#visual4-alluvial",
    "title": "可視化[発展]",
    "section": "沖積図",
    "text": "沖積図\n　沖積図 (alluvial plot)は同じ対象を複数回観察したデータ（パネル・データなど）から変化を示すことに適したグラフです。たとえば、同じ回答者を対象に2回世論調査を実施し、1回目調査時の支持政党と2回目調査時の支持政党を変化を見ることも可能です。もし、変化が大きい場合は政党支持態度は弱いこととなりますし、変化が小さい場合は安定していると解釈できるでしょう。\n　ここでは2020年の選挙と2021年の選挙における有権者の投票先の変化を沖積図で確認してみたいと思います。まずは、沖積図の作成に特化した{ggalluvial}パッケージをインストールし、読み込みます。\n\npacman::p_load(ggalluvial)\n\n　続きまして、実習用データを読み込みます。データは架空のデータです。\n\nVote_2021 <- read_csv(\"Data/Vote_20_21.csv\")\n\nhead(Vote_2021, 20)\n\n# A tibble: 20 × 3\n      ID Vote20 Vote21\n   <dbl> <chr>  <chr> \n 1     1 棄権   棄権  \n 2     2 政党A  政党A \n 3     3 政党A  政党A \n 4     4 政党B  政党A \n 5     5 政党B  政党C \n 6     6 政党A  政党C \n 7     7 その他 その他\n 8     8 政党B  政党B \n 9     9 政党A  政党A \n10    10 政党C  政党C \n11    11 棄権   政党B \n12    12 政党B  政党A \n13    13 棄権   棄権  \n14    14 政党B  政党B \n15    15 政党C  政党C \n16    16 DK     政党C \n17    17 政党B  DK    \n18    18 政党A  政党A \n19    19 政党A  政党A \n20    20 政党A  政党A \n\n\n\n\n\n変数名\n説明\n\n\n\n\nID\n回答者ID\n\n\nVote20\n2020年選挙における当該回答者の投票先\n\n\nVote21\n2021年選挙における当該回答者の投票先\n\n\n\n　たとえば、1番目の回答者は2020年に棄権し、2021年も棄権したことを意味する。また、5番目の回答者は2020年に政党Bに投票し、2021年は政党Cに投票したことを意味する。続いて、このデータを{ggalluvial}に適した形式のデータに加工します。具体的には「2020年棄権、かつ2021年棄権」、「2020年棄権、かつ2021年政党Aへ投票」、…、「2020年政党Bへ投票、かつ2021年政党Aへ投票」のように全ての組み合わせに対し、該当するケース数を計算する必要があります。今回のデータだと、投票先はいずれも政党A、政党B、政党C、政党D、その他、棄権、DK (わからない)の7であるため、49パターンができます。それぞれのパターンに該当するケース数を計算するためにはVote20とVote21でデータをグループ化し、ケース数を計算します。\n\nVote_2021 <- Vote_2021 %>%\n  group_by(Vote20, Vote21) %>%\n  summarise(Freq    = n(),\n            .groups = \"drop\")\n\nVote_2021\n\n# A tibble: 47 × 3\n   Vote20 Vote21  Freq\n   <chr>  <chr>  <int>\n 1 DK     DK       111\n 2 DK     その他     8\n 3 DK     政党A    116\n 4 DK     政党B     29\n 5 DK     政党C     15\n 6 DK     政党D      7\n 7 DK     棄権      57\n 8 その他 DK        18\n 9 その他 その他    73\n10 その他 政党A    121\n# … with 37 more rows\n\n\n　2020年の調査で「わからない」と回答し、2021年の調査でも「わからない」と回答した回答者数は111名、2020年の調査で「わからない」と回答し、2021年の調査では「その他」と回答した回答者数は8名、…といったことが分かりますね。\n　続いて、グラフを作成する前に投票先変数 (Vote20とVote21)をfactor化します。可視化の際、投票先が出力される順番に決まりはありませんが、政党A、政党B、政党C、…、DKの順が自然かと思います。むろん、こちらは自分から見て分かりやいように順番を決めましょう。ただし、2変数における水準 (level)の順番は一致させた方が良いでしょう。\n\nVote_2021 <- Vote_2021 %>%\n  mutate(Vote20 = factor(Vote20, levels = c(\"政党A\", \"政党B\", \"政党C\", \"政党D\", \n                                            \"その他\", \"棄権\", \"DK\")),\n         Vote21 = factor(Vote21, levels = c(\"政党A\", \"政党B\", \"政党C\", \"政党D\", \n                                            \"その他\", \"棄権\", \"DK\")))\n\n　それでは沖積図を描いてみましょう。使用する幾何オブジェクトはgeom_alluvium()とgeom_stratum()です。必ずこの順番でレイヤーを重ねてください。マッピングはy、axis1、axis2に対し、yには当該パターン内のケース数 (Freq)、axis1は2009年の投票先 (Vote20)、axis2は2010年の投票先 (Vote21)を指定します3。これらのマッピングはgeom_stratum()とgeom_alluvim()共通であるため、ggplot()内でマッピングした方が効率的です。\n\nVote_2021 %>%\n    ggplot(aes(y = Freq, axis1 = Vote20, axis2 = Vote21)) +\n    geom_alluvium() +\n    geom_stratum()\n\n\n\n\n\n\n\n\n　何かの図は出てきましたが、これだけだと、それぞれの四角形がどの政党を示しているのかが分かりませんね。四角形内に政党名を出力するためには{ggplot2}内蔵のgeom_text()を使用します。マッピング要素はggplot()内でマッピングしたものに加え、labelが必要ですが、ここではafter_stat(stratum)を指定します。そして、aes()のその側にstat = \"stratum\"を指定するだけです。もし、文字化けが生じる場合は、geom_text()内にフォントの指定が必要があり、familyを使います (たとえば、family = \"HiraginoSans-W3\"など)。theme_*()内でbase_familyを指定した場合でも必要です。\n\nVote_2021 %>%\n    ggplot(aes(y = Freq, axis1 = Vote20, axis2 = Vote21)) +\n    geom_alluvium() +\n    geom_stratum() +\n    geom_text(aes(label = after_stat(stratum)), \n              stat = \"stratum\")\n\n\n\n\n\n\n\n\n　これで沖積図はとりあえず完成ですが、少し読みやすく加工してみましょう。たとえば、2020年に政党Aに投票した回答者における2021年の投票先の割合を見たいとした場合、geom_alluvium()内にfill = Vote20をマッピングします。これで帯に2020年の投票先ごとの色付けができます。\n\nVote_2021 %>%\n    ggplot(aes(y = Freq, axis1 = Vote20, axis2 = Vote21)) +\n    geom_alluvium(aes(fill = Vote20)) +\n    geom_stratum() +\n    geom_text(aes(label = after_stat(stratum)), \n              stat = \"stratum\")\n\n\n\n\n\n\n\n\n　fill = Vote21とマッピングした場合は、感覚が変わります。実際にやってみましょう。\n\nAlluvial_Plot <- Vote_2021 %>%\n    ggplot(aes(y = Freq, axis1 = Vote20, axis2 = Vote21)) +\n    geom_alluvium(aes(fill = Vote21)) +\n    geom_stratum() +\n    geom_text(aes(label = after_stat(stratum)), \n              stat = \"stratum\", family = \"HiraginoSans-W3\")\n\nAlluvial_Plot\n\n\n\n\n\n\n\n\n　この場合、2020年に政党Aに投票した人が2021年にどこに流れたかが分かりやすくなります。Vote20に色分けするか、Vote21に色分けするかは作成する人が決める問題であり、自分の主張・メッセージに適したマッピングをしましょう。\n　最後に、図をもう少し加工してみましょう。まず、横軸に0.8、1.2、1.6、2.0となっている目盛りを修正し、1と2の箇所に「2020年選挙」と「2021年選挙」を出力します。これはscale_x_continuous()で調整可能です。そして、theme_minimal()で余計なものを排除したテーマを適用します。最後にtheme()内で全ての凡例を削除 (legend.position = \"none\")し、パネルのグリッド (panel.grid)と縦軸・横軸のタイトル (axis.title)、縦軸の目盛りラベル (axis.text.y)を削除 (element_blank())します。\n\nAlluvial_Plot +\n    scale_x_continuous(breaks = 1:2, \n                       labels = c(\"2020年選挙\", \"2021年選挙\")) +\n    theme_minimal(base_size = 16) +\n    theme(legend.position = \"none\",\n          panel.grid = element_blank(),\n          axis.title = element_blank(),\n          axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n　これでだいぶスッキリした沖積図が出来上がりました。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-tree",
    "href": "tutorial/R/ggplot_intro4.html#visual4-tree",
    "title": "可視化[発展]",
    "section": "ツリーマップ",
    "text": "ツリーマップ\n　ツリーマップ（tree map）は変数の値の大きさを長方形の面積として表すグラフです。全体におけるシェアを示す時に使う点で、円グラフの代替案の一つになります。円グラフは項目が多すぎると読みにくいデメリットがありますが、ツリーマップは項目が多い場合でも有効です。むろん、多すぎると読みにくいことは同じですので、注意が必要です。\n　ツリーマップを作成するためには{treemapify}パッケージのgeom_treemap()幾何オブジェクトを使用します。まず、{treemapify}をインストールし、読み込みます。\n\npacman::p_load(treemapify)\n\n　ここではCountry_dfからアジア諸国の人口（Population）をツリーマップをして可視化したいと思います。geom_treemap()の場合、各長方形は面積の情報のみを持ちます。この面積の情報をareaにマッピングします。\n\nCountry_df %>%\n  filter(Continent == \"Asia\") %>%\n  ggplot() +\n  geom_treemap(aes(area = Population))\n\n\n\n\n\n\n\n\n　これだけだと各長方形がどの国を指しているのかが分かりませんね。長方形の上に国名（Country）を追加するためにはgeom_treemap_text()幾何オブジェクトを使用します。マッピングはareaとlabelに対し、それぞれ面積を表すPopulationと国名を表すCountryを指定します。areaはgeom_treemap()とgeom_treemap_text()両方で使われるのでggplot()の内部でマッピングしても問題ありません4。また、aes()の外側にcolor = \"white\"で文字を白に指定し、place = \"center\"で長方形の真ん中にラベルが付くようにします。\n\nCountry_df %>%\n  filter(Continent == \"Asia\") %>%\n  ggplot(aes(area = Population)) +\n  geom_treemap() +\n  geom_treemap_text(aes(label = Country), color = \"white\", place = \"center\")\n\n\n\n\n\n\n\n\n　これでツリーマップが完成しました。インドと中国の存在感がかなり大きいですね。更にラベルのサイズを長方形に合わせると、その存在感をより高めることができます。ラベルの大きさを長方形に合わせるにはgeom_treepmap_text()の内部にgrow = TRUEを指定します。\n\nCountry_df %>%\n  filter(Continent == \"Asia\") %>%\n  ggplot(aes(area = Population, label = Country)) +\n  geom_treemap() +\n  geom_treemap_text(color = \"white\", place = \"center\",\n                    grow = TRUE)\n\n\n\n\n\n\n\n\n　ここで更に次元を追加するために、色塗りをしてみましょう。たとえば、G20加盟国か否かで色分けをしたい場合、fillにG20をマッピングします。ただし、今のままだとG20は連続変数扱いになりますので、character型、またはfactor型に変換します。\n\nCountry_df %>%\n  mutate(G20 = if_else(G20 == 1, \"Member\", \"Non-member\")) %>%\n  filter(Continent == \"Asia\") %>%\n  ggplot(aes(area = Population, fill = G20,\n             label = Country)) +\n  geom_treemap() +\n  geom_treemap_text(color = \"white\", place = \"centre\",\n                    grow = TRUE) +\n  labs(fill = \"G20\") +\n  ggtitle(\"Population in Asia\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　色塗りは連続変数に対して行うことも可能です。ここでは2018年人間開発指数（HDI_2108）の値に応じて色塗りをしてみます。また、HDI_2018が低い（low）とbrown3、高い（high）とcornflowerblue色にします。真ん中の値（midpoint）は0.7とし、色（mid）はcornsilkを使います。\n　連続変数でマッピングされた色塗り（fill）の調整にはscale_fill_gradient()、またはscale_fill_gradient2()を使います。前者は中間点なし、後者は中間点ありです。これらの使い方は応用編で紹介しましたscale_color_gradient()と同じです。\n\nCountry_df %>%\n  filter(Continent == \"Asia\") %>%\n  ggplot(aes(area = Population, fill = HDI_2018,\n             label = Country)) +\n  geom_treemap() +\n  geom_treemap_text(color = \"white\", place = \"centre\",\n                    grow = TRUE) +\n  scale_fill_gradient2(low = \"brown3\",\n                       mid = \"cornsilk\",\n                       high = \"cornflowerblue\",\n                       midpoint = 0.7) +\n  labs(fill = \"UN Human Development Index (2018)\") +\n  ggtitle(\"Population in Asia\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　ちなみに以上の図を円グラフにすると以下のようになります（国名は人口の割合が2.5%を超える国のみ表示）。ツリーマップと比較してかなり読みにくいことが分かります。\n\nCountry_df %>%\n  filter(Continent == \"Asia\") %>%\n  arrange(Population) %>%\n  mutate(Prop        = Population / sum(Population) * 100,\n         LabelY      = 100 - (cumsum(Prop) - 0.5 * Prop),\n         CountryName = if_else(Prop < 2.5, \"\", Country),\n         Country     = fct_inorder(Country)) %>%\n  ggplot() +\n  geom_bar(aes(x = 1, y = Prop, group = Country, fill = HDI_2018), \n           color = \"black\", stat = \"identity\", width = 1) +\n  geom_text(aes(x = 1, y = LabelY, label = CountryName)) +\n  coord_polar(\"y\", start = 0) +\n  scale_fill_gradient2(low = \"brown3\",\n                       mid = \"cornsilk\",\n                       high = \"cornflowerblue\",\n                       midpoint = 0.7) +\n  labs(fill = \"UN Human Development Index (2018)\") +\n  ggtitle(\"Population in Asia\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        axis.title = element_blank(),\n        axis.text  = element_blank())\n\n\n\n\n\n\n\n\n　ただし、ツリーマップが必ずしも円グラフより優れているとは言えません。たとえば、 Heer and Bostock (2010) の研究では円グラフとツリーマップを含む9種類のグラフを用い、被験者に大小関係を判断してもらう実験を行いましたが、ツリーマップ（四角形の面積）は円グラフ（角度）よりも判断までの所要時間が長いことが述べています。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-mosaic",
    "href": "tutorial/R/ggplot_intro4.html#visual4-mosaic",
    "title": "可視化[発展]",
    "section": "モザイクプロット",
    "text": "モザイクプロット\nモザイクプロットは2つの離散変数（おもに名目変数）の関係を可視化するために Hartigan and Kleiner (1984) が考案した図です。2つの名目変数間の関係を見る際によく使われるものはクロス表（クロス集計表）でしょう。\n\npacman::p_load(ggmosaic)\n\n\nMosaic_df <- Country_df %>%\n    select(Country, Continent, Polity = Polity_Type, PPP = PPP_per_capita) %>%\n    mutate(Continent = factor(Continent, \n                              levels = c(\"Africa\", \"America\", \"Asia\",\n                                         \"Europe\", \"Oceania\")),\n           Polity    = factor(Polity,\n                              levels = c(\"Autocracy\", \"Closed Anocracy\",\n                                         \"Open Anocracy\", \"Democracy\",\n                                         \"Full Democracy\")),\n           PPP       = if_else(PPP >= 15000, \"High PPP\", \"Low PPP\"),\n           PPP       = factor(PPP, levels = c(\"Low PPP\", \"High PPP\"))) %>%\n    drop_na()\n\n\nhead(Mosaic_df)\n\n# A tibble: 6 × 4\n  Country     Continent Polity          PPP     \n  <chr>       <fct>     <fct>           <fct>   \n1 Afghanistan Asia      Closed Anocracy Low PPP \n2 Albania     Europe    Democracy       Low PPP \n3 Algeria     Africa    Open Anocracy   Low PPP \n4 Angola      Africa    Closed Anocracy Low PPP \n5 Argentina   America   Democracy       High PPP\n6 Armenia     Europe    Democracy       Low PPP \n\n\n　クロス表を作成する内蔵関数としてはtable()があります。2つの変数が必要となり、第一引数が行、第二引数が列を表します。\n\nMosaic_Tab <- table(Mosaic_df$Continent, Mosaic_df$Polity)\nMosaic_Tab\n\n         \n          Autocracy Closed Anocracy Open Anocracy Democracy Full Democracy\n  Africa          3              14            11        18              1\n  America         0               1             4        16              5\n  Asia           13               6             0        15              3\n  Europe          2               1             2        16             20\n  Oceania         0               0             2         0              2\n\n\n　このMosaic_Tabのクラスは\"table\"ですが、\"table\"クラスのオブジェクトをplot()に渡すと別途のパッケージを使わずモザイクプロットを作成することができます。\n\n\n\n\nplot(Mosaic_Tab)\n\n\n\n\n\n\n\n\n　やや地味ではありますが、モザイクプロットが出来ました。ここからはより読みやすいモザイクプロットを作成するために{ggmosaic}パッケージのgeom_mosaic()関数を使います。\n　geom_mosaic()の場合、xのみのマッピングで十分です。ただし、特定の変数を指定するのではなく、product(変数1, 変数2)をxにマッピングする必要があります。table()関数同様、変数1は行、変数2は列です。また、欠損値が含まれている行がある場合は、aes()の外側にna.rm = TRUEを指定する必要があります。今回はdrop_na()で欠損値をすべて除外しましたが、念の為に指定しておきます。\n\nMosaic_df %>%\n    ggplot() +\n    geom_mosaic(aes(x = product(Polity, Continent)), na.rm = TRUE) +\n    labs(x = \"Continent\", y = \"Polity Type\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank())\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nPlease use `unite()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n\n\n\n\n\n　これで出来上がりですが、\"table\"オブジェクトをplot()に渡した結果とあまり変わらないですね。続いて、この図を少し改良してみましょう。まずはセルの色分けですが、これはfillに色分けする変数をマッピングするだけです。今回は政治体制ごとにセルを色分けしましょう。また、文字を大きめにし、横軸の目盛りラベルを回転します。\n\nMosaic_df %>%\n    ggplot() +\n    geom_mosaic(aes(x = product(Polity, Continent), fill = Polity), \n                na.rm = TRUE) +\n    labs(x = \"Continent\", y = \"Polity Type\") +\n    theme_minimal(base_size = 16) +\n    theme(legend.position = \"none\",\n          panel.grid      = element_blank(),\n          axis.text.x     = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n　次元を追加するためにはファセット分割を使います。たとえば、一人当たりPPP GDPの高低（PPP）でファセットを分割する場合、facet_wrap(~PPP)レイヤーを足すだけです。\n\nMosaic_df %>%\n    ggplot() +\n    geom_mosaic(aes(x = product(Polity, Continent), fill = Polity), \n                na.rm = TRUE) +\n    labs(x = \"Continent\", y = \"Polity Type\") +\n    facet_wrap(~PPP, ncol = 2) +\n    theme_minimal(base_size = 16) +\n    theme(legend.position = \"none\",\n          panel.grid      = element_blank(),\n          axis.text.x     = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n　ただし、一つ問題があります。それは目盛りラベルの位置です。例えば、右側の横軸目盛りラベルの場合、セルの位置とラベルの位置がずれています。これは2つのファセットが同じ目盛りを共有し、左側の方に合わせられたため生じるものです。よく見ると横軸も縦軸も目盛りラベルに位置が同じであることが分かります。これを解消するためには、facet_wrap()の内部にscale = \"free\"を指定します5。これは各ファセットが独自のスケールを有することを意味します。\n\nMosaic_df %>%\n    ggplot() +\n    geom_mosaic(aes(x = product(Polity, Continent), fill = Polity), \n                na.rm = TRUE) +\n    labs(x = \"Continent\", y = \"Polity Type\") +\n    facet_wrap(~PPP, ncol = 2, scale = \"free\") +\n    theme_minimal(base_size = 16) +\n    theme(legend.position = \"none\",\n          panel.grid      = element_blank(),\n          axis.text.x     = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n　右側ファセットの横軸ラベルが重なってしまいましたが、これでとりあえず完成です。アフリカにおけるOpen AnocracyとClosed Anocracyの頻度が0であるため、これは仕方ありません。一つの対処方法としては以下のように縦軸目盛りを削除し、凡例で代替することが考えられます。\n\nMosaic_df %>%\n    ggplot() +\n    geom_mosaic(aes(x = product(Polity, Continent), fill = Polity), \n                na.rm = TRUE) +\n    labs(x = \"Continent\", y = \"Polity Type\", fill = \"Polity Type\") +\n    facet_wrap(~PPP, ncol = 2, scale = \"free_x\") +\n    theme_minimal(base_size = 16) +\n    theme(panel.grid  = element_blank(),\n          axis.text.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
  },
  {
    "objectID": "tutorial/R/ggplot_intro4.html#visual4-further",
    "href": "tutorial/R/ggplot_intro4.html#visual4-further",
    "title": "可視化[発展]",
    "section": "その他のグラフ",
    "text": "その他のグラフ\nThe R Graph Galleryでは本書で紹介できなかった様々な図のサンプルおよびコードを見ることができます。ここまで読み終わった方なら問題なくコードの意味が理解できるでしょう。{ggplot2}では作成できないグラフ（アニメーションや3次元図、インタラクティブなグラフ）についても、他のパッケージを利用した作成方法について紹介されているので、「こんな図が作りたいけど、作り方が分からん！」の時には、まずThe R Graph Galleryに目を通してみましょう。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro1.html",
    "href": "tutorial/R/ggplot_intro1.html",
    "title": "可視化[理論]",
    "section": "",
    "text": "Rによる可視化は様々な方法がありますが、可視化のために使うパッケージとして代表的なものは (1) パッケージを使わない方法、(2) {lattice}パッケージ、(3) {ggplot2}パッケージがあります。ここでは、以下のデータ ( 表 1 )を可視化しながら、それぞれの特徴について簡単に解説します。\n\n\n\n\n\n\n\n\n\n表 1:  サンプルデータの最初の10行 \n  \n  \n    \n      Country\n      PPP\n      HDI\n      OECD\n    \n  \n  \n    Afghanistan\n2125\n0.496\n非加盟国\n    Albania\n13781\n0.791\n非加盟国\n    Algeria\n11324\n0.759\n非加盟国\n    Angola\n6649\n0.574\n非加盟国\n    Argentina\n22938\n0.830\n非加盟国\n    Armenia\n12974\n0.760\n非加盟国\n    Australia\n50001\n0.938\n加盟国\n    Austria\n55824\n0.914\n加盟国\n    Azerbaijan\n14257\n0.754\n非加盟国\n    Bahrain\n43624\n0.838\n非加盟国\n  \n  \n  \n\n\n\n\n\nこのデータは各国 (Country) の一人当たり購買力平価基準GDP (PPP)、人間開発指数 (HDI)、OECD加盟有無 (OECD)の変数で構成されています。このデータを使って横軸はPPP、縦軸はHDIとし、OECDの値によって色分けしたグラフを作成します。\n\n\nRは統計学、データ分析に特化したプログラミング言語であるため、別途のパッケージなしで作図が可能です。後ほど紹介する{lattice}や{ggplot2}を用いた作図とは違って、Base Rによる作図は、紙にペンでグラフを書くイメージに近いです。キャンバスを用意し、そこにペンで点や線を描く感じです。これはレイヤーという概念を導入した{ggplot2}に近いかも知れませんが、{ggplot2}はレイヤーを変更出来る一方、Base Rは図が気に入らない場合、一からやり直しです。つまり、キャンバスに引いた線や点は消すことができません。また、作成した図はオブジェクトとして保存することが出来ないため、もう一度図示するためには一から書く必要があります。Base Rによる作図は短所だけでなく、メリットもあります。まず、パッケージを必要としないため、Rインストール直後から使える点です。そして、{lattice}や{ggplot2}よりも速いです。他にも人によってはBase Rの方がシンプルでかっこいいという方もいますが、これは好みの問題でしょう。\n以下の 図 1 はBase Rを使った散布図の例です。\n\n# Base Rを用いた作図の例\nplot(x = Country_df$PPP, y = Country_df$HDI, pch = 19, \n     col = ifelse(Country_df$OECD == \"加盟国\", \"red\", \"blue\"),\n     xlab = \"一人当たり購買力平価GDP (USD)\", ylab = \"人間開発指数\")\nlegend(\"bottomright\", pch = 19,\n       legend = c(\"OECD加盟国\", \"OECD非加盟国\"), \n       col    = c(\"red\", \"blue\"))\n\n\n\n\n図 1: Base Rによるグラフ\n\n\n\n\n\n\n\n{lattice}はDeepayan Sarkarによって開発された可視化パッケージです。このパッケージの最大特徴は「1つの関数で可視化が出来る」点です。作図に必要な様々な情報が1つの関数内に全て入ります。むろん、指定しない情報に関しては多くの場合、自動的に処理してくれます。\n図 1 を{lattice}を使って作る場合は 図 2 のようなコードになります。\n\n# latticeを用いた作図の例\nxyplot(HDI ~ PPP, data = Country_df,\n       group = OECD, pch = 19, grid = TRUE,\n       auto.key = TRUE,\n       key = list(title     = \"OECD加盟有無\",\n                  cex.title = 1,\n                  space     = \"right\",\n                  points    = list(col = c(\"magenta\", \"cyan\"),\n                                   pch = 19),\n                  text      = list(c(\"加盟国\", \"非加盟国\"))), \n       xlab = \"一人当たり購買力平価GDP (USD)\", ylab = \"人間開発指数\")\n\n\n\n\n図 2: {lattice}によるグラフ\n\n\n\n\n1つの関数で全てを処理するので、関数が非常に長くなり、人間にとって読みやすいコードにはなりにくいのが短所です。しかし、{lattice}はBase Rでは出来ない、プロットのオブジェクトとしての保存ができます。Base Rは出来上がったプロットをオブジェクトとして保存することが出来ず、同じ図をもう一回出力するためには、改めてコードを書く必要があります。しかし、{lattice}はオブジェクトとして保存ができるため、いつでもリサイクルが可能です。他にも、{lattice}は条件付きプロットの作成において非常に強力です。しかし、これらの特徴は今は{ggplot2}も共有しているため、{lattice}独自の長所とは言いにくいです。\n\n\n\n{ggplot2}はHadely Wickhamが大学院生の時に開発した可視化パッケージであり1、 Wilkinson (2005) の「グラフィックの文法 (grammer of graphics)」の思想をR上で具現化したものです。グラフィックの文法という思想は今は{ggplot2}以外にもPlotlyやTableauなどでも採用されています。\nグラフィックの文法は後ほど詳細に解説しますが、{ggplot2}による作図の特徴は「レイヤーを重ねる」ことです。グラフの様々な要素をそれぞれ1つの層 (layer)と捉え、これを重ねられていくことでグラフが出来上がる仕組みです。これはBase Rの書き方に似ています。たとえば、{ggplot2}を使って 図 1 を作る場合、以下のようなコードになります。\n\n# ggplot2を用いた作図の例\nggplot(data = Country_df) +\n  geom_point(aes(x = PPP, y = HDI, color = OECD)) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       color = \"OECD加盟有無\") +\n  theme_bw()\n\n\n\n\n図 3: {ggplot2}によるグラフ\n\n\n\n\nこのように{ggplot2}による作図コードはBase Rや{lattice}に比べ、読みやすいのが特徴です。また、書く手間も大きく省かれる場合が多く、結果として出力されるグラフも綺麗です（これは好みによりますが）。しかし、{ggplot2}にも限界はあり、代表的なものとして (1) 3次元グラフが作成でないこと、(2) 処理速度が遅い点があります。後者は多くの場合においてあまり気にならない程度ですが、3次元プロットが必要な場合は{lattice}や別途のパッケージを使う必要があります。しかし、社会科学において3次元プロットが使われる機会は少なく、2次元平面であっても3次元以上のデータを表現することも可能です。本書では{ggplot2}を用いた可視化方法のみについて解説していきます。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro1.html#visual-ggplot",
    "href": "tutorial/R/ggplot_intro1.html#visual-ggplot",
    "title": "可視化[理論]",
    "section": "グラフィックの文法",
    "text": "グラフィックの文法\n\nグラフィックの文法\n本書では特別な事情がない限り、{ggplot2}による可視化のみを扱います。既に{ggplot2}のggが「グラフィックの文法 (grammar of graphics)」だと説明しましたが、この概念は Wilkinson (2005) によって提唱された比較的新しいものであり、{ggplot2}はHadley先生がグラフィックの文法に則った作図のプロセスをRで具現化したものです。\nグラフィックスの文法とは、グラフを構造化された方法で記述し、レイヤーを積み重ねることによってグラフを構築するフレームワークです。グラフは様々な要素で構成されています。横軸と縦軸、点、線、グラフ、凡例、図のタイトルなどがあります。横軸や縦軸は線の太さ、目盛りの間隔、数字の大きさなどに分割することも可能です。このように1つのグラフは数十、数百以上の要素の集合です。これら一つ一つの要素をレイヤーとして捉え、それを積み重ねることでグラフを作成します。これが簡単な{ggplot2}による作図のイメージですが、以下でもうちょっと目に見える形でこれを解説していきます。\n\n\nggplot2のイメージ\nそれでは{ggplot2}によるグラフが出来上がる過程を見ていきます。例えば、以下のようなデータセットdfがあるとします。変数は年度を表すYear、鉄道事業者のタイプを表すCompany_Type1、一日利用者数の平均値を表すPがあります。例えば、2行目は2011年度におけるJRが管理する駅の一日平均利用者数が約7399名であることを意味します。\n\n\n# A tibble: 28 × 3\n    Year Company_Type1     P\n   <dbl> <fct>         <dbl>\n 1  2011 その他         4769\n 2  2011 JR             7399\n 3  2011 大手私鉄      19421\n 4  2011 準大手私鉄     7683\n 5  2012 その他         5014\n 6  2012 JR             7289\n 7  2012 大手私鉄      21286\n 8  2012 準大手私鉄    10471\n 9  2013 その他         5154\n10  2013 JR             7383\n# … with 18 more rows\n\n\nこのデータdfを使って 図 4 のようなグラフを作成します。以下では作図のコードも載っていますが、詳しく理解しなくても結構です。理解しなくてもいいですが、必ずコードには目を通し、説明文との対応を自分で考えてください。\n\n\n\n\n\n図 4: 鉄道駅の事業者区分による平均利用者数の推移\n\n\n\n\n\nまずは、グラフに使用するデータを指定し、空のキャンバスを用意します。\n\nggplot(data = df)の代わりにggplot(df)、df %>% ggplot()も可能\n\n\n\nggplot(data = df) #<\n\n\n\n\n図 5: 第1層: データとキャンバスの用意\n\n\n\n\n\n折れ線グラフを作成します。折れ線グラフは点の位置を指定すると、勝手に点と点の間を線で繋いでぐれます。したがって、必要な情報は点の情報ですが、横軸 (X軸)はYear、縦軸 (Y軸)はPにした点を出力します。この点をCompany_Type1ごとに色分けします。これで折れ線グラフが出来上がります。線の太さは1にします。\n\n\nggplot(data = df) +\n  geom_line(aes(x = Year, y = P, color = Company_Type1), size = 1) #<\n\n\n\n\n図 6: 第2層: 幾何オブジェクト (折れ線グラフ) の指定とマッピング\n\n\n\n\n\n続いて、折れ線グラフに散布図を載せます。これは線が引かれていない折れ線グラフと同じです。したがって、横軸、縦軸、色分けの情報は同じです。しかし、点と線が重なると点がよく見えないこともあるので、点の大きさを3にし、形は枠線付きの点にします。点の中身は白塗りをします。つまり、Company_Type1によって変わるのは、点の枠線です。\n\n\nggplot(data = df) +\n  geom_line(aes(x = Year, y = P, color = Company_Type1), size = 1) +\n  geom_point(aes(x = Year, y = P, color = Company_Type1),  #<\n             size = 3, shape = 21, fill = \"white\")         #<\n\n\n\n\n図 7: 第3層: 幾何オブジェクト (散布図) の指定とマッピング\n\n\n\n\n\n横軸と縦軸、そして凡例のタイトルを日本語に直します。日本語のレポート、論文なら図表も日本語にすべきです。横軸のラベルは\"年度\"、縦軸のラベルは\"平均利用者数 (人/日)\"にします。色の凡例タイトルは\"事業者区分\"にします。\n\n\nggplot(data = df) +\n  geom_line(aes(x = Year, y = P, color = Company_Type1), size = 1) +\n  geom_point(aes(x = Year, y = P, color = Company_Type1), \n             size = 3, shape = 21, fill = \"white\") +\n  labs(x = \"年度\", y = \"平均利用者数 (人/日)\", color = \"事業者区分\") #<\n\n\n\n\n図 8: ラベルの修\n\n\n\n\n\n横軸のスケールを修正します。横軸は連続 (continuous)変数です。今の目盛りは2012、2014、2016になっていますが、これを1年刻みにし、それぞれの目盛りのラベルも2011、2012、2013、…にします。\n\n\nggplot(data = df) +\n  geom_line(aes(x = Year, y = P, color = Company_Type1), size = 1) +\n  geom_point(aes(x = Year, y = P, color = Company_Type1), \n             size = 3, shape = 21, fill = \"white\") +\n  labs(x = \"年度\", y = \"平均利用者数 (人/日)\", color = \"事業者区分\") +\n  scale_x_continuous(breaks = 2011:2017, labels = 2011:2017) #<\n\n\n\n\n図 9: スケールの修正\n\n\n\n\n\nグラフのテーマを{ggplot2}が基本的に提供しているminimalに変更し、フォントサイズを12に変更します。\n\n\nggplot(data = df) +\n  geom_line(aes(x = Year, y = P, color = Company_Type1), size = 1) +\n  geom_point(aes(x = Year, y = P, color = Company_Type1), \n             size = 3, shape = 21, fill = \"white\") +\n  labs(x = \"年度\", y = \"平均利用者数 (人/日)\", color = \"事業者区分\") +\n  scale_x_continuous(breaks = 2011:2017, labels = 2011:2017) +\n  theme_minimal(base_size = 12) #<\n\n\n\n\n図 10: 見た目の調整\n\n\n\n\nこれで図が出来上がりました。このように{ggplot2}では図の要素をレイヤーと捉えます。このレイヤーを生成する関数がggplot()、geom_line()、lab()などであり、これらを+演算子を用いて重ねていきます。このイメージを図にすると 図 11 のように表現できます。\n\n\n\n\n\n図 11: {ggplot2}の図が出来上がるまで (全体像)\n\n\n\n\nそれでは、グラフは具体的にどのような要素で構成されているかを以下で解説します。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro1.html#グラフィックの構成要素",
    "href": "tutorial/R/ggplot_intro1.html#グラフィックの構成要素",
    "title": "可視化[理論]",
    "section": "グラフィックの構成要素",
    "text": "グラフィックの構成要素\n{ggplot2}におけるプロット (plot)は「データ + 幾何オブジェクト + 座標系」で構成されます。\nここでいうデータは主にデータフレームまたはtibbleです。これは主にggplot()関数の第一引数と指定するか、パイプで渡すのが一般的です。ただし、{ggplot2}で作図するためには、データを予め整然データに整形する必要があります。\n幾何オブジェクト (geometry object)とは簡単に言うと図の種類です。散布図、折れ線グラフ、棒グラフ、ヒストグラムなど、{ggplot2}は様々なタイプの幾何オブジェクトを提供しており、ユーザー自作の幾何オブジェクトもRパッケージとして多く公開されています。幾何オブジェクトは関数の形で提供されており、geom_で始まるといった共通点があります。散布図はgeom_point()、折れ線グラフはgeom_line()のような関数を使います。\nこの幾何オブジェクトに線や点、棒などを表示する際には、どの変数が横軸で、どの変数が縦軸かを明記する必要があります。また、変数によって点や線の色が変わったりする場合も、どの変数によって変わるかを明記します。これを マッピング (mapping)と呼びます。また、必要に応じて位置 (position)と統計量 (stat)を明記する必要がありますが、これは指定しなくてもとりあえず何らかの図は出力されます。\n最後に座標系 (coordinate system)は幾何オブジェクトが表示される空間の特徴を定義します。最も重要な特徴は横軸と縦軸の下限と上限です。または、空間を回転することなどもできます。\n{ggplot2}の図は以上の3つ要素を重ねることで出来ます（ 図 12 ）。\n\n\n\n\n\n図 12: {ggplot2}の構造の例\n\n\n\n\nただし、この中で座標系は適切だと判断される座標系に設定してくれるため、ユーザーが必ず指定すべきものはデータと幾何オブジェクトのみです。また、幾何オブジェクトはマッピングを含んでおり、これも必ず指定する必要があります。したがって、{ggplot2}で作図するための最小限のコードは以下のようになります。\n\n# ggplot2におけるプロットの基本形\n# データはggplotの第一引数と使う場合が多いため、「data =」は省略可能\n# マッピングは主に幾何オブジェクトの第一引数として使うため、「mapping =」は省略可能\nggplot(data = データ名) +\n  幾何オブジェクト関数(mapping = aes(マッピング))\n\n# パイプを使う場合\nデータ名 %>%\n  ggplot() +\n  幾何オブジェクト関数(mapping = aes(マッピング))\n\n注意すべき点は{ggplot2}においてレイヤーを重ねる際は%>%でなく、+を使う点です。パイプ演算子は左側の結果を右に渡す意味を持ちますが、{ggplot2}はデータを渡すよりも、レイヤーを足していくイメージですから、+を使います。\n以下は{ggplot2}の必須要素であるデータと幾何オブジェクト、マッピングなどについて解説し、続いて図は見栄を調整するための関数群を紹介します。\n\nデータ\n作図のためにはデータはなくてはなりません。データはdata.frmae型、またはtibble型であり、一般的にはggplot()関数の第一引数として指定します。例えば、ggplot(data = データ名)のように書いてもいいですし、data =は省略して、ggplot(データ名)でも構いません。\nデータを指定するもう一つの方法はパイプ演算子を使うことです。この場合、データ名 %>% ggplot()のように書きます。書く手間はほぼ同じですが、dplyrやtidyrなどでデータを加工し、それをオブジェクトとして保存せずにすぐ作図に使う場合は便利です。\n実際、使う機会は少ないですが、1つのグラフに複数のデータを使う場合もあります。{ggplot2}は複数のデータにも対応していますが、とりあえずメインとなるデータをggplot()に指定し、追加的に必要なデータは今度説明する幾何オブジェクト関数内で指定します。この方法については適宜必要に応じて説明します。\n\n\n幾何オブジェクト\nしかし、データを指定しただけで図が出来上がるわけではありません。指定したデータを使ってどのような図を作るかも指定する必要があります。この図のタイプが幾何オブジェクト (geometry object)であり、geom_*()関数で表記します。たとえば、散布図を作る場合、\n\nデータ名 %>%\n  ggplot() +\n  geom_point()\n\nのように指定します。以上のコードは「あるデータを使って (データ名 %>%)、キャンバスを用意し (ggplot() +)、散布図を作成する (geom_point())。」と読むことが出来ます。\nまた、この幾何オブジェクトは重ねることも可能です。よく見る例としては、散布図の上に回帰曲線 (geom_smooth())や折れ線グラフ (geom_line())を重ねたものであり、これらの幾何オブジェクトは+で繋ぐことが可能です。\n{ggplot2}が提供する幾何オブジェクト関数は散布図だけでなく、棒グラフ (geom_bar())、ヒストグラム (geom_histogram())、折れ線グラフ (geom_line())、ヒートマップ (geom_tile())など、データ分析の場面で使われるほとんどの種類が含まれています。他にもユーザーが作成した幾何オブジェクトもパッケージとして多く公開されています（たとえば、非巡回有向グラフ作成のための{ggdag}、ネットワークの可視化のための{ggnetwork}など）。\n\n\nマッピング\nどのようなデータを使って、どのような図を作るかを指定した後は、変数を指定します。たとえば、散布図の場合、各点の横軸と縦軸における位置情報が必要です。ヒストグラムならヒストグラムに必要な変数を指定する必要があります。このようにプロット上に出力されるデータの具体的な在り方を指定するのをマッピング (mapping)と呼びます。\nマッピングは幾何オブジェクト関数内で行います。具体的にはgeom_*()内にmapping = aes(マッピング)で指定します。aes()も関数の一種です。散布図ならgeom_point(mapping = aes(x = X軸の変数, y = Y軸の変数))です。ヒストグラムなら横軸のみを指定すればいいのでgeom_histogram(mapping = aes(x = 変数名))で十分です。マッピングは一般的にはgeom_*()の第一引数として渡しますが、この場合、mapping =は省略可能です。\nマッピングに必要な変数、つまりaes()に必要な引数は幾何オブジェクトによって異なります。散布図や折れ線グラフならX軸とY軸の情報が必須であるため、2つ必要です (xとy)。ヒストグラムは連続変数の度数分布表を自動的に作成してからグラフが作られるから1つが必要です (x)。また、等高線図の場合、高さの情報も必要なので3つの変数が必要です (xとy、z)。これらの引数は必ず指定する必要があります。\n以上の引数に加え、追加のマッピング情報を入れることも可能です。たとえば、鉄道事業者ごとの平均利用者数を時系列で示した最初の例を考えてみましょう。これは折れ線グラフですので、mapping = aes(x = 年度, y = 利用者数)までは必須です。しかし、この図にはもう一つの情報がありますね。それは事業者のタイプです。事業者のタイプごとに線の色を変えたい場合は、aes()内にcolor = 事業者のタイプを、線の種類を変えたい場合は、linetype = 事業者のタイプのように引数を追加します。こうすると2次元のプロットに3次元の情報 (年度、利用者数、事業者タイプ)を乗せることが可能です。むろん、4次元以上にすることも可能です。たとえば、地域ごとに異なる色を、事業者タイプごとに異なる線のタイプを指定する場合は、mapping = aes(x = 年度, y = 利用者数, color = 地域, linetype = 事業者のタイプ)のように指定します。colorやlinetype以外にも大きさ (size)、透明度 (alpha)、点のタイプ (shape)、面の色 (fill)などを指定することができます。\nそれでは、最初にお見せした 図 10 のマッピングはどうなるでしょうか。図 10 の幾何オブジェクトは折れ線グラフ (geom_line())と散布図 (geom_point())の2つです。それぞれの幾何オブジェクトのマッピング情報をまとめたのが 表 2 です。\n\n\n表 2: 図 10 のマッピング情報\n\n\n幾何オブジェクト\nマッピング要素\n変数\n引数\n\n\n\n\ngeom_line\nX軸\nYear\nx\n\n\n同上\nY軸\nP\ny\n\n\n同上\n線の色\nCompany_Type1\ncolor\n\n\ngeom_point\nX軸\nYear\nx\n\n\n同上\nY軸\nP\ny\n\n\n同上\n枠線の色\nCompany_Type1\ncolor\n\n\n\n\n先ほどマッピング引数は幾何オブジェクト関数内で指定すると言いましたが、実はggplot()内に入れ、geom_*()内では省略することも可能です。幾何オブジェクトが1つのみならどっちでも問題ありません。しかし、幾何オブジェクトが2つ以上の場合は注意が必要です。全ての幾何オブジェクトがマッピングを共有する場合はggplot()の方が書く手間が省きます。たとえば、表 2 を見ると、geom_line()とgeom_point()はx、y、color引数の値が同じですから、ggplot()内にaes()を入れることも可能です。しかし、幾何オブジェクトがマッピングを共有しない場合は幾何オブジェクト関数内に別途指定する必要があります。あるいは、共有するところだけ、ggplot()に書いて、共有しない部分だけ幾何オブジェクトで指定することも可能です。したがって、上の図は以下のコードでも作成することができます。\n\n# geom_line()とgeom_point()はマッピング情報を共有しているため、\n# ggplot()内に指定\ndf %>%\n  ggplot(aes(x = Year, y = P, color = Company_Type1)) +\n  geom_line(size = 1) +\n  geom_point(size = 3, shape = 21, fill = \"white\") +\n  labs(x = \"年度\", y = \"平均利用者数 (人/日)\", color = \"事業者区分\") +\n  scale_x_continuous(breaks = 2011:2017, labels = 2011:2017) +\n  theme_bw()\n\n\n\nマッピング: その他\n以上のことさえ覚えれば、とりあえず図は作れます。最後に、必須要素ではありませんが、幾何オブジェクトに使う引数について説明します。先ほど説明しましたcolorやlinetype、sizeなどはマッピングの情報として使うことも可能ですが、aes()の外側に置くことも可能です。しかし、その挙動はかなり異なります。colorがaes()の外側にある場合は、幾何オブジェクトの全要素に対して反映されます。\nたとえば、横軸が「ゲームのプレイ時間」、縦軸が「身長」の散布図を作成しるとします ( 図 13 )。ここでcolor引数を追加しますが、まずはaes()の外側に入れます。\n\n# 例1: 全ての点の色が赤になる\nデータ %>%\n  ggplot() +\n  geom_point(aes(x = ゲームのプレイ時間, y = 身長),\n             color = \"red\")\n\n\n\n\n\n\n図 13: colorをaes()外側に置いた場合\n\n\n\n\nここで注目する点は\n\n散布図におけるすべての点の色がcolorで指定した色 (\"red\" = 赤)に変更された点\ncolorの引数は変数名でなく、具体的な色を指定する点\n\n以上の2点です。aes()の内側にcolorを指定する場合 ( 図 14 ) は、以下のように変数名を指定します。たとえば、性別ごとに異なる色を付けるとしたら、\n\n# 例2: 性別ごとに点の色が変わる\nデータ %>%\n  ggplot() +\n  geom_point(aes(x = ゲームのプレイ時間, y = 身長, color = 性別))\n\n\n\n\n\n\n図 14: colorをaes()内側に置いた場合\n\n\n\n\n以上のように書きます。aes()の内部はマッピングの情報が含まれています。言い換えると、aes()の中はある変数がグラフ上においてどのような役割を果たしているかを明記するところです。2つ目の例では性別という変数が色分けをする役割を果たすため、aes()の内側に入ります。一方、1つ目の例では色分けが行われておりません。\n\n\n座標系\n座標系はデータが点や線、面などで出力される空間を意味し、coord_*()関数群を用いて操作します。\n座標系のズームイン (zoom-in) やズームアウト (zoom-out) を行うcoord_cartesian()、横軸と縦軸を交換するcoord_flip()、横軸と縦軸の比率を固定するcoord_fixed()がよく使われます。座標系の説明は次章以降で詳しく解説しますが、ここでは座標系のズームインを見てみましょう。以下の 図 15 は各国のCOVID19の感染者数を時系列で示したものです。これを見るとアメリカ、ブラジル、インドは大変だなーくらいしかわかりません。感染者数が比較的に少ない国のデータは線としては存在しますが、なかなか区別ができません。\n\n\n\n\n\n図 15: ズームイン前\n\n\n\n\nここで座標系をズームインすると、一部の情報は失われますが、ズームインされた箇所はより詳細にグラフを観察できます。ズームインと言っても難しいものではありません。単に、軸の上限、下限を調整するだけです。たとえば、縦軸の上限を10万人に変更したのが 図 16 です。アメリカなど感染者数が10万人を超える国家の時系列情報の一部は失われましたが、日中韓などのデータはより見やすくなったかと思います2。\n\n\n\n\n\n図 16: ズームイン後\n\n\n\n\nまた、同じ棒グラフや散布図、折れ線グラフでも座標系を変えることによって図の見方が劇的に変わることもあります。我々にとって最も馴染みのある座標系はデカルト座標系 (直交座標系)です。このデカルト座標系に切片0、傾き1の直線を引きます。そして同じ図に対して座標系のみを極座標系 (polar coordinates system)に変更します。この2つを比較したのが 図 17 です。この2つの図は同じデータ、同じ変数、同じ幾何オブジェクトで構成されています。異なるのは座標系ですが、見方が劇的に変わります。\n\n\n\n\n\n\n\n\n図 17: 直交座標系と極座標系の比較 (1)\n\n\n\n\nこんな座標系を実際に使う機会は多くないかも知れませんが、極座標系は割と身近なところで見ることができます。それは積み上げ棒グラフと円グラフの関係です。図 18 はデカルト座標系上の積み上げ棒グラフを極座標系に変換したものです。\n\n\n\n\n\n\n\n\n図 18: 直交座標系と極座標系の比較 (2)\n\n\n\n\n他にも軸を対数スケールなどに変換する coord_trans() 、地図の出力に使われる coord_map() や coord_sf() などがあり、適宜紹介していきます。\n\n\nスケール\n既に説明しました通り、{ggplot2}は様々な幾何オブジェクトがあり、これによってグラフ上におけるデータの示し方が変わります。例えば、散布図だとデータは点として表現され、折れ線グラフだと線、棒グラフやヒストグラムだと面で表現されます。これらの点、線、面などが持つ情報がスケール (scale)です。点だと縦軸上の位置、横軸上の位置が必ず含まれ、他にも点の形、点の大きさ、点の色、枠線の色、透明度などの情報を含むことが可能です。線も線が折れるポイントの横軸・縦軸上の位置、線の太さ、線の色などがあります。\nこの形、色、大きさ、太さなどを調整するためにはscale_*_*()関数群を使います。例えば、図 13 の場合、点は横軸上の位置、縦軸上の位置、色の3つのスケールを持っています。横軸と縦軸は連続変数 (時間と身長)、色は名目変数 (性別)です。ここで横軸のスケールを調整するためにはscale_x_continuous()レイヤーを追加します。縦軸も同様にscale_y_continuous()を使います。ここのxとyはスケール、continuousは連続変数であることを意味します。それでは、色を調整するためにはどうすれば良いでしょうか。それはscale_color_manual()です。colorは色を、manualは手動調整を意味しますが、主に性別や都道府県のような名目変数のスケール調整に使います。\nこれらのscale_*_*()関数群は点、線、面が持つ位置情報や性別を変更することではなく、その見せ方を変更するだけです。図 14 の縦軸の目盛りは「150、160、170、180」となっていますが、scale_y_continuous()を使うとこの目盛りが変わります。点の位置が変わることはありません。たとえば、以下のコードはscale_y_continuous()を使って縦軸の目盛りを5cm刻みに変更するためのコードであり、図 19 はその結果です。\n\n# scale_y_continuous()の例\nデータ %>%\n  ggplot() +\n  geom_point(aes(x = ゲームのプレイ時間, y = 身長, color = 性別)) +\n  scale_y_continuous(breaks = c(145, 150, 155, 160, 165, 170, 175, 180),\n                     labels = c(145, 150, 155, 160, 165, 170, 175, 180))\n\n\n\n\n\n\n図 19: scale_y_continuous()を使って縦軸を5cm刻みに変更\n\n\n\n\nまた、性別ごとの色を変更する際はscale_color_manual()を使います（ 図 20 ）。\n\n# scale_color_manual()の例\nデータ %>%\n  ggplot() +\n  geom_point(aes(x = ゲームのプレイ時間, y = 身長, color = 性別)) +\n  scale_y_continuous(breaks = c(145, 150, 155, 160, 165, 170, 175, 180),\n                     labels = c(145, 150, 155, 160, 165, 170, 175, 180)) +\n  scale_color_manual(values = c(\"男性\" = \"#ff9900\", \"女性\" = \"#339900\"))\n\n\n\n\n\n\n図 20: さらにscale_color_manual()を使って色を指定\n\n\n\n\nscale_*_*()関数群は以上のように、scale_スケールのタイプ_変数のタイプ()です。つまり、scale_x_date()、scale_y_discrete()やscale_color_contiuous()など様々な組み合わせが可能であり、{ggplot2}は様々なタイプのスケールと変数のための関数群を提供しています。むろん、ユーザーから独自のスケール関数を作ることも可能であり、パッケージとして公開することも可能です。\n\n\nファセット\nファセット (facet)は日本語では「面」、「切子面」などで訳されますが、誤解を招く可能性があるため、本書ではそのままファセットと訳します。ファセットとはデータの部分集合 (subset)を対象にしたグラフを意味します。\nたとえば、フリーダムハウスでは毎年、各国を「自由」、「部分的に自由」、「不自由」と格付けをし、結果を公表しています。世界に自由な国、部分的に自由な国、不自由な国が何カ国あるかを大陸ごとに示したいとします。カテゴリごとの個数を示すには棒グラフが効果的であり、 図 21 のように示すことができます。\n\n\n\n\n\n図 21: ファセットを分けないグラフの例\n\n\n\n\nこのグラフを見ると各大陸に自由な国がどれほどあるかが分かりますが、人によっては読みにくいかも知れません。できれば大陸ごとに分けたグラフの方が見やすいでしょう。そのためにはデータを特定の大陸に絞って、そのデータを用いた棒グラフを作り、最終的には出来上がったグラフたちを結合する必要があります。\nしかし、{ggplot2}のファセットを指定するとそのような手間が省けます。これはデータを大陸ごとの部分集合に分割し、1つのプロット上に小さい複数のプロットを出力します。\n\n\n\n\n\n図 22: 大陸ごとにファセットを分けたグラフの例\n\n\n\n\nファセットを指定するには facet_*() 関数群を使います。具体的には facet_wrap() と facet_grid() がありますが、今回のように1つの変数 (ここでは「大陸」)でファセットを分割する場合は主に facet_wrap() を、2つの変数で分割する場合は facet_grid() を使います。\nここまでが{ggplot2}の入門の入門の入門です。韓国旅行に例えると、やっと仁川国際空港の入国審査を通ったところです。The R Graph Galleryを見ると、主に{ggplot2}で作成された綺麗な図がいっぱいあります。しかし、ここまで勉強してきたものだけでは、このような図を作るのは難しいです。そもそもサンプルコードを見ても理解するのが難しいかも知れません。次章では本格的な{ggplot2}の使い方を解説します。到達目標は(1)「よく使う」グラフが作成できること、そして(2)The R Graph Galleryのサンプルコードを見て自分で真似できるようになることです。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro1.html#visual-principal",
    "href": "tutorial/R/ggplot_intro1.html#visual-principal",
    "title": "可視化[理論]",
    "section": "良いグラフとは",
    "text": "良いグラフとは\n本格的な作図に入る前に、「優れたグラフとは何か」について考えてみましょう。しかし、優れたグラフの条件を数ページでまとめるのは難しいです。筆者らがいつか暇になったら、これに関しても詳細に1つの章として解説しますが、ここでは参考になる資料をいくつかリストアップします。\n\nYau, Nathan. 2011. Visualize This: The FlowingData Guide to Design, Visualization, and Statistics. Wiley\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. New Riders. Princeton University Press.\n藤俊久仁・渡部良一. 2019. 『データビジュアライゼーションの教科書』秀和システム. (第5章以降)\n永田ゆかり. 2020. 『データ視覚化のデザイン』SBクリエイティブ.\nBBC Visual and Data Journalism cookbook for R graphics\n\n\nデータ・インク比\n可視化が本格的に注目を浴びるようになったのはEdward R. Tufteの1983年著作、The Visual Display of Quantitative Informationからですが、この本のメッセージは単純で、「データ・インク比 (Data-ink ratio) を最大化せよ」の一言に要約できます。データ・インク比は以下のように定義されます (Tufte 2001)。\n\\[\\begin{align}\n\\text{Data-ink ratio} = & \\frac{\\text{data-ink}}{\\text{total ink used to print the graphic}} \\\\\n= & \\text{proportion of a graphic's ink devoted to the} \\\\\n  & \\text{non-redundant display of data-information} \\\\\n= & \\text{1.0 - proportion of a graphic that can be earase} \\\\\n  & \\text{without loss of data-information.}\n\\end{align}\\]\nデータ・インク比は「データ・インク」を「グラフの出力に使用されたインクの総量」で割ったものです。データ・インクとはデータの情報を含むインクの量を意味します。これを言い換えると、グラフにおいて情報損失なしに除去できるグラフの要素が占める割合を1から引いたものです。\nここで1つの例を紹介します。たとえば、G20加盟国におけるCOVID19の累積感染者数を時系列で示すとします。そこで最近、インドにおいて感染者数が急増していることを示したいとします。まず、 図 23 から見ましょう。\n\n\n\n\n\n図 23: データ・インク比の例 (改善前)\n\n\n\n\nそもそもどの線がインドを表しているのかが分かりにくいです。カテゴリが増えると使える色に制約が生じてしまうからです。実際、図からフランス、ドイツ、インド、インドネシアを区別するのは非常に難しいでしょう。現実に色分けが出来る天才的な色覚を持つ読者ならこちらの方が情報も豊富であり、いいかも知れません。しかし、インドにおける感染者数の急増を示すには無駄な情報が多すぎます。そこでインドを除く国の色をグレーにまとめたものが 図 24 です。\n\n\n\n\n\n図 24: データ・インク比の例 (改善後)\n\n\n\n\nこちらの方は多くの情報が失われています。アメリカや日本、韓国がどの線に該当するかが分かりません。しかし、図で示したいメッセージとは無関係でしょう。ここからもう一歩踏み込んで、「ならばインド以外の線を消せばいいじゃん」と思う方もいるかも知れません。しかし、インドの線のみ残している場合、比較対象がなくなるため、急増していることを示しにくくなります。この場合、示したい情報の損失が生じるため、インド以外の国の線はデータ・インクに含まれます。\nしかし、このデータ・インク比に基づく可視化は常に正しいとは言えません。そこでもう一つの例を紹介します。 図 25 の左は Kuznicki and McCutcheon (1979) の論文に掲載された図であり、右はTufteによる改善案です。\n\n\n\n\n\n図 25: データ・インク比改善の例\n\n\n\n\n確かに棒グラフは面を使用しており、高さを示すには線のみで十分かも知れません。また、エラー・バーも線の位置を若干ずらすことによって表現できます。それでは、読者の皆さんから見て、どのグラフが見やすいと思いますか。これについて興味深い研究結果があります。Inbar, Tractinsky, and Meyer (2007) は87人の学部生を対象に3つのグループに分けました。そして、学生たちは 図 26 を「美しさ」、「明瞭さ」、「簡潔さ」の3つの面で評価し、最後にどの図が最も好きかを尋ねました。\n\nグループ1: 図 26 のAとDを評価\nグループ2: 図 26 のAとDを評価。\n\nただし、事前にTufteスタイル (図 26 のD)の読み方について学習させる。\n\nグループ3: 図 26 のA、B、C、Dを評価\n\n\n\n\n\n\n図 26: Inbar, Tractinsky, and Meyer (2007) から抜粋\n\n\n\n\n皆さんもある程度は結果が予想できたかと思いますが、いずれのグループにおいても、Tufteが推奨する 図 26 のDが「美しさ」、「明瞭さ」、「簡潔さ」のすべての点において最下位でした。また、どの図が最も好きかに対しても 表 3 のような結果が得られました。\n\n\n\n\n表 3:  Inbar, Tractinsky, and Meyer (2007)の実験結果 \n \n  \n    Group \n    Graph A \n    Graph B \n    Graph C \n    Graph D \n  \n \n\n  \n    Group 1 \n    24 \n     \n     \n    3 \n  \n  \n    Group 2 \n    29 \n     \n     \n    2 \n  \n  \n    Group 3 \n    14 \n    3 \n    12 \n    0 \n  \n\n\n\n\n\n\n図 26 のDはデータ・インク比の観点から見れば最も優れた図ですが、それが分かりやすさを意味するわけではありません。今は、人々の認知の観点からも図を評価するようになり、近年の可視化の教科書ではこれらに関しても詳しく触れているものが多いです。Tufte (2001) の本は可視化を勉強する人にとって必読の書かも知れませんが、これだけでは十分ではなく、近年の教科書も合わせて読むことをおすすめします。\n\n\n3次元プロット\nこれまで見てきた全ての図は縦軸と横軸しか持たない2次元プロットです。しかし、実際の生活において3次元プロットを見にすることは珍しくないでしょう。3次元プロットの場合、縦軸と横軸以外に、高さ (深さ)といったもう一つの軸が含まれます。しかし、多くの場合、このもう一つの軸はグラフにおいて不要な場合が多いです。たとえば、 図 27 をを見てみましょう3。これはCOVID19による定額給付金の給付済み金額を時系列で並べたものです。\n\n\n\n\n\n図 27: 3次元プロットの例 (1)\n\n\n\n\nこの図の目的は筆者にとってはよく分かりませんが、少なくとも給付金が順調（？）に配られているということですかね。その傾向を見るにはこの図は大きな問題はありません。しかし、細かい数値を見ようとすると誤解が生じる可能性があります。たとえば、7月22日の給付済み額は12.12兆円です。しかし、 図 28 を見ると、7月22日の棒は12兆円に達しておりません。なぜでしょうか。\nこれは棒と壁（？）との間隔が理由です。 図 28 の右下にある青い円を見ればお分かりかと思いますが、棒が浮いています。この棒を壁（？）側に密着すると12の線を超えると考えられますが、このままだと12兆円に達していないのに12兆円超えてると、何かの入力ミスじゃないかと考えさせるかも知れません。\n\n\n\n\n\n図 28: 3次元プロットの罠\n\n\n\n\nこの図において3D要素の必要性は0と言えます。2次元の棒グラフ ( 図 29 )、または折れ線グラフ ( 図 30 )の方がデータ・インク比の観点からも、分かりやすさからも優れていると言えるでしょう。\n\n\n\n\n\n図 29: 図 27 を2次元プロットに再構成した例 (棒グラフ)\n\n\n\n\n\n\n\n\n\n図 30: 図 27 を2次元プロットに再構成した例 (折れ線グラフ)\n\n\n\n\nただ、総務省の図はまだマシかも知れません。世の中には誤解を招かすために作成された3次元プロットもあります。以下の図は早稲田アカデミーが作成した早慶高の合格者数を年度ごとに示した図です。2001年は754人で2012年は1494人です。比較対象がないので本当に12年連続全国No.1かどうかは判断できませんが、2倍近く増加したことは分かります。ただし、棒グラフの高さを見ると、2倍どころか3倍程度に見えます。他にも一時期、合格者が減少した時期 (2002、2003年)があるにもかかわらず、あまり目立ちません。これには2つの原因があります。それは(1)多分、ベースラインが0人ではない、(2) 遠近法により遠いものは小さく見えることです。\n\n\n\n\n\n図 31: 3次元プロットの例 (2)\n\n\n\n\nこれを2次元棒グラフに直してものが 図 32 です。こちらの方が合格者をより客観的に確認することができるでしょう。\n\n\n\n\n\n図 32: 図 31 を2次元プロットに再構成した例\n\n\n\n\nむろん、3次元プロットそのものが悪いわけではありません。むしろ、3次元の方が解釈しやすい、見やすいケースもあるでしょう。それには共通点があり、深さというもう一つの軸も何らかの情報があるという点です。一方、ここでお見せしました2つの例の場合、深さは何の情報も持ちません。つまり、データ・インク比の観点から見れば望ましくない図です。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html",
    "href": "tutorial/R/ggplot_intro2.html",
    "title": "可視化[基礎]",
    "section": "",
    "text": "前ページでは{ggplot2}の仕組みおよびグラフィックの文法と良いグラフについて説明しました。本章では実際に簡単なグラフを作りながら{ggplot2}に慣れて頂きたいと思います。{ggplot2}で作れる図の種類は非常に多いですが、本章では、データサイエンスで頻繁に利用される以下の5つのプロットの作り方を紹介します。\nその他の図や、図の細かい修正については発展編で解説します。また、使用するPCの環境によっては図の文字化けが生じる可能性があります。この場合は第@ref(visual2-japanese)章を参照してください。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-data",
    "href": "tutorial/R/ggplot_intro2.html#visual2-data",
    "title": "可視化[基礎]",
    "section": "実習用データ",
    "text": "実習用データ\n　実習の前に本章で使用するデータと{ggplot2}パッケージが含まれている{tidyverse}を読み込みます。\n\nデータ1: 国家別民主主義および政治的自由データ\nデータ2: COVID-19データ\n\n　COVID19_Worldwide.csvの場合、普通に読み込むとTest_DayとTest_Totalが数値型であるにも関わらず、logical型変数として読み込まれます。read_csvはデフォルトだと最初の100行までのデータからデータ型を判断しますが、COVID19_Worldwide.csvの場合、Test_DayとTest_Totalの最初の100要素は全て欠損しており、判断不可となるため、自動的にlogical型として判断します。これを避けるために、guess_max = 10000を追加します。これは「データ型を判断するなら10000行までは読んでから判断しろ」という意味です。\n\npacman::p_load(tidyverse)\n\nCountry_df <- read_csv(\"Data/Countries.csv\")\nCOVID19_df <- read_csv(\"Data/COVID19_Worldwide.csv\", guess_max = 10000)\n\n　これらの変数はSONGが適当にインターネットなどで集めたデータであり、あくまでも実習用データとしてのみお使い下さい。各変数の詳細は以下の通りです。\n\n\n表 1: 国家別民主主義および政治的自由データの詳細\n\n\n変数名\n説明\n備考\n\n\n\n\nCountry\n国名\n\n\n\nPolulation\n人口\n人\n\n\nArea\n面積\nkm\\(^2\\)\n\n\nGDP\n国内総生産 (GDP)\n100万米ドル\n\n\nPPP\nGDP (購買力平価)\n100万米ドル\n\n\nGDP_per_capita\n一人あたりGDP\n米ドル\n\n\nPPP_per_capita\n一人あたりGDP (購買力平価)\n米ドル\n\n\nG7\nG7構成国\n1:構成国, 0:構成国以外\n\n\nG20\nG20構成国\n1:構成国, 0:構成国以外\n\n\nOECD\nOECD構成国\n1:構成国, 0:構成国以外\n\n\nHDI_2018\n人間開発指数\n2018年基準\n\n\nPolity_Score\n民主主義の程度\nPolity IVから; -10:権威主義〜10:民主主義\n\n\nPolity_Type\n民主主義の程度 (カテゴリ)\n\n\n\nFH_PR\n政治的自由の指標\n2020年基準; Freedom Houseから\n\n\nFH_CL\n市民的自由の指標\n2020年基準; Freedom Houseから\n\n\nFH_Total\n政治的自由と市民的自由の合計\n2020年基準; Freedom Houseから\n\n\nFH_Status\n総合評価\nF:完全な自由; PF:一部自由; NF:不自由\n\n\nContinent\n大陸\n\n\n\n\n\n\n\n表 2: COVID-19データの詳細\n\n\n変数名\n説明\n備考\n\n\n\n\nID\nID\n\n\n\nCountry\n国名\n\n\n\nDate\n年月日\n\n\n\nConfirmed_Day\nCOVID-19 新規感染者数\n人\n\n\nConfirmed_Total\nCOVID-19 累積感染者数\n人\n\n\nDeath_Day\nCOVID-19 新規死亡者数\n人\n\n\nDeath_Total\nCOVID-19 累積死亡者数\n人\n\n\nTest_Day\nCOVID-19 新規検査数\n人\n\n\nTest_Total\nCOVID-19 累積検査数\n人"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-barplot",
    "href": "tutorial/R/ggplot_intro2.html#visual2-barplot",
    "title": "可視化[基礎]",
    "section": "棒グラフ",
    "text": "棒グラフ\n　棒グラフについては以下の2つのタイプについて説明します。\n\nある変数の数の表す棒グラフ\n各グループの統計量を表す棒グラフ\n\n　前者は「データ内にアフリカのケースはいくつあるか、アジアの行はいくつあるか」のようなものであり、後者は「大陸ごとの民主主義の度合いの平均値はいくつか」を出力するグラフです。\n\nケース数のグラフ\n　まず、Country_dfのContinent変数における各値の頻度数を棒グラフとして出してみましょう。表としてまとめる簡単な方法はtable()関数があります。\n\ntable(Country_df$Continent)\n\n\n Africa America    Asia  Europe Oceania \n     54      36      42      50       4 \n\n\n　ケース数の棒グラフは、大陸名を横軸に、ケース数を縦軸にしたグラフです。それではグラフを作ってみます。データはCountry_dfであり、使う幾何オブジェクトはgeom_bar()です。ここで必要な情報は横軸、つまり大陸 (Continent)のみです。縦軸も「ケース数」という情報も必要ですが、{ggplot2}が勝手に計算してくれるので、指定しません。また、labs()レイヤーを追加します。labs()レイヤーはマッピング要素のラベルを指定するものです。これを指定しない場合、aes()内で指定した変数名がそのまま出力されます。\n\nCountry_df %>%\n  ggplot() +\n  geom_bar(aes(x = Continent)) +\n  labs(x = \"大陸\", y = \"ケース数\")\n\n\n\n\n\n\n\n\n　これで初めての{ggplot2}を用いたグラフが完成しましたね！\n\n\n記述統計量のグラフ\n　次は記述統計量のグラフを出してみます。たとえば、大陸ごとに民主主義の指標の1つであるPolity Scoreの平均値を図示するとします。まずは、dplyrを使って、大陸ごとにPolity Score (Polity_Score)の平均値を計算し、Bar_df1という名で保存します。\n\nBar_df1 <- Country_df %>%\n  group_by(Continent) %>%\n  summarise(Democracy = mean(Polity_Score, na.rm = TRUE),\n            .groups   = \"drop\")\n\nBar_df1\n\n# A tibble: 5 × 2\n  Continent Democracy\n  <chr>         <dbl>\n1 Africa        2.48 \n2 America       6.93 \n3 Asia          0.342\n4 Europe        7.93 \n5 Oceania       7.25 \n\n\n　それでは、このBar_df1を基にグラフを作りますが、今回は縦軸の情報も必要です。横軸はContinent、縦軸はDemocracy変数に指定します。そして、重要なものとしてstat引数を指定します。これはマッピングと関係なく、棒グラフの性質に関係するものなので、aes()の外側に位置します。これを指定しない場合、geom_bar()は基本的にはケース数を計算し、図示します。Passengerの値そのものを縦軸にしたい場合はstat = \"identity\"を指定します。後は、先ほどの棒グラフと同じです。\n\nBar_df1 %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = Democracy), stat = \"identity\") +\n  labs(x = \"大陸\", y = \"Polity IV スコアの平均値\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　考えてみれば、大陸名が英語になっていますね。図内の言語は統一するのが原則であり、図の言語は論文やレポート、報告書の言語とも一致させるべきです。ここはBar_df1のContinent列の値を日本語に置換するだけでいいので、recode()関数を使います。recode()の使い方はdplyr入門を参照してください。また、順番はローマ字順にしたいので、fct_inorder()を使って、Bar_df1における表示順でfactor化を行います。\n\nBar_df1 %>%\n  mutate(Continent = recode(Continent,\n                            \"Africa\"   = \"アフリカ\",\n                            \"America\"  = \"アメリカ\",\n                            \"Asia\"     = \"アジア\",\n                            \"Europe\"   = \"ヨーロッパ\",\n                            .default   = \"オセアニア\"),\n         Continent = fct_inorder(Continent)) %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = Democracy), stat = \"identity\") +\n  labs(x = \"大陸\", y = \"Polity IV スコアの平均値\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n次元を追加する\n　記述統計量のグラフを見るとマッピング要素は2つであり、これは図が2つの次元、つまり大陸とPolity IVスコアの平均値で構成されていることを意味します。記述統計量のグラフはマッピング要素は1つですが、ケース数という次元が自動的に計算されるため2次元です。ここにもう一つの次元を追加してみましょう。たとえば、大陸ごとのPolity IVスコアの平均値を出しますが、これを更にOECD加盟有無で分けてみましょう。そのためには、まず大陸とOECD加盟有無でグループを分けてPolity IVスコアの平均値を計算する必要があります。\n\nBar_df2 <- Country_df %>%\n  group_by(Continent, OECD) %>%\n  summarise(Democracy = mean(Polity_Score, na.rm = TRUE),\n            .groups   = \"drop\")\n\nBar_df2\n\n# A tibble: 9 × 3\n  Continent  OECD Democracy\n  <chr>     <dbl>     <dbl>\n1 Africa        0     2.48 \n2 America       0     6.55 \n3 America       1     8.6  \n4 Asia          0    -0.314\n5 Asia          1     8    \n6 Europe        0     5.87 \n7 Europe        1     9.12 \n8 Oceania       0     4.5  \n9 Oceania       1    10    \n\n\n　続いて、ContinentとOECD列を日本語に直します。また、順番を指定するためにfactor化しましょう。\n\nBar_df2 <- Bar_df2 %>%\n  mutate(Continent = recode(Continent,\n                            \"Africa\"   = \"アフリカ\",\n                            \"America\"  = \"アメリカ\",\n                            \"Asia\"     = \"アジア\",\n                            \"Europe\"   = \"ヨーロッパ\",\n                            .default   = \"オセアニア\"),\n         Continent = fct_inorder(Continent),\n         OECD      = recode(OECD,\n                            \"0\" = \"OECD非加盟国\",\n                            \"1\" = \"OECD加盟国\"),\n         OECD      = fct_inorder(OECD)) \n\nBar_df2\n\n# A tibble: 9 × 3\n  Continent  OECD         Democracy\n  <fct>      <fct>            <dbl>\n1 アフリカ   OECD非加盟国     2.48 \n2 アメリカ   OECD非加盟国     6.55 \n3 アメリカ   OECD加盟国       8.6  \n4 アジア     OECD非加盟国    -0.314\n5 アジア     OECD加盟国       8    \n6 ヨーロッパ OECD非加盟国     5.87 \n7 ヨーロッパ OECD加盟国       9.12 \n8 オセアニア OECD非加盟国     4.5  \n9 オセアニア OECD加盟国      10    \n\n\n　これで作図の準備ができました。それではこの新しい次元であるOECDをどのように表現すれば良いでしょうか。Continentは横軸の位置で表現され、Democracyは縦軸の位置として表現されているため、xとy以外の要素を考えてみましょう。棒グラフの場合、もう一つの軸が追加されるとしたらそれは、棒の色です。棒の色はfillで指定できます。colorでないことに注意してください。colorも指定可能ですが、これは棒の色ではなく、棒を囲む線の色であり、通常は「なし」となっています。それではfillをaes()内に書き加えます。\n\nBar_df2 %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = Democracy, fill = OECD), \n           stat = \"identity\") +\n  labs(x = \"大陸\", y = \"Polity IV スコアの平均値\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　なんか思ったものと違うものが出てきました。たとえば、アメリカ大陸の場合、Polity IVスコアの平均値が約15ですが、明らかにおかしいです。なぜならPolity IVスコアの最大値は10だからです。これは2つの棒が積み上げられているからです。アメリカ大陸においてOECD加盟国の平均値は8.6、非加盟国のそれは6.55であり、足したら15.15になります。これをずらすためにはpositionを設定する必要があります。しかし、positionというのはBar_df2の何かと変数の値を表すわけではないため、aes()の外側に入れます。そして、その値ですが、ここでは\"dodge\"を指定します。これは棒の位置が重ならないように調整することを意味します。このpositionのデフォルト値は\"stack\"であり、言葉通り「積み上げ」です。\n\nBar_df2 %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = Democracy, fill = OECD), \n           stat = \"identity\", position = \"dodge\") +\n  labs(x = \"大陸\", y = \"Polity IV スコアの平均値\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　これで私たちが期待した図が出来上がりました。「\"dodge\"の方が普通なのになぜデフォルトが\"stack\"か」と思う方もいるかも知れませんが、実は\"stack\"も頻繁に使われます。それはケース数のグラフにおいてです。\n　たとえば、大陸ごとにOECD加盟/非加盟国を計算してみましょう。\n\nBar_df3 <- Country_df %>%\n  group_by(Continent, OECD) %>%\n  summarise(N = n(),\n            .groups   = \"drop\") %>%\n  mutate(Continent = recode(Continent,\n                            \"Africa\"   = \"アフリカ\",\n                            \"America\"  = \"アメリカ\",\n                            \"Asia\"     = \"アジア\",\n                            \"Europe\"   = \"ヨーロッパ\",\n                            .default   = \"オセアニア\"),\n         Continent = fct_inorder(Continent),\n         OECD      = recode(OECD,\n                            \"0\" = \"OECD非加盟国\",\n                            \"1\" = \"OECD加盟国\"),\n         OECD      = fct_inorder(OECD)) \n\nBar_df3\n\n# A tibble: 9 × 3\n  Continent  OECD             N\n  <fct>      <fct>        <int>\n1 アフリカ   OECD非加盟国    54\n2 アメリカ   OECD非加盟国    31\n3 アメリカ   OECD加盟国       5\n4 アジア     OECD非加盟国    39\n5 アジア     OECD加盟国       3\n6 ヨーロッパ OECD非加盟国    23\n7 ヨーロッパ OECD加盟国      27\n8 オセアニア OECD非加盟国     2\n9 オセアニア OECD加盟国       2\n\n\n　これを可視化したのが以下の図です。\n\nBar_df3 %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = N, fill = OECD), \n           stat = \"identity\") +\n  labs(x = \"大陸\", y = \"国家数\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　この図はposition = \"dodge\"でも良いと思いますが、大陸内の比率を考えるならposition = \"stack\"でも問題ないでしょう。また、積み上げグラフの特性上、大陸ごとの国数の合計も一瞬で判別できるといった長所もあります。position = \"dodoge\"だと、それが難しいですね。むろん、積み上げ棒グラフはベースラインが一致したいため、避けるべきという人も多いですし、著者 (SONG)も同意見です。どの図を作成するかは分析者の責任で判断しましょう。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-histogram",
    "href": "tutorial/R/ggplot_intro2.html#visual2-histogram",
    "title": "可視化[基礎]",
    "section": "ヒストグラム",
    "text": "ヒストグラム\n　ヒストグラムは棒グラフと非常に形が似ていますが、横軸が大陸のような離散変数でなく、連続変数であるのが特徴です。連続変数をいくつの区間 (階級)に分け、その区間内に属するケース数 (度数)を示したのが度数分布表、そして度数分布表をかしかしたものがヒストグラムです。連続変数を扱っているため、棒間に隙間がありません。それでもケース数の棒グラフと非常に似通っているため、マッピングの仕方も同じです。異なるのは幾何オブジェクトがgeom_bar()でなく、geom_histogram()に変わるくらいです。世界の富がどのように分布しているかを確認するために、Country_dfのGDPのヒストグラムを作ってみます。\n\nCountry_df %>% \n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (100万米ドル)\", y = \"度数\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　ケース数の棒グラフのコードとほぼ同じです。横軸の数値が2.0e+07になっているのは2 \\times 10^7、つまり2千万を意味します。普通に表記すると20000000になりますね。また、GDPの単位は100万ドルであるため、実際のGDPは20兆ドルになります。つまり、今のヒストグラムにおいて横軸の目盛りは5兆ドルになっています。この軸の数値を「0, 5e+06, 1e+07, 1.5e+07, 2e+07」から「0, 5, 10, 15, 20」 にし、X軸のラベルを「国内総生産 (100万米ドル)」から「国内総生産 (兆米ドル)」に替えてみましょう。ここで使うのはscale_x_continuous()関数です。これは横軸 (X軸)が連続変数 (continuous)の場合のスケール調整関数です。目盛りの再調整にはbreaksとlabels引数が必要です。breaksは新しい目盛りの位置、labelsは目盛りに表記する値です。それぞれベクトルが必要であり、breaksとlabelsの実引数の長さは必ず一致する必要があります。また、breaksは数値型ベクトルですが、labelsは数値型でも文字型でも構いません。\n\nCountry_df %>% \n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (兆米ドル)\", y = \"度数\") +\n  scale_x_continuous(breaks = c(0, 5000000, 10000000, 15000000, 20000000),\n                     labels = c(0, 5, 10, 15, 20)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　これで一通りヒストグラムが完成しました。ほんの一部の国は非常に高いGDPを誇っていることが分かります。GDPが10兆ドル以上の国はアメリカと中国のみであり、5兆ドルを国まで拡大しても日本が加わるだけです。そもそも1兆ドルを超える国はデータには16カ国しかなく、90%以上の国が図の非常に狭い範囲内 (0~1兆ドル)に集まっていることが分かります。\n\n\n\n\n\n\n\n\n\n　この場合、2つの方法が考えられます。1つ目は方法は情報の損失を覚悟した上で、GDPが1兆ドル未満の国でヒストグラムを書く方法です。これはデータをggplot()関数を渡す前にfilter()を使って、GDPが100万未満のケースに絞るだけで出来ます。ただし、横軸の最大値が2000万でなく、100万になるため、目盛りを調整した方が良いでしょう。\n\nCountry_df %>% \n  filter(GDP < 1000000) %>%\n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (兆米ドル)\", y = \"度数\") +\n  scale_x_continuous(breaks = seq(0, 1000000, 100000),\n                     labels = seq(0, 1, 0.1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　2つ目の方法は横軸を対数化することです。GDPを底10の対数化 (常用対数)をすると、10兆のような非常に大きい値があっても比較的に狭い範囲内にデータを収めることが出来ます。たとえば、10を常用対数化すると1, 1000は3, 10000000は7になります。自然対数 (底が\\(e\\))も可能ですが、「読む」ためのグラフとしては底が10の方が読みやすいでしょう。横軸の変数が対数化されるということは、横軸のスケールを対数化することと同じです。そのためにはscale_x_continuous()内にtrans引数を指定し、\"log10\"を渡します。\n\nCountry_df %>% \n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (兆米ドル)\", y = \"度数\") +\n  scale_x_continuous(breaks = seq(0, 20000000, by = 5000000),\n                     labels = seq(0, 20, by = 5),\n                     trans = \"log10\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　対数化することによってGDPの分布が綺麗な形になりました。対数化すると横軸における目盛りの間隔が等間隔でないことに注意すべきです。0から5兆ドルの距離はかなり広めですが、5兆から10兆までの距離は短くなり、10兆から15兆までの距離は更に短くなります。したがって、この図から「世界のGDPは鐘型に分布している」と解釈することは出来ません。分布を可視化するには対数化する前の図が適します。\n　対数化のもう一つの方法はscale_x_continuous()の代わりにscale_x_log10()を使うことです。使い方はscale_x_continuous()と同じですが、trans = \"log10\"の指定は不要です。\n\nCountry_df %>% \n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (兆米ドル)\", y = \"度数\") +\n  scale_x_log10() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　横軸を修正するにはscale_x_continuous()と同様、breaksとlabels引数を指定します。\n\nCountry_df %>% \n  ggplot() +\n  geom_histogram(aes(x = GDP)) +\n  labs(x = \"国内総生産 (兆米ドル)\", y = \"度数\") +\n  scale_x_log10(breaks = c(0, 1000, 10000, 100000, 1000000, 10000000),\n                labels = c(0, 0.001, 0.01, 0.1, 1, 10)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　他にもcoord_*()関数群、つまり座標系の操作を用いて軸を対数化することも可能です。他にも、データをggplot()を渡す前に変数を対数化するのもありでしょう。プログラミングにおいてある結果にたどり着く方法は複数あるので、色々試してみるのも良いでしょう。\n\nヒストグラムの棒の大きさを調整する\n　棒グラフのようにもう一つの次元を追加してみましょう。まず、人間開発指数 (HDI_2018)の分布を示してみましょう。\n\nCountry_df %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018)) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　これでもヒストグラムとしては十分すぎるかも知れませんが、色々調整してみましょう。まずは、棒の枠線を白にしてみましょう。枠線はデータ内の変数に対応していないため、aes()の外側に入れます。枠線を指定する引数はcolorです。ちなみに棒の色を指定する引数はfillです。また、警告メッセージも気になるので、HDI_2018が欠損している行を除外します。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), color = \"white\") +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　人によってはこちらの方が見やすかも知れません。\n　これで一通りの作図は出来ましたが、これまで無視してきたメッセージについて考えてみましょう。\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n　これは「連続変数HDI_2018を30区間 (=階級)に分けました」という意味です。この区間数を調整する方法は2つあり、(1) 区間数を指定する、(2) 区間の幅を指定する方法があります。\n　区間数を指定することはすなわちヒストグラムの棒の数を指定することであり、bins引数で調整可能です。たとえば、棒の数を10個にするためにはgeom_histogram()内にbins = 10を指定します。むろん、棒の数もデータとは無関係であるため、aes()の外側に位置します。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), color = \"white\", bins = 10) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　数えてみると棒が10個だということが分かります。\n　他にも区間の幅を指定することも可能です。区間の幅は棒の幅と一致します。たとえば、棒の幅を0.1にしてみましょう。棒の幅はbinwidthで調整可能です。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), color = \"white\", binwidth = 0.1) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　ヒストグラムが出力されましたが、棒の幅がbinwidthで指定した0.1と一致することが分かります。たとえば、一番左の棒は0.35から0.45まで、つまり幅が0.1です。そして、一番右の棒は0.95から1.05に渡って位置します。ただし、ここでに疑問を持つ読者もいるでしょう「。なぜ0.3から0.4、0.4から0.5、…ではなく、0.35から0.45、0.45から0.55なのか」です。これは{ggplot2}の基本仕様です。ヒストグラムは度数分布表を基に作成されますが、本グラフの度数分布表は以下のようになります。\n\n\n\nから\nまで\n度数\n\n\n\n\n0.35\n0.45\n10\n\n\n0.45\n0.55\n26\n\n\n0.55\n0.65\n22\n\n\n0.65\n0.75\n37\n\n\n0.75\n0.85\n47\n\n\n0.85\n0.95\n37\n\n\n0.95\n1.05\n1\n\n\n\n　簡単に言うと、ヒストグラムの最初の棒は0を中央にした上で、棒の幅を0.1にしたとも言えます。これによってヒストグラムの境界線 (boundary)が、データより左右に0.05 (binwidthの半分)ずつ広くなります。もし、これを調整したい場合は、boundary引数を指定します。指定しない場合、boundaryは「棒の広さ / 2」となります。棒がデータの範囲を超えないようにするためには、geom_histogram()内にboundary = 0を指定します。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), color = \"white\", \n                 binwidth = 0.1, boundary = 0) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　ここまで抑えとけば、普段使われるヒストグラムは問題なく作れるでしょう。\n\n\n密度を追加する\n　ヒストグラムには以下のように密度の表す線を同時に載せるケースもあります。\n\n\n\n\n\n\n\n\n\n　この密度の線を追加するにはgeom_density()という幾何オブジェクトを追加する必要があります。密度の線を示すには、横軸と縦軸両方の情報が必要です。横軸はHDI_2018で問題ないですが、縦軸はどうでしょう。縦軸には密度の情報が必要ですが、Country_dfにそのような情報はありません。幸い、{ggplot2}はy = ..density..と指定するだけで、自動的にHDI_2018のある時点における密度を計算してくれます。したがって、マッピングはaes(x = HDI_2018, y = ..density..)のように書きます。こうなるとマッピング要素xはgeom_histogram()とgeom_density()に共通するため、ggplot()に入れても問題ありません。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  # 全幾何オブジェクトにおいてxを共有するため、ここでマッピング指定\n  ggplot(aes(x = HDI_2018)) +\n  geom_histogram(color = \"white\", binwidth = 0.05, boundary = 0) +\n  geom_density(aes(y = ..density..), size = 1) +\n  labs(x = \"人間開発指数 (2018)\", y = \"密度\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　なんとも言えない微妙なグラフが出来ました。考えたものと全然違いますね。これはなぜでしょうか。それはgeom_density()は密度を表す一方、geom_histogram()は度数を表すからです。2つは単位が全然違います。したがって、どちらかに単位を合わせる必要があり、この場合はヒストグラムの縦軸を度数でなく、密度に調整する必要があります。ヒストグラムの縦軸を密度にするためには、y = ..density..を指定するだけです。こうなると、geom_histogram()とgeom_density()はxとyを共有するため、全部ggplot()内で指定しましょう。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot(aes(x = HDI_2018, y = ..density..)) +\n  geom_histogram(color = \"white\", binwidth = 0.05, boundary = 0) +\n  geom_density(size = 1) +\n  labs(x = \"人間開発指数 (2018)\", y = \"密度\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　これで密度を表す線が出来ました。\n\n\n次元を追加する\n　次元を追加するには棒グラフと同様、aes()内にマッピング要素を追加します。たとえば、OECD加盟有無によって人間開発指数の分布がどう異なるかを確認してみましょう。OECD変数は0/1であり、Rでは連続変数扱いになっているため、これを離散変数化するためには文字型かfactor型にする必要があります。ifelse()を使って、OECD == 1なら「OECD加盟国」、OECD == 0なら「OECD非加盟国」と置換したものをOECD2という列として追加します。そして、OECD2ごとに棒の色を変えるために、マッピング要素としてfillを追加します。\n\nCountry_df %>%\n  mutate(OECD2 = ifelse(OECD == 1, \"加盟国\", \"非加盟国\")) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018, fill = OECD2), \n                 color = \"white\", binwidth = 0.05, boundary = 0) +\n  # fill要素のラベルをOECDに変更\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\", fill = \"OECD\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　ヒストグラムが出来上がりました。OECD加盟国の場合、人間開発指数が相対的に高いことが分かります。しかし、積み上げヒストグラムになっています。これはある階級においてOECD加盟国と非加盟国の比率を比較する際に有効ですが、OECD加盟国と非加盟国の分布の違いを見るにはやや物足りません。したがって、棒グラフ同様、position引数で棒の位置を調整します。ただし、ヒストグラムの場合、postion = \"dodge\"は向いていないので、ここではposition = \"identity\"を指定します。しかし、この場合、棒が重なってしまうと、一方の棒が見えなくなる可能性もあるので、alpha引数で棒の透明度を調整します。alphaの値は0から1までであり、0になると、完全透明になります。ここでは0.5くらいにしてみましょう。\n\nCountry_df %>%\n  mutate(OECD2 = ifelse(OECD == 1, \"OECD加盟国\", \"OECD非加盟国\")) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018, fill = OECD2), \n                 color = \"white\", alpha = 0.5, position = \"identity\",\n                 binwidth = 0.05, boundary = 0) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\", fill = \"OECD\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　これで2つのヒストグラムを綺麗にオーバーラッピングできました。また、先ほど紹介しましたgeom_density()オブジェクトを重ねることも可能です。\n\nCountry_df %>%\n  mutate(OECD2 = ifelse(OECD == 1, \"OECD加盟国\", \"OECD非加盟国\")) %>%\n  filter(!is.na(HDI_2018)) %>%\n  # xはHDI_2018、yは密度 (..density..)とする\n  ggplot(aes(x = HDI_2018, y = ..density..)) +\n  geom_histogram(aes(fill = OECD2), \n                 color = \"white\", alpha = 0.5, position = \"identity\",\n                 binwidth = 0.05, boundary = 0) +\n  # OECD2ごとに異なる色を付ける。線の太さは1、凡例には表示させない\n  geom_density(aes(color = OECD2), size = 1, show.legend = FALSE) +\n  # 縦軸のラベルを「密度」に変更\n  labs(x = \"人間開発指数 (2018)\", y = \"密度\", fill = \"OECD\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　この場合、OECD加盟国と非加盟国の密度をそれぞれ計算するため、ヒストグラムの見た目がこれまでのものと変わることに注意してください。\n　ここまで、次元拡張の方法としてヒストグラムのオーバーラッピングについて紹介しました。しかし、ヒストグラムを重なってしまうと、読みにくい人もいるかも知れません。オーバーラップ以外の方法はプロットを2つに分けることです。つまり、1つのプロットに小さいブロットを複数載せることであり、この小さいプロットをファセット (facet)と呼びます。これにはfacet_*()関数群の中の、facet_wrap()関数を使います。facet_wrap(~ 分ける変数名)を追加すると変数ごとにプロットを分割してくれます。ncolやnrow引数を指定すると、ファセットの列数や行数も指定可能です。\n　それではやってみましょう。facet_wrap(~ OECD2)を追加するだけです。2つのファセットを縦に並べるために、ncol = 1を追加します。2つのファセットを1列に並べるという意味です。\n\nCountry_df %>%\n  mutate(OECD2 = ifelse(OECD == 1, \"OECD加盟国\", \"OECD非加盟国\")) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), \n                 color = \"white\",\n                 binwidth = 0.05, boundary = 0) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\", fill = \"OECD\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  facet_wrap(~ OECD2, ncol = 1) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　OECDは加盟/非加盟だけですから、オーバーラッピングされたヒストグラムで十分かも知れません。しかし、大陸のように、3つ以上のグループになると、オーバーラッピングよりもファセットで分けた方が効率的です。たとえば、大陸ごとの人間開発指数のヒストグラムを作ってみましょう。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_histogram(aes(x = HDI_2018), \n                 color = \"white\",\n                 binwidth = 0.1, boundary = 0) +\n  labs(x = \"人間開発指数 (2018)\", y = \"ケース数\", fill = \"OECD\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1),\n                     labels = seq(0, 1, 0.1)) +\n  facet_wrap(~ Continent, ncol = 3) +\n  theme_bw()"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-box",
    "href": "tutorial/R/ggplot_intro2.html#visual2-box",
    "title": "可視化[基礎]",
    "section": "箱ひげ図",
    "text": "箱ひげ図\n　データの分布を示す際には、離散変数ならケース数 (度数)の棒グラフ、連続変数ならヒストグラムがよく使われますが、連続変数の場合、もう一つの方法があります。それが箱ひげ図です。箱ひげ図はヒストグラムより情報量がやや損なわれますが、データの中央値、四分位点（四分位範囲）、最小値と最大値の情報を素早く読み取れます。また、複数の変数、またはグループの分布を1つのプロットに示すにはヒストグラムより優れています。\n　まずは人間開発指数の箱ひげ図を出してみます。使用する幾何オブジェクトはgeom_boxplot()です。そして、指定するマッピング要素はyのみです。箱ひげ図から読み取れる情報は最小値・最大値、中央値、第1四分位点、第3四分位点ですが、これらの情報は縦軸として表現されます。それでは、とりあえず実際に作ってみましょう。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_boxplot(aes(y = HDI_2018)) +\n  labs(y = \"人間開発指数 (2018)\")\n\n\n\n\n\n\n\n\n　箱ひげ図の読み方は以下の通りです。\n\n\n\n\n\n\n\n\n\n　これで箱ひげ図は完成ですが、人間開発指数は0から1の相対を取るので、座標系の縦軸を調整してみましょう。座標系の操作はcoord_*()関数群を使いますが、現在使っているのは直交座標系（デカルト座標系）ですのでcoord_cartesian()を使います。ここで縦軸の上限と下限を指定する引数がylimであり、長さ2の数値型ベクトルが必要です。下限0、上限1ですので、ylim = c(0, 1)とします。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_boxplot(aes(y = HDI_2018)) +\n  labs(y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n　まだまだ改善の余地はありますが、それなりの箱ひげ図の出来上がりです。しかし、一変数の箱ひげ図はあまり使われません。変数が1つだけならヒストグラムの方がより情報量は豊富でしょう。情報量が豊富ということはヒストグラムから（完璧には無理ですが）箱ひげ図を作ることは可能である一方、その逆は不可能か非常に難しいことを意味します。\n　ヒストグラムが力を発揮するのは複数の変数、または複数のグループごとの分布を比較する際です。先ほど、大陸ごとの人間開発指数の分布を確認するためにヒストグラムを5つのファセットで分割しました。箱ひげ図なら1つのグラフに5つの箱を並べるだけです。これは横軸を大陸 (Continent)にすることを意味します。先ほどのコードのマッピングにx引数を追加してみましょう。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_boxplot(aes(x = Continent, y = HDI_2018)) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n　いかがでしょうか。5大陸の分布を素早く確認することができました。ヨーロッパの場合、人間開発指数が高く、バラツキも小さいことが分かります。一方、アジアとアフリカはバラツキが非常に大きいですね。アメリカ大陸はバラツキは非常に小さいですが、極端に高い国や低い国が含まれています。このアメリカ大陸に3つの点がありますが、これは外れ値です。つまり、最小値より小さい、または最大値より大きいケースを表します。最小値より小さい、または最大値より大きいという表現に違和感を感じるかも知れません。実は一般的な箱ひげ図の最小値は「第1四分位点 - 1.5 \\(\\times\\) 四分位範囲」より大きい値の中での最小値です。同じく最大値は「第3四分位点 + 1.5 \\(\\times\\) 四分位範囲」より小さい値の中での最大値です。普通に分布している場合、ほとんどのケースは箱ひげ図の最小値と最大値の範囲内に収まりますが、極端に大きい値、小さい値が含まれる場合は箱ひげ図の最小値と最大値の範囲からはみ出る場合があります。\n　また、複数の箱を並べる際、それぞれの箱に異なる色を付ける場合があります。これはデータ・インク比の観点から見れば非効率的ですが、可読性を落とすこともないのでさほど問題はないでしょう。先ほどの箱ひげ図の場合、大陸ごとに異なる色を付けるにはマッピング要素としてfill = Continentを追加するだけです。この場合、色に関する凡例が自動的に出力されますが、既に横軸で大陸の情報が分かるため、この判例は不要でしょう。したがって、aes()の外側にshow.legned = FALSEを付けます。これは当該幾何オブジェクトに関する凡例を表示させないことを意味します。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot() +\n  geom_boxplot(aes(x = Continent, y = HDI_2018, fill = Continent),\n               show.legend = FALSE) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n　彩りどりでちょっとテンションが上がる箱ひげ図ができました。\n\n個別のデータを表示する\n　箱ひげ図は複数のグループや変数の分布を素早く比較できる長所がありますが、中央値、最小値、最大値、第1・3四分位点、外れ値の情報しか持ちません。人によってはもうちょっと情報量を増やすために個々の観測値を点として示す場合もあります。点を出力する幾何オブジェクトは次節で紹介するgeom_point()です。以下の内容は散布図の節を一読してから読むのをおすすめします。\n　点の幾何オブジェクトにおける必須マッピング要素は点の横軸の位置と縦軸の位置、つまりxとyです。今回の場合、点の横軸が大陸（Continent）、縦軸は人間開発指数（HDI_2018）です。つまり、geom_boxplot()のマッピングと同じです。したがって、xとyはggplot()に入れておきます。あとは大陸ごとに店の色分けをしたいので、color引数をaes()内に指定します。また、点が重なる場合、読みづらくなる可能性があるため、alpha引数をaes()外に指定して透明度を調整します。また、箱ひげ図もある程度は透明にしないと、裏にある点が見えないため、こちらもalphaを調整します。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  # 全幾何オブジェクトにおいてxとyは共有されるため、ここで指定\n  ggplot(aes(x = Continent, y = HDI_2018)) +\n  geom_point(aes(color = Continent), alpha = 0.5,\n             show.legend = FALSE) +\n  geom_boxplot(aes(fill = Continent),\n               alpha = 0.5, show.legend = FALSE) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n　点が透明ではあるものの、それでも重なっている箇所は相変わらず読みにくいです。この場合有効な方法がジッター（jitter）です。これは点の位置に若干のノイズを付けることによって、点が重ならないようにすることです。ジッターの方法は2つありますが、ここではジッター専用の幾何オブジェクトgeom_jitter()を使います1。geom_jitter()はノイズが追加された散布図ですので、geom_point()と使い方はほぼ同じです。\n\nCountry_df %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot(aes(x = Continent, y = HDI_2018)) +\n  geom_jitter(aes(color = Continent),\n              show.legend = FALSE) +\n  geom_boxplot(aes(fill = Continent),\n               alpha = 0.5, show.legend = FALSE) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n　これで点が重ならなくなりましたが、ちょっと散らばりすぎるという印象もあります。この散らばり具合を調整する引数がwidthとheightです。もちろん、これはデータの中身に対応する要素ではないため、aes()の外側にいれます。それぞれの実引数は0から1の間の数値になりますが、数字が大きいほど散らばり具合が大きくなり、0になるとジッター無しの散布図と同じものになります。ここでは横の散らばり具合を0.15（width = 0.15）、縦の散らばり具合は0（height = 0）にしてみましょう。そして、横軸が英語のままなので、これも日本語に直します。\n\nCountry_df %>%\n  mutate(Continent2 = factor(Continent,\n                             levels = c(\"Africa\", \"America\", \"Asia\",\n                                        \"Europe\", \"Oceania\"),\n                             labels = c(\"アフリカ\", \"アメリカ\", \"アジア\",\n                                        \"ヨーロッパ\", \"オセアニア\"))) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot(aes(x = Continent2, y = HDI_2018)) +\n  geom_jitter(aes(color = Continent2),\n              width = 0.15, height = 0,\n              show.legend = FALSE) +\n  geom_boxplot(aes(fill = Continent2),\n               alpha = 0.5, show.legend = FALSE) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n次元を追加する\n　箱ひげ図に次元を追加する方法はこれまで見てきたように、1つのグラフに次元を追加するか、次元でファセットを分割するかの問題になります。まずは、簡単なファセット分割からやってみます。追加する次元は先進国か否かです。先進国の基準は不明瞭ですが、ここではG7、G20、OECDいずれかに加盟していれば先進国と定義しましょう。そのためにCountry_dfにDeveloped変数を追加します。この変数はG7、G20、OECDの合計が1以上の場合は1、0の場合は0とします。そして、このDeveloped変数をfactor化します。具体的にはDevelopedが1だと\"先進国\"、0だと\"その他\"にします。あとはfacet_wrap()でファセットを分割します。\n\nCountry_df %>%\n  mutate(Continent2 = factor(Continent,\n                             levels = c(\"Africa\", \"America\", \"Asia\",\n                                        \"Europe\", \"Oceania\"),\n                             labels = c(\"アフリカ\", \"アメリカ\", \"アジア\",\n                                        \"ヨーロッパ\", \"オセアニア\")),\n         # G7 + G20 + OECDが1以上なら1、0なら0とするDeveloped変数作成\n         Developed  = ifelse(G7 + G20 + OECD >= 1, 1, 0),\n         # Developed変数のfactor化\n         Developed  = factor(Developed, levels = c(1, 0),\n                             labels = c(\"先進国\", \"その他\"),\n                             ordered = TRUE)) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot(aes(x = Continent2, y = HDI_2018)) +\n  geom_jitter(aes(color = Continent2), alpha = 0.5,\n              width = 0.15, height = 0,\n              show.legend = FALSE) +\n  geom_boxplot(aes(fill = Continent2),\n               alpha = 0.5, show.legend = FALSE) +\n  # caption引数で図の右下にテキストを入れる\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\",\n       caption = \"先進国: G7, G20, OECDのいずれかに加盟している国\") +\n  # ファセット分割\n  facet_wrap(~ Developed) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　次の方法はファセットを分割せずに次元を追加する方法です。ファセットに分ける場合、「先進国における大陸別人間開発指数の分布」、「先進国外における大陸別人間開発指数の分布」は素早く読み取れますが、「ある大陸における先進国/その他の国の人間開発指数の分布」を比較するにはあまり向いておりません。なぜなら目の動線が長いからです。先進国とその他の国を大陸ごとに横に並べると視線の動線が短くなり比較しやすくなります。\n　やり方はあまり変わりませんが、今回は色分けをContinentでなくDevelopedでする必要があります。1つの画面に先進国とその他の国の情報が同時に出力されるため、この2つを見分けるためには色分けが有効です。したがって、geom_jitter()とgeom_boxplot()のcolorとfillをContinentからDevelopedへ変更します。\n　そしてもう一つ重要なことがあります。それはgeom_jitter()の位置です。次元ごとに横軸の位置をずらす場合、geom_bar()はposition = \"dodge\"を使いました。しかし、geom_jitter()では\"dodge\"が使えません。その代わりにpositonの実引数としてposition_jitterdodge()を指定します。この中には更にjitter.widthとjitter.height引数があり、散らばりの具合をここで調整します。なぜ、geom_jitter()でwidthとheightを指定するのではなく、position_jitter()内で指定するかですが、これはgeom_jitter()関数の仕様です。geom_jitter()の場合、positionとwidthまたはheightを同時に指定することは出来ません。 |　 　それでは早速やってみましょう。\n\nCountry_df %>%\n  mutate(Continent2 = factor(Continent,\n                             levels = c(\"Africa\", \"America\", \"Asia\",\n                                        \"Europe\", \"Oceania\"),\n                             labels = c(\"アフリカ\", \"アメリカ\", \"アジア\",\n                                        \"ヨーロッパ\", \"オセアニア\")),\n         Developed  = ifelse(G7 + G20 + OECD >= 1, 1, 0),\n         Developed  = factor(Developed, levels = c(1, 0),\n                             labels = c(\"先進国\", \"その他\"),\n                             ordered = TRUE)) %>%\n  filter(!is.na(HDI_2018)) %>%\n  ggplot(aes(x = Continent2, y = HDI_2018)) +\n  # jitterでdodgeを使うためにはposition_jitterdodgeを使う\n  geom_jitter(aes(color = Developed), alpha = 0.5,\n              position = position_jitterdodge(jitter.width = 0.5,\n                                              jitter.height = 0),\n              show.legend = FALSE) +\n  geom_boxplot(aes(fill = Developed),\n               alpha = 0.5) +\n  labs(x = \"大陸\", y = \"人間開発指数 (2018)\", fill = \"\",\n       caption = \"先進国: G7, G20, OECDのいずれかに加盟している国\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n　ファセット分割と色分け、どれが良いかという正解はありません。分析者の目的に依存するものです。もし、先進国の中での比較を強調したい場合はファセット分割が有効でしょう。しかし、同じ大陸内で先進国とその他の国の比較なら色分けの方が適切です。\n\n\n複数の変数の場合\n　箱ひげ図はグループごとにある変数の分布を比較することもできますが、複数の変数の比較も可能です。そのためにはデータの形を変形する必要があります。たとえば、Polity IVの民主主義指標（Polity_Score）とFreedom Houseの民主主義指標（FH_Total）の分布を、大陸ごとに示したいとします。Country_dfの場合、Polity_ScoreとFH_Totalは別々の変数になっていますが、pivot_longer()を使って、これらを1つの変数にまとめる必要があります。\n\nDemocracy_df <- Country_df %>% \n  select(Country, Continent, Polity_Score, FH_Total) %>%\n  filter(!is.na(Polity_Score), !is.na(FH_Total)) %>%\n  pivot_longer(cols      = contains(\"_\"),\n               names_to  = \"Type\",\n               values_to = \"Value\") %>%\n  mutate(Type = recode(Type,\n                       \"FH_Total\"     = \"Freedom House\",\n                       \"Polity_Score\" = \"Polity IV\"))\n\nDemocracy_df\n\n# A tibble: 316 × 4\n   Country     Continent Type          Value\n   <chr>       <chr>     <chr>         <dbl>\n 1 Afghanistan Asia      Polity IV        -1\n 2 Afghanistan Asia      Freedom House    27\n 3 Albania     Europe    Polity IV         9\n 4 Albania     Europe    Freedom House    67\n 5 Algeria     Africa    Polity IV         2\n 6 Algeria     Africa    Freedom House    34\n 7 Angola      Africa    Polity IV        -2\n 8 Angola      Africa    Freedom House    32\n 9 Argentina   America   Polity IV         9\n10 Argentina   America   Freedom House    85\n# … with 306 more rows\n\n\n　続いて、箱ひげ図の作成ですが、これは次元の追加と全く同じやり方になります。fillで色分けをするか、ファセット分割をするかですね。ここでは箱の色分けをします。\n\nDemocracy_df %>%\n  ggplot() +\n  geom_boxplot(aes(x = Continent, y = Value, fill = Type)) +\n  labs(x = \"大陸\", y = \"民主主義の程度\", fill = \"指標\") +\n  scale_x_discrete(breaks = c(\"Africa\", \"America\", \"Asia\",\"Europe\", \"Oceania\"), \n                   labels = c(\"アフリカ\", \"アメリカ\", \"アジア\", \"ヨーロッパ\", \"オセアニア\"))\n\n\n\n\n\n\n\n\n　しかし、1つ問題があります。それはPolity IVは-10から10までの指標なのに対して、Freedom Houseは0から100までの指標になっている点です。この場合、正確な比較が出来ません。複数の変数を1つの箱ひげ図に出す際は、変数のスケールが一致させた方が良いでしょう。たとえば、複数の人、または団体に対する感情温度はスケールが0から100であるため、使えます。しかし、今回の場合はあまり良いケースではありません。\n　もし、スケールが異なるものを示すためにはスケールを調整する必要があります。よく使われるのはスケールを平均0、標準偏差1にする「標準化」ですが、変数の分布を似通ったものにするため、あまり良くないかも知れません。ここではFreedom House指標から50を引き、そこから5で割った値を使います。こうすることで、Freedom House指標が100なら10、0なら-10になり、最小値と最大値のスケールをPolity IVに合わせることが可能です。\n\nDemocracy_df <- Country_df %>% \n  select(Country, Continent, Polity_Score, FH_Total) %>%\n  # FH_Totalのすケース調整\n  mutate(FH_Total = (FH_Total - 50) / 5) %>%\n  filter(!is.na(Polity_Score), !is.na(FH_Total)) %>%\n  pivot_longer(cols      = contains(\"_\"),\n               names_to  = \"Type\",\n               values_to = \"Value\") %>%\n  mutate(Type2 = recode(Type,\n                        \"FH_Total\"     = \"Freedom House\",\n                        \"Polity_Score\" = \"Polity IV\"))\n\nDemocracy_df %>%\n  ggplot() +\n  geom_boxplot(aes(x = Continent, y = Value, fill = Type2)) +\n  labs(x = \"大陸\", y = \"民主主義の程度\", fill = \"指標\") +\n  scale_x_discrete(breaks = c(\"Africa\", \"America\", \"Asia\",\"Europe\", \"Oceania\"), \n                   labels = c(\"アフリカ\", \"アメリカ\", \"アジア\", \"ヨーロッパ\", \"オセアニア\"))\n\n\n\n\n\n\n\n\n　先よりは比較可能な図のように見えますが、あくまでも最小値と最大値を一致させたものであるため、厳密な意味ではこれもよくありません。複数の変数を1つの箱ひげ図としてまとめる場合は、スケールが一致するもののみを使うことを推奨します。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-scatter",
    "href": "tutorial/R/ggplot_intro2.html#visual2-scatter",
    "title": "可視化[基礎]",
    "section": "散布図",
    "text": "散布図\n　続いて、散布図の作成について解説します。散布図においてデータは点で表現され、点を表示するためには、少なくとも横軸と縦軸といった2つの情報が必要です。したがって、マッピングに使う変数は最低2つであり、横軸はx、縦軸はyで指定します。今回はCountry_dfを使って、一人当たりGDP (購買力平価基準)と人間開発指数の関係を調べてみましょう。散布図の幾何オブジェクト関数はgeom_point()です。そして、それぞれの変数はPPP_per_capita、HDI_2018であるから、マッピングはaes(x = PPP_per_capita, y = HDI_2018)になります。\n\nCountry_df %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018)) +\n  labs(x = \"一人当たり購買力平価GDP (USD)\", y = \"人間開発指数\") +\n  theme_bw()\n\nWarning: Removed 11 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n　以下のメッセージが表示されますが、これは一人当たりGDP (購買力平価基準)または人間開発指数が欠損しているケースが11カ国あることを意味します。たとえば、教皇聖座 (Holy See; いわゆるバチカン)や西サハラ、ソマリアなどの国があります。\n## Warning: Removed 11 rows containing missing values (geom_point).\n　どのようなケースが除外されたかはdplyr::filter()関数を使えば簡単に調べられます。\n\nCountry_df %>% \n  filter(is.na(PPP_per_capita) | is.na(HDI_2018)) %>% \n  select(Country, PPP_per_capita, HDI_2018)\n\n# A tibble: 11 × 3\n   Country        PPP_per_capita HDI_2018\n   <chr>                   <dbl>    <dbl>\n 1 Andorra                   NA     0.857\n 2 Cuba                      NA     0.778\n 3 Holy See                  NA    NA    \n 4 Kosovo                 11078.   NA    \n 5 Liechtenstein             NA     0.917\n 6 Monaco                    NA    NA    \n 7 San Marino             62554.   NA    \n 8 Somalia                   NA     0.557\n 9 Syria                     NA     0.549\n10 Taiwan                 46145.   NA    \n11 Western Sahara            NA    NA    \n\n\n　このように何らかのケースが除外されたとメッセージが出力された場合、ちゃんとどのケースが除外されたかを確認することは重要です。この11カ国は未承認国や国内政治の不安定によりデータが正確に把握できないところがほとんどですね。\n　散布図を見ると経済力と人間開発指数の間には正の関係が確認できます。ただし、経済力が高くなるにつれ、人間開発指数の増加幅は減少していきます。どちらかと言えば対数関数のような関係でしょう。実際、scale_x_log10()や、scale_x_continuous(trans = \"log10\")などで横軸を対数化するとほぼ線形の関係が観察できます。今回は座標系を変換して横軸を対数化してみます。座標系の調整はcoord_*()関数群を使いますが、対数化にはcoord_trans()を使います。ここで変換する軸と変換方法を指定します。今回は横軸を底10の対数化を行うからx = \"log10\"と指定します。これはscale_x_log10()と全く同じですが、縦軸も対数化するなどの場面においてはcoord_trans()の方が便利でしょう。\n\nCountry_df %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018)) +\n  labs(x = \"一人当たり購買力平価GDP (USD)\", y = \"人間開発指数\") +\n  coord_trans(x = \"log10\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　対数化してみたら、かなり綺麗な線形関係が確認できます。事実を言うと、そもそも人間開発指数は所得も評価項目であるため、線形関係があるのは当たり前です。\n\n次元を追加する\n　散布図の点は横軸の数値と縦軸の数値を持っています。2次元平面で表現できる散布図は少なくとも2つの情報を持つことになります。しかし、2次元平面であっても、3つ以上の情報、つまり3次元以上の情報を示すことが可能です。たとえば、点ごとに色を変更することもできます。OECD加盟国と非加盟国に異なる色を与えると、2次元平面上であっても、3つの情報を含む散布図が作れます。他にも透明度（alpha）、点の形（shape）、大きさ（size）などで点に情報を持たせることが可能です。たとえば、国の面積に応じて点の大きさが変わる散布図を作成するならaes()内にsize = Areaを追加するだけです。面積の単位は非常に大きいので、Areaを100万で割った値を使います。また、点が大きくなると重なりやすくなるため、半透明（alpha = 0.5）にします。\n\nCountry_df %>%\n  mutate(Area2 = Area / 1000000) %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018, size = Area2),\n             alpha = 0.5) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       size = \"面積 (100万km2)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　一人当たりGDPが非常に高い国の多くは面積が小さい国が多いですね。\n　以上のコードでは、aes()だけでなく、labs()内にもsize引数を指定しました。labs()内にxとy以外の引数を指定することはヒストグラムのところでもしましたが、説明はしませんでした。ここで詳しく説明します。labs()内の引数はマッピング要素と対応します。今回はgeom_point()幾何オブジェクト内にx、y、sizeがあり、それぞれにラベルを付けることになります。これを指定しない場合、変数名がデフォルト値として出力されます。xとyは縦軸と横軸のラベルですが、その他のマッピング要素は凡例として出力される場合が多いです。凡例のラベルを修正したい時にはとりあえずlabs()から覗いてみましょう。\n　続きまして、もう一つ次元を追加してみましょう。散布図は他のグラフに比べ次元拡張が容易なグラフです。ここでは色分けをしてみましょう。たとえば、OECD加盟有無（OECD）で色分けする場合、これまでと同様、colorをaes()内に加えるだけです。\n\nCountry_df %>%\n  mutate(Area2 = Area / 1000000) %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018, \n                 size = Area, color = OECD)) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       size = \"面積 (100万km2)\", color = \"OECD加盟有無\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　色分けはされていますが、凡例を見ると想像したものとやや違いますね。なぜならOECD変数が数値型になっているからです。実際のデータにはOECD = 0.5やOECD = 0.71のような値は存在しませんが、数値型である以上、その値をとること自体は出来ます。したがって、0から1までの数値に対応できるように、色分けもグラデーションになっています。これを見やすく二分するためには、OECD変数をfactor型か文字型に変換する必要があります。\n\nCountry_df %>%\n  mutate(Area2 = Area / 1000000,\n         OECD2 = factor(OECD, levels = 0:1, \n                      labels = c(\"非加盟国\", \"加盟国\"))) %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018, \n                 size = Area2, color = OECD2)) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       size = \"面積 (100万km2)\", color = \"OECD加盟有無\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　これで散布図の出来上がりです。2次元座標系で表現された散布図ですが、この図から分かる情報は何があるでしょうか。\n\n一人当たり購買力平価GDP\n人間開発指数\n面積\nOECD加盟有無\n\n　1つの図から4つの情報が読み取れます。つまり、4次元グラフと言えます。ここにファセット分割、透明度の調整、点の形の変更まですると7次元のグラフもできます。ただし、あまりにも次元数の多いグラフは読みにくくなるため、必要だと思う変数のみマッピングしましょう。\n\n\n一部の点をハイライトする\n　これまでの図と比べ、散布図は一般的に表示される情報が多いです。棒グラフ、箱ひげ図の場合、数個、あるいは十数個程度の項目ごとに棒や箱が出力されますが、散布図の場合、データのサイズだけ点が出力されます。むろん、散布図というのは個々の点の情報よりも二変数間の関係を確認するために使われるため、これは短所ではありません。\n　しかし、散布図の中で一部の点をハイライトしたい場合もあるでしょう。たとえば、先ほどの図において日中韓だけハイライトするためにはどうすれば良いでしょうか。まず、考えられるのはcolorによるマッピングでしょう。Country変数を日本、韓国、中国、その他の4つの水準にまとめ、この値によって色分けをすればいいでしょう。実際にやってみましょう。\n\nCountry_df %>%\n  mutate(Area2    = Area / 1000000,\n         Country2 = case_when(Country == \"Japan\"       ~ \"Japan\",\n                              Country == \"South Korea\" ~ \"South Korea\",\n                              Country == \"China\"       ~ \"China\",\n                              TRUE                     ~ \"Other\"),\n         Country2 = factor(Country2, \n                           levels = c(\"Japan\", \"South Korea\", \"China\",\n                                      \"Other\"),\n                           labels = c(\"Japan\", \"South Korea\", \"China\",\n                                      \"Other\"))) %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018, \n                 size = Area2, color = Country2)) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       size = \"面積 (100万km2)\", color = \"国家\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　日本と韓国を見つけるのは大変ですが、目的達成と言えるでしょう。ただ、もっと楽な方法があります。それが湯谷啓明さんが開発した{gghighlight}パッケージです。詳しい使い方は湯谷さんによる解説ページを参照して頂きますが、ここでは簡単な使い方のみ紹介します。まずは、install.packages(\"gghighlight\")でパッケージをインストールし、読み込みます。\n\npacman::p_load(gghighlight)\n\n　続いてですが、国ごとに色分けする予定ですので、散布図のcolorはCountry変数でマッピングします。そして、{gghighlight}幾何オブジェクトを追加します。まずは使い方からです。\n\n# gghighlight()の使い方\ngghighlight(条件式, label_params = list(引数1, 引数2, ...))\n\n　第一引数は条件式であり、これはdplyr::filter()の条件式をそのまま使えます。そして、label_parmsを指定しますが、実引数はリスト型です。中にはgeom_labels()幾何オブジェクトに使用する引数が使えます。たとえば、ハイライトされた点にはラベルが付きますが、size引数を入れてラベルの大きさを指定できます。それでは実際にやってみましょう。条件式はCountry %in% c(\"Japan\", \"South Korea\", \"China\")であり、ラベルの大きさは3にします。\n\nCountry_df %>%\n  mutate(Area2 = Area / 1000000) %>%\n  ggplot() +\n  geom_point(aes(x = PPP_per_capita, y = HDI_2018, \n                 size = Area2, color = Country)) +\n  gghighlight(Country %in% c(\"Japan\", \"South Korea\", \"China\"),\n              label_params = list(size = 3)) +\n  labs(x = \"一人あたり購買力平価GDP (USD)\", y = \"人間開発指数\",\n       size = \"面積 (100万km2)\", color = \"OECD加盟有無\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n　非常に簡単なやり方で点のハイライトが出来ました。これは後ほど紹介する折れ線グラフだけでなく、様々な幾何オブジェクトにも対応しています。湯谷さんの解説ページを参照して下さい。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-line",
    "href": "tutorial/R/ggplot_intro2.html#visual2-line",
    "title": "可視化[基礎]",
    "section": "折れ線グラフ",
    "text": "折れ線グラフ\n　続きまして、折れ線グラフについて解説します。折れ線グラフは横軸が順序付き変数、縦軸は連続変数になります。横軸は順序付きであれば、年月日のような離散変数でも、連続変数でも構いません。注意すべき点は横軸上の値はグループ内において1回のみ登場するという点です。たとえば、株価のデータなら「2020年8月8日」はデータは1回だけ登場します。世界各国の株価データなら、グループ（国）内において「2020年8月8日」は一回のみ登場することになります。\n　必要なマッピング要素は線の傾きが変化しうるポイントの横軸上の位置（x）と縦軸上の位置（y）です。ここではCOVID-19の新規感染者数データを使用します。まず、Date変数が文字型になっているため、これをDate型に変換します。文字型の場合、順序関係がないからです。基本的に折れ線グラフの横軸になり得るデータ型はnumeric、順序付きfactor、Date、complex型だと考えていただいても結構です。\n\nCOVID19_df <- COVID19_df %>%\n  mutate(Date = as.Date(Date))\n\n　それでは図を作成してみましょう。横軸は日付（Date）、縦軸は累積感染者数（Confirmed_Total）にしましょう。また、縦軸のスケールは底10の対数を取ります。感染者数が数百万人となるアメリカと、数万人の国が同じグラフになると、見にくくなるからです。\n\nCOVID19_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total)) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\") +\n  theme_minimal()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\n\n\n\n\n\n\n\n　??????????????????\n　なんでしょう…。想像したものと違いますね。また、警告メッセージも出力されますが、これは0を対数化すると-Infになるから出力されるものです。大きな問題はないので、ここでは無視しましょう。\n　先ほど申し上げた通り、横軸の値はデータ内に1回のみ登場すべきです。しかし、COVID19_dfの場合、（たとえば）2020年5月1日の行が国の数だけあります。この場合は、ちゃんと国（Country）ごとに線を分けるように指定する必要があります。ここで使うマッピング要素がgroupです。groupで指定した変数の値ごとに異なる折れ線グラフを出力してくれます。\n\nCOVID19_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, group = Country)) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　これで折れ線グラフは出力されましたが、どの線がどの国かが分かりませんね。groupの場合、凡例が表示されないので、groupでなく、colorで色分けしてみましょう。\n\nCOVID19_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, color = Country)) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　???????????????????\n　なんか出ましたが、国数が多すぎて凡例だけで図が埋め尽くされています。この場合、方法は3つあります。1つ目の方法は図のサイズを大きくする方法です。これは保存の際、サイズを再調整するだけですので、ここでは割愛します。また、それでも凡例内の項目が非常に多いグラフをグラフの種類と関係なく推奨されません。2つ目は国を絞る方法であり、3つ目はgghighlight()で一部の国のみハイライトする方法です。まずは、G7（カナダ、フランス、ドイツ、イタリア、日本、イギリス、アメリカ）のみデータを絞って折れ線グラフを作ってみましょう。\n\nG7 <- c(\"Cananda\", \"France\", \"Germany\", \"Italy\", \"Japan\", \n        \"United Kingdom\", \"United States\")\n\nCOVID19_df %>%\n  filter(Country %in% G7) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, color = Country)) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\", color = \"国\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　これだけでもG7国のCOVID-19の状況が比較可能ですが、この図は改善の余地があります。それは凡例の順番です。できれば、一番最新の日付のデータを基準に凡例の順番を揃えることが出来たら、どの線がどの国かが分かりやすくなります。そこで登場するのはdplyr入門で紹介しましたfct_reorder2関数です。実際にやってみましょう。\n\nCOVID19_df %>%\n  filter(Country %in% G7) %>%\n  mutate(Country = fct_reorder2(Country, Date, Confirmed_Total, last2)) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, color = Country)) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\", color = \"国\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　これでより読みやすい図が出来上がりました。\n\n一部の線をハイライトする\n　グループが多すぎる場合の対処法、その2つ目はデータを全て使い、一部の国のみハイライトする方法です。線のハイライトにもgghighlight()が使えます。同じく日米中韓の線のみハイライトしてみましょう。\n\nCOVID19_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, color = Country)) +\n  gghighlight(Country %in% c(\"Japan\", \"China\", \"South Korea\",\n                             \"United States\")) +\n  scale_y_continuous(breaks = c(10, 100, 1000, 10000, 100000, 1000000),\n                     labels = c(\"10\", \"100\", \"1000\", \"10000\", \n                                \"100000\", \"1000000\"),\n                     trans = \"log10\") +\n  labs(x = \"月\", y = \"累積感染者数 (人)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n　情報量の損失を最小化しながら、一部の国のみをハイライトすることによって世界における日米中韓のトレンドが確認出来ました。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-japanese",
    "href": "tutorial/R/ggplot_intro2.html#visual2-japanese",
    "title": "可視化[基礎]",
    "section": "日本語が含まれた図について",
    "text": "日本語が含まれた図について\n(以下は作成中の内容)\n　{ggplot2}で作成した図に日本語が含まれている場合、日本語が正しく表示されない場合があります。また、RStudioのPlotsペインには日本語が表示されていても、PDF/PNG/JPGなどのファイルとして出力した場合、表示されないケースもあります。{ggplot2}の使い方を解説する前に、ここでは日本語が正しく表示/出力されない場合の対処法について紹介します。\n\nmacOSの場合\n　今後紹介する{ggplot2}の図の最後に以下のようなレイヤーを追加すればヒラギノフォントを使用することができます。\n\ntheme(text = element_text(family = \"HiraginoSans-W3\"))\n\n　または、theme_bw()やtheme_minimal()など、特定のテーマを使用するのであればそのテーマのレイヤーにフォントを指定することもできます。たとえば、{ggplot2}のデフォルトテーマであるgrayテーマを使用するなら、{ggplot2}のオブジェクトに以下のようなレイヤーを追加する。\n\ntheme_gray(base_familiy = \"HiraginoSans-W3\")\n\n　もう一つの方法としましては、コードの最初に以下のようなコードを追加します。このコードが実行された以降の図には全てヒラギノフォントが適用されます。\n\ntheme_update(text = element_text(family = \"HiraginoSans-W3\"))\n\n　ただし、いずれの方法でも、geom_text()、geom_label()、annotate()のような文字列を出力する幾何オブジェクトを使用する際は、各レイヤーごとにフォント族を指定する必要があります (幾何オブジェクト内にfamily = \"HiraginoSans-W3\"を追加)。\n　作成した図ファイル (PDF) として出力 (保存) する際はquartz()関数とdev.off()関数の間に{ggplot2}で作成したオブジェクト名を入れるだけです。図でフォントが正しく指定されていれば、問題なく日本語を含まれたPDF形式の図が保存できます。\n\nquartz(file = \"出力するファイル名\", type = \"pdf\", width = 幅, height = 高さ)\n{ggplot2}で作成した図のオブジェクト名\ndev.off()\n\n　PNG形式の図を保存する場合は、後述する{ragg}パッケージの仕様をおすすめします。\n\n\nWindowsの場合\n???\n\n\n{ragg}の使用\n　{ragg}パッケージを使用すると、使用するOSと関係なくRStudioのPlotsペイン、R Markdown (HTML出力)の文字化けを回避することができます。{ragg}はインストールのみで十分であり、別途読み込む必要がないため、インストールだけします。\n\npacman::p_install(ragg)\n\n　RStudio上の文字化けを回避するためには、RStudioの設定を以下のように変更します。\n\nRStudioのTools>Global Options…\n左のGeneral>右側上段のGraphicsタブ\nGraphic DeviceのBackendでAGGを選択\n\n　R Markdown (HTML出力)の場合、YAMLヘッダーの直後に以下のようなチャンクを追加します。チャンクのオプションとしてinclude = FALSEを指定しておけば、出力結果にコードが表示されなくなるため、指定するのを推奨します。dpiは任意の値で良いですが、PNG形式の場合、解像度が低いと見た目があまり良くないため、150以上に設定することをおすすめします。Web表示だけなら150でも良いですが、印刷を考えると300がおすすめです。\n\nknitr::opts_chunk$set(dev = \"ragg_png\", dpi = 300)"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-save",
    "href": "tutorial/R/ggplot_intro2.html#visual2-save",
    "title": "可視化[基礎]",
    "section": "図の保存",
    "text": "図の保存\n　綺麗な図も出来上がりましたので、みんなに自慢してみましょう。ただ、自慢のためにパソコンを持ち歩くのは大変なので、ファイルと保存してみんなに配りましょう。自慢する相手がいないならSONGに自慢しても結構です。図のフォーマットはPNGかPDFが一般的です。JPEGなど圧縮フォーマットは絶対にダメです。PNGとPDFは前者はピクセルベース、後者がベクトルベースで、PDFの方が拡大してもカクカクせず綺麗です。ただし、一部の文書作成ソフトウェアでPDFを図として扱えない点や、サイズの問題 (複雑な図になるほど、PDFの方がサイズが大きくなる)もあるので、PNGを使うケースも多いです。むろん、LaTeXで文章を作成するなら、PDF一択です。\n　保存方法として以下の3つを紹介します。\n\nggsave()を利用した図の保存\n　ggsave()が{ggplot2}が提供する図の保存関数です。まずは、使い方から紹介します。\n\n# ggsave()を利用した保存方法\nggsave(filename = \"ファイル名\",\n       plot     = 図のオブジェクト名,\n       device   = \"pdf\"や\"png\"など,\n       width    = 図の幅,\n       height   = 図の高さ、\n       units    = \"in\"や\"cm\"など、幅/高さの単位,\n       dpi      = 図の解像度)\n\n　deviceで指定可能なフォーマットとしては\"png\"、\"eps\"、\"ps\"、\"tex\"、\"pdf\"、\"jpeg\"、\"tiff\"、\"bmp\"、\"svg\"、\"wmf\"があります。また、unitsは\"in\" (インチ)、\"cm\"、\"mm\"が指定可能です。他にもdpi引数でdpi (dots per inch; 1平方インチにどれだけのドットの表現ができるか)の指定も可能ですが、デフォルトは300となっており、出版用としては一般的なdpiとなっています。それでは、本章の最初に作成した棒グラフをBarPlot1という名で保存し、Figsフォルダー内にBarPlot1.pngとして保存してみます。幅と高さは5インチにします。\n\nBarPlot1 <- Country_df %>%\n  ggplot() +\n  geom_bar(aes(x = Continent)) +\n  labs(x = \"大陸\", y = \"ケース数\") +\n  theme_bw()\n\nggsave(filename = \"Figs/BarPlot.png\",\n       plot     = BarPlot1,\n       device   = \"png\",\n       width    = 5,\n       height   = 5,\n       units    = \"in\")\n\n\n\nquartz()を利用した図の保存\n　macOSを使用し、日本語などのマルチバイトの文字が含まれる図の場合、ggsave()が正しく作動しません。この場合はquartz()とdev.off()関数を利用して図を保存します。この関数の使い方は以下の通りです。\n\n# quartz()を利用した保存方法\nquartz(type = \"pdf\", file = \"ファイル名\", width = 図の幅, height = 図の高さ)\n作図のコード、または図オブジェクトの呼び出し\ndev.off()\n\nquartz()のwidthとheight引数の場合、単位はインチ (=2.54cm)です。\n　たとえば、最初に作成した図をFigsフォルダーのBarPlot1.pdfという名で保存し、幅と高さを5インチにするなら以下のようなコードになります。\n\n# 方法1\nquartz(type = \"pdf\", file = \"Figs/BarPlot1.pdf\", width = 5, height = 5)\nCountry_df %>%\n  ggplot() +\n  geom_bar(aes(x = Continent)) +\n  labs(x = \"大陸\", y = \"ケース数\") +\n  theme_bw()\ndev.off()\n\n　または、予め図をオブジェクトとして保存しておいたなら (先ほどのBarPlot1オブジェクトのように)、以下のようなコードも可能です。\n\n# 方法2\nquartz(type = \"pdf\", file = \"Figs/BarPlot1.pdf\", width = 5, height = 5)\nBarPlot1\ndev.off()\n\n{ragg}が導入されている状態で、図をPNG形式のファイルとして保存 (出力) したい場合は、2つの方法があります。\n\n\n{ragg}が導入済みの場合\nPNG形式限定\n方法1: ragg::agg_png()を使用する。\n　これはragg::agg_png()関数とdev.off()関数の間に{ggplot2}で作成したオブジェクト名を入れる方法でmacOSのquartz()関数とほぼ同じ使い方です。ただし、仮引数名が異なるため、注意してください。また、dpiの仮引数は存在せず、代わりにresを使います。\n\nragg::agg_png(filename = \"出力するファイル名\", res = 300, \n              width = 幅, height = 高さ)\n{ggplot2}で作成した図のオブジェクト名\ndev.off()\n\n方法2: ggsave()にdevice = ragg::agg_pngを指定する。\n　この方法は{ggplot2}が提供するggsave()関数を使用する方法です。ここではdeviceにragg:agg_pngを指定すれば、日本語が文字化けせずPNG形式のファイルとして保存されます。\n\nggsave(filename = \"出力するファイル名\", \n       plot     = 図のオブジェクト名,\n       device   = ragg::agg_png,\n       dpi      = 150,\n       width    = 幅, \n       height   = 高さ)\n\n　PDFファイルとして出力したい場合は各OSごとの説明を参照してください。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro2.html#visual2-summary",
    "href": "tutorial/R/ggplot_intro2.html#visual2-summary",
    "title": "可視化[基礎]",
    "section": "まとめ",
    "text": "まとめ\n　以上の話をまとめると、データが与えられた場合、ある図を作成するためには少なくとも以下の情報が必要です。\n\nどの幾何オブジェクトを使用するか\nその幾何オブジェクトに必要なマッピング要素は何か\n\n　座標系や、スケール、ラベルの設定なども最終的には必要ですが、これらは設定しなくても{ggplot2}自動的に生成してくれます。しかし、幾何オブジェクトとマッピングは必須です。幾何オブジェクトはグーグルで「ggplot 棒グラフ」や「ggplot 等高線図」などで検索すれば、どのような幾何オブジェクトが必要化が分かります。問題はマッピングです。この幾何オブジェクトに使える引数は何か、そしてどの引数をaes()の中に入れるべきかなどはコンソールで?geom_barplotなどで検索すれば分かります。\n　筆者のオススメは以下のチートシートを印刷し、手元に置くことです。\n\nggplot2 cheatsheet\nggplot2 aesthetics cheatsheet\n\n　2番目の資料は非常に分かりやすい資料ですが、Google Drive経由で公開されているため、いつリンクが切れかが不明です。念の為に本ページにも資料を転載します。\n\n\n\n\n\nggplot2 aesthetics cheatsheet\n\n\n\n\n　青い点は「ほとんど」のケースにおいてaes()の中に位置する引数です。ただし、geom_bar()とgeom_histogram()は自動的に度数を計算してくれるため、xとy一方だけでも可能です。黄色い資格はaes()の中でも、外でも使うことが可能な引数です。aes()の中ならマッピング要素となるため、データの変数と対応させる必要があります。ピンク色のひし形四角形は必ずaes()の外に位置する引数です。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html",
    "href": "tutorial/R/ggplot_intro3.html",
    "title": "可視化[応用]",
    "section": "",
    "text": "理論編と基礎編では{ggplot2}の概念と5つの代表的なグラフ（棒、ヒストグラム、箱ひげ図、散布図、折れ線）の作り方について説明しました。本記事では軸の調整、座標系の調整など、幾何オブジェクト以外のレイヤーについて説明します。基礎編で紹介しなかった図の作成方法については発展編で解説します。\nここで使用するデータは基礎編で使用したものと同じデータを使います。データの詳細については基礎編を参照してください。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html#visual3-labs",
    "href": "tutorial/R/ggplot_intro3.html#visual3-labs",
    "title": "可視化[応用]",
    "section": "labs(): ラベルの修正",
    "text": "labs(): ラベルの修正\n　既にlabs()レイヤーは基礎編で使ったことがあるでしょう。ここではlabs()の仕組みについて簡単に解説します。\n　labs()関数は軸、凡例、プロットのラベル（タイトルなど）を修正する際に使用する関数です。軸ラベルは横軸（x）と縦軸（y）のラベルを意味します。指定しない場合は、マッピングで指定した変数名がそのまま出力されます。これは凡例ラベルも同じです。{ggplot2}は2次元のグラフの出力に特化したパッケージであるため、出力される図には必ず横軸と縦軸があります。したがって、引数としてxとyは常に指定可能です。\n　一方、凡例はマッピングされない場合、表示されません。幾何オブジェクトのaes()内にcolor、size、linetypeなどの要素がマッピングされてから初めて凡例で表示されます。凡例が存在することは何かの要素にマッピングがされていることを意味します。このマッピング要素名（color、size、linetypeなど）をlabs()の引数として使うことで凡例のラベルが修正されます。マッピングされていない要素に対してラベルを指定しても、図に影響はありません。たとえば、Country_dfの一人あたりGDP（GDP_per_capita）を横軸、フリーダムハウスのスコア（FH_Total）を縦軸にした散布図を作成します。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = GDP_per_capita)) +\n    labs(x = \"フリーダムハウススコア\",y = \"一人あたりGDP (USD)\", \n         color = \"大陸\") +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　geom_point()は横軸と縦軸のみにマッピングをしているため、labs()にcolor =を指定しても何の変化もありません。そもそも凡例が存在しないからです。それでは大陸ごとに色分けした散布図に修正してみましょう。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = GDP_per_capita, color = Continent)) +\n    labs(x = \"フリーダムハウススコア\",y = \"一人あたりGDP (USD)\", \n         color = \"大陸\") +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\ncolorにContinent変数をマッピングすることによって、各点の色は何らかの情報を持つようになりました。そして各色がContinentのどの値に対応しているかを示すために凡例が表示されます。凡例のラベルはデフォルトは変数名（この例の場合、「Continent」）ですが、ここでは「大陸」と修正されました。\n　ここまでが基礎編で使用しましたlabs()レイヤーの使い方です。他にもlabs()はプロットのラベルを指定することもできます。ここでいう「プロットのラベル」とはプロットのタイトルとほぼ同じです。使用可能な引数はtitle、subtitle、tagです。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = GDP_per_capita, color = Continent)) +\n    labs(x = \"フリーダムハウススコア\",y = \"一人あたりGDP (USD)\", color = \"大陸\",\n         title = \"民主主義の度合いと所得の関係\", subtitle = \"大陸別の傾向\", \n         tag = \"(a)\") +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　titleは図のメインタイトルとおり、プロットのタイトルを意味します。上の図だと「民主主義の度合いと所得の関係」です。また、subtitle引数を指定することでサブタイトルを付けることも可能です。上の図の「大陸別の傾向」がサブタイトルです。最後のtagは複数の図を並べる際に便利な引数です。図が横に2つ並んでいる場合、それぞれ(a)と(b)という識別子を付けると、文中において「図3(a)は…」のように、引用しやすくなります。この「(a)」がtag引数に対応します。複数の図を並べる方法は本記事の後半にて説明します。\n最後にキャプションの付け方について説明します。たとえば、外部のデータを利用して作図を行った場合、図の右下に「データ出典：〜〜」や「Source: https://www.jaysong.net 」などを付ける場合があります。このキャプションはlabs()関数内にcaption引数を指定するだけで付けることができます。たとえば、先程の図に「Source: Freedom House」を付けてみましょう。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = GDP_per_capita, color = Continent)) +\n    labs(x = \"フリーダムハウススコア\",y = \"一人あたりGDP (USD)\", color = \"大陸\",\n         title = \"民主主義の度合いと所得の関係\", subtitle = \"大陸別の傾向\", \n         tag = \"(a)\", caption = \"Source: Freedom House\") +\n    theme_bw(base_size = 12)"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html#visual3-coordinate",
    "href": "tutorial/R/ggplot_intro3.html#visual3-coordinate",
    "title": "可視化[応用]",
    "section": "coord_*(): 座標系の調整",
    "text": "coord_*(): 座標系の調整\n　{ggplot2}はいくつかの座標系を提供しています。円グラフを作成する際に使われる極座標系（coord_polar()）や地図の出力によく使われるcoord_map()やcoord_sf()がその例です。中でも最も頻繁に使われる座標系はやはり縦軸と横軸は直交する直交座標系（デカルト座標系）でしょう。ここでは直交座標系の扱い方について解説します。\n\n直交座標系の操作\n　まずは座標系の上限と下限を指定する方法から考えましょう。日米中間のCOVID-19累積感染者数の折れ線グラフを作成してみましょう。\n\nFig1 <- COVID19_df %>%\n  mutate(Date = as.Date(Date)) %>%\n  filter(Country %in% c(\"Japan\", \"South Korea\", \"China\", \"United States\")) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_Total, color = Country)) +\n  labs(x = \"月\", y = \"累積感染者数 (人)\") +\n  theme_minimal(base_size = 12)\n\nprint(Fig1)\n\n\n\n\n\n\n\n\n　アメリカの感染者が圧倒的に多いこともあり、日韓がほぼ同じ線に見えます。これを是正するために対数変換などを行うわけですが、対数変換したグラフは直感的ではないというデメリットがあります。それでもう一つの方法として、アメリカに関する情報は一部失われますが、縦軸の上限を10万にすることが考えられます。直交座標系の上限・下限を調整する関数がcoord_cartesian()です。横軸はxlim、縦軸はylim引数を指定し、実引数としては長さ2のnumericベクトルを指定します。たとえば、縦軸の下限を0、上限を10万にするなら、ylim = c(0, 100000)となります。先ほどの図は既にFig1という名のオブジェクトとして保存されているため、ここにcoord_cartesian()レイヤーを追加してみましょう。\n\nFig1 +\n  coord_cartesian(ylim = c(0, 100000))\n\n\n\n\n\n\n\n\n3月下旬以降、アメリカの情報は図から失われましたが、日中韓についてはよりトレンドの差が区別できるようになりました。{ggplot2}は座標系の上限と下限をデータの最小値と最大値に合わせて自動的に調整してくれます。たとえば、以下のような例を考えてみましょう。\n\nFig2 <- tibble(Class = paste0(LETTERS[1:5], \"組\"),\n       Score = c(78, 80, 85, 77, 70)) %>%\n  ggplot() +\n  geom_bar(aes(x = Class, y = Score), stat = \"identity\") +\n  labs(x = \"クラス\", y = \"数学成績の平均 (点)\") +\n  theme_minimal(base_size = 12)\n\nprint(Fig2)\n\n\n\n\n\n\n\n\n　数学成績平均値は最大100点までありえますが、手元のデータにおける最高得点が85店であるため、棒グラフの縦軸の上限が85点程度となります。この場合、上限は満点である100点に調整した方が良いでしょう。\n\nFig2 +\n  coord_cartesian(ylim = c(0, 100))\n\n\n\n\n\n\n\n\n　このように上限を調整すると、成績の満点が何点かに関する情報が含まれ、グラフにより豊富な情報を持たせることが可能です。\n\n\n座標系の変換\n　続きまして座標系の変換について説明します。座標系の変換については実は基礎編でも取り上げました。対数化がその例です。例えば、連続型変数でマッピングされた横軸を底が10の対数化する場合、以下のような方法が考えれます。\n\nlog10()関数を使用し、データレベルで値を対数化する\nscale_x_continuous()レイヤーを重ね、trans = \"log10\"引数を指定する\nscale_x_log10()レイヤーを重ねる\ncoord_trans()レイヤーを重ね、x = \"log10\"引数を指定する\n\n　どの方法でも得られる結果はさほど変わりませんが、coord_trans()は座標系全般を調整することができます。たとえば、xlimやylim引数を使って座標系の上限と下限を同時に指定することも可能です。たとえば、座標系の上限を横軸は[0, 100]、縦軸は[-100, 100]とし、全て対数化を行うとします。方法はいくつか考えられます。たとえば、scale_*_log10()とcoord_cartesian()を組み合わせることもできます。\n\n# coord_trans()を使用しない場合\nggplotオブジェクト +\n  scale_x_log10() +\n  scale_y_log10() +\n  coord_cartesian(xlim = c(0, 100), ylim = c(-100, 100))\n\n　しかし、上のコードはcoord_trans()を使うと一行にまとめることができます。\n\n# coord_trans()を使用する場合\nggplotオブジェクト +\n  coord_trans(x = \"log10\", y = \"log10\", xlim = c(0, 100), ylim = c(-100, 100))\n\n　coord_trans()のx、y引数は\"log10\"以外にもあります。自然対数変換の\"log\"、反転を意味する\"reverse\"、平方根へ変換する\"sqrt\"などがあります。グラフを上下、または左右に反転する\"reverse\"は覚えておいて損はないでしょう。こちらは具体的には{tidyverse}パッケージ群に含まれている{scales}パッケージにある*_trans()関数に対応することになります。詳細は{scales}パッケージのヘルプを参照してください。\n\n\n座標系の回転\n　続きまして座標系を反時計方向回転するcoord_flip()について紹介します。以下はCountry_dfを用い、大陸（Continent）ごとにPolity IVスコア（Polity_Score）の平均値を示した棒グラフです。\n\nFlip_Fig <- Country_df %>%\n  group_by(Continent) %>%\n  summarise(Democracy = mean(Polity_Score, na.rm = TRUE),\n            .groups   = \"drop\") %>%\n  ggplot() +\n  geom_bar(aes(x = Continent, y = Democracy), stat = \"identity\") +\n  labs(x = \"大陸\", y = \"Polity IV スコアの平均値\") +\n  theme_minimal(base_size = 12)\n\nprint(Flip_Fig)\n\n\n\n\n\n\n\n\n　この図を反時計方向回転する場合は以上のプロットにcoord_flip()レイヤーを追加します。\n\nFlip_Fig +\n  coord_flip() # 座標系の回転\n\n\n\n\n\n\n\n\n　非常に簡単な方法で図を回転させることができました。しかし、実はこのcoord_flip()関数、最近になって使う場面がどんどん減っています。たとえば、先ほどのgeom_bar()幾何オブジェクトの場合、xをPolity IVスコアの平均値で、yを大陸名でマッピングすることが可能です。昔の{ggplot2}は横軸と縦軸にマッピングでいるデータ型が厳格に決まっていましたが、最近になってはますます柔軟となってきました1。coord_flip()を使用する前に、各幾何オブジェクトのヘルプを確認し、coord_flip()を用いた回転が必要か否かを予め調べておくのも良いでしょう。\n\n\n座標系のアスペクト比の固定\n　他にも地味に便利な機能として座標系比を固定するcoord_fixed()を紹介します。これは出力される座標系の「横:縦」を調整するレイヤーです。たとえば、以下のような散布図を考えてみましょう。\n\nggplot() +\n  geom_point(aes(x = 1:10, y = 1:10))\n\n\n\n\n\n\n\n\n　こちらは横と縦が同じスケールでありますが、図の大きさに応じて、見た目が変わってきます。たとえば、上の図だと、横軸における1間隔は縦軸のそれの約2倍です。もし、図を上下に大きくし、左右を縮小したら同じ図でありながら随分と見た目が変わってきます。\n\nggplot() +\n  geom_point(aes(x = 1:10, y = 1:10))\n\n\n\n\n\n\n\n\n　2つの図は本質的に同じですが、図の見せ方によって、傾きが緩やかに見せたり、急に見せたりすることができます。ここで活躍するレイヤーがcoord_fixed()です。これを追加すると横を1とした場合の縦の比率を指定することができます。\n\nggplot() +\n  geom_point(aes(x = 1:10, y = 1:10)) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\n\n　ratio = 1を指定すると縦横比は1:1となり、図の高さや幅を変更してもこの軸は変わりません。たとえば、RStudioのPlotsペインの大きさを変更すると図の大きさが変わりますが、coord_fixed(ratio = 1)を指定すると1:1の比率は維持されるまま図が拡大・縮小されます。直接やってみましょう。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html#visual3-scale",
    "href": "tutorial/R/ggplot_intro3.html#visual3-scale",
    "title": "可視化[応用]",
    "section": "scale_*_*(): スケールの調整",
    "text": "scale_*_*(): スケールの調整\n　続いてscale_*_*()関数群を用いたスケールを解説しますが、こちらの関数は非常に多く、全てのスケールレイヤーについて解説すことは難しいです。しかし、共通する部分も非常に多いです。本説ではこの共通項に注目します。\n　連続変数でマッピングされた横軸のスケールを調整する関数ははscale_x_continuous()です。ここでxが横軸を意味し、continuousが連続であることを意味します。このxの箇所は幾何オブジェクトのaes()内で指定した仮引数と一致します。つまり、scale_x_continuous()のxの箇所にはy、alpha、linetype、sizeなどがあります。そして、実引数として与えられた変数のデータ型がcontinuousの箇所に相当します。もし、離散変数ならdiscrete、時系列ならtime、全て手動で調整する場合はmanualを使います。他にも2020年10月現在、最近追加されたものとしてbinnedがあり、こちらはヒストグラムに特化したものです（scale_x_binned()とscale_y_binned()）。つまり、スケール調整関数はaes()内に登場した仮引数名とそのデータ型の組み合わせで出来ています。\n　たとえば、時系列の折れ線グラフにおいて横軸のスケールを調整するなら、sacle_x_time()を使います。また、棒グラフのように横軸が名目変数ならscale_x_manual()、順序変数のような離散変数ならscale_x_discrete()を使います。また、連続変数の縦軸のスケール調整ならscale_y_continuous()を使います。グラフによってはaes()内にxまたはyを指定しないケースもあります。基礎編において度数の棒グラフや1つの箱ひげ図を出す場合、前者はx、後者はyのみを指定しました。これは指定されていないyやxが存在しないことを意味しません。{ggplot2}が自動的に計算しマッピングを行ってくれることを意味します。{ggplot2}で出来上がった図は2次元座標系を持つため、横軸と縦軸は必ず存在します。したがって、aes()内の引数と関係なくscale_x_*()とscale_y_*()関数群は使用することが出来ます。\n\n横軸・縦軸スケールの調整\n　軸のスケールを調整する目的は1) 目盛りの調整、2) 目盛りラベルの調整、3) 第2軸の追加などがありますが、主な目的は1と2です。第2軸の追加はスケールが異なるグラフが重なる場合に使用しますが、一般的に推奨されません。したがって、ここでは1と2について説明します。\n\n連続変数の場合\n　軸に連続変数がマッピングされている場合はscale_*_continuous()レイヤーを追加します。*の箇所は横軸の場合はx、縦軸の場合はyとなります。それではCountry_dfのFH_Totalを横軸、GDP_per_capitaを縦軸にした散布図を作成し、Scale_Fig1という名のオブジェクトに格納します。\n\nScale_Fig1 <- Country_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = GDP_per_capita)) +\n    labs(x = \"フリーダムハウススコア\",y = \"一人あたりGDP (USD)\") +\n    theme_bw(base_size = 12)\n\nprint(Scale_Fig1)\n\n\n\n\n\n\n\n\n　Scale_Fig1の横軸の場合、最小値0、最大値100であり、目盛りは25間隔となっております。ここではこの横軸を調整したいと思います。まず、プロットにおける最小値と最大値はスケールではなく座標系の問題ですので、coord_*()を使用します。ここでは目盛りを修正してみましょう。たとえば、目盛りを10間隔にし、そのラベルも0、10、20、…、100にします。FH_Totalは連続変数ですので、scale_x_continuous()を使います。使い方は以下の通りです。\n\nScale_Fig1 +\n  scale_x_continuous(breaks = 目盛りの位置,\n                     labels = 目盛りのラベル)\n\n　breaksとlabelsの実引数としては数値型ベクトルを指定します。0から100まで10刻みの目盛りとラベルなら、seq(0, 100, by = 10)、またはc(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)と指定します。\n\nScale_Fig1 +\n  scale_x_continuous(breaks = seq(0, 100, by = 10),\n                     labels = seq(0, 100, by = 10))\n\n\n\n\n\n\n\n\n　目盛りのラベルを文字型にすることも可能です。例えば、目盛りを0、50、100にし、それぞれ「最低」、「中間」、「最高」としたい場合は以下のようにします。\n\nScale_Fig1 +\n  scale_x_continuous(breaks = c(0, 50, 100),\n                     labels = c(\"最低\", \"中間\", \"最高\"))\n\n\n\n\n\n\n\n\n　scale_x_continuous()は目盛りの調整が主な使い道ですが、他にも様々な機能を提供しています。たとえば、座標系の最小値と最大値の指定はcoord_*()を使うと説明しましたが、実はscale_*_continuous()でもlimits引数で指定することも可能です。たとえば、縦軸の範囲を0ドルから10万ドルにしたい場合はscale_y_continuous()の中にlimits = c(0, 100000)を指定します。\n\nScale_Fig1 +\n  scale_x_continuous(breaks = seq(0, 100, by = 10),\n                     labels = seq(0, 100, by = 10)) +\n  scale_y_continuous(limits = c(0, 100000))\n\n\n\n\n\n\n\n\n　他にも目盛りと目盛りラベルの位置を変更することも可能です。これはposition引数を使います。基本的に横軸の目盛りは下（\"bottom\"）、縦軸は左（\"left\"）ですが、\"top\"や\"right\"を使うことも可能です。もし、縦軸の目盛りとラベル、軸のラベルを右側にしたい場合はscale_y_continuous()の中にposition = \"right\"を指定します。\n\nScale_Fig1 +\n  scale_y_continuous(position = \"right\")\n\n\n\n\n\n\n\n\n\n\n離散変数の場合\n　もし、軸が名目変数や順序変数のような離散変数でマッピングされている場合は、scale_*_discrete()を使います。たとえば、座標系の回転の例で使いましたFlip_Figの場合、横軸の大陸名が英語のままになっています。これを日本語にする場合、データレベルで大陸名を日本語で置換することも可能ですが、scale_x_discrete()を使うことも可能です。使い方はscale_*_continuous()と同じであり、breaksとlabels引数を指定するだけです。\n\nFlip_Fig +\n  scale_x_discrete(breaks = c(\"Africa\", \"America\", \"Asia\", \"Europe\", \"Oceania\"),\n                   labels = c(\"アフリカ\", \"アメリカ\", \"アジア\", \"ヨーロッパ\", \"オセアニア\"))\n\n\n\n\n\n\n\n\n　今回は大陸名が文字型の列でしたが、factor型の場合、いくつか便利な機能が使えます。たとえば、Polity_Typeごとに国数を計算し、棒グラフを作成するとします。\n\nScale_df1 <- Country_df %>%\n  group_by(Polity_Type) %>%\n  summarise(N       = n(),\n            .groups = \"drop\")\n\nScale_df1\n\n# A tibble: 6 × 2\n  Polity_Type         N\n  <chr>           <int>\n1 Autocracy          19\n2 Closed Anocracy    23\n3 Democracy          65\n4 Full Democracy     31\n5 Open Anocracy      20\n6 <NA>               28\n\n\n　続きまして、Polity_Type列をfactor型にします。最もスコアの低い独裁（Autocracy）から最もスコアの高い完全な民主主義（Full Democracy）の順番のfactorにします。\n\nScale_df1 <- Scale_df1 %>%\n  mutate(Polity_Type = factor(Polity_Type, ordered = TRUE,\n                              levels  = c(\"Autocracy\",\n                                          \"Closed Anocracy\",\n                                          \"Open Anocracy\",\n                                          \"Democracy\",\n                                          \"Full Democracy\")))\n\nScale_df1$Polity_Type\n\n[1] Autocracy       Closed Anocracy Democracy       Full Democracy \n[5] Open Anocracy   <NA>           \n5 Levels: Autocracy < Closed Anocracy < Open Anocracy < ... < Full Democracy\n\n\n　問題なくfactor化も出来たので、それでは作図をしてみましょう。\n\nScale_df1 %>% \n  ggplot() +\n  geom_bar(aes(x = Polity_Type, y = N), stat = \"identity\")\n\n\n\n\n\n\n\n\n　Polityプロジェクトの対象外があり、この場合は欠損値（NA）になります。そして、問題はこの欠損値も表示されることです。むろん、欠損値のカテゴリも出力したいケースもありますが、もし欠損値カテゴリの棒を消すにはどうすれば良いでしょうか。1つ目の方法はScale_df1 %>% drop_na()で欠損値を含む行を除去してから作図する方法です。2つ目の方法はscale_x_discrete()でna.translate = FALSEを指定する方法です。ここでは横軸の目盛りラベルも日本語に変更し、欠損値のカテゴリを除外してみましょう。また、地味に便利な機能として、軸ラベルもscale_*_*()で指定可能です。第1引数として長さ1の文字ベクトルを指定すると、自動的に軸ラベルが修正され、labs()が不要となります。\n\nScale_df1 %>% \n  ggplot() +\n  geom_bar(aes(x = Polity_Type, y = N), stat = \"identity\") +\n  scale_x_discrete(\"Polity IV スコア\", # 第1引数で軸ラベルも指定可能\n                   breaks = c(\"Autocracy\", \"Closed Anocracy\", \"Open Anocracy\",\n                              \"Democracy\", \"Full Democracy\"),\n                   labels = c(\"独裁\", \"閉じられたアノクラシー\",\n                              \"開かれたアノクラシー\", \"民主主義\",\n                              \"完全な民主主義\"),\n                   na.translate = FALSE) +\n  scale_y_continuous(\"国数\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nこれで欠損値を除外することができました。目盛りラベルが重なる箇所があり、多少気になりますが、この問題に関しては後述します。\n\n\n\ncolorスケールの調整\n　グラフの次元を増やす際において広く使われている方法は変数の値に応じて色分けをすることでした。この場合、幾何オブジェクトのaes()内にcolor = マッピングする変数名を指定することになります。このように色が変数でマッピングされている場合は、色分けのスケールも調整可能です。たとえば、折れ線グラフにおいて日本が赤い線だったのを青い線に変更することが考えられます。ここでもcolorにマッピングされている変数のデータ型によって使用する関数が変わります。ここでは連続変数、順序付き離散変数、順序なし離散変数について説明します。\n\ncolor引数が連続変数でマッピングされている場合\n　まずは、連続変数からです。横軸は底10の対数変換した一人あたりGDP（GDP_per_capita）、縦軸は人間開発指数（HDI_2018）にした散布図を作成し、Polity IVスコア（Polity_Score）で点の色分けをしてみましょう。\n\nScale_Fig2 <- Country_df %>%\n    ggplot() +\n    geom_point(aes(x = GDP_per_capita, y = HDI_2018, color = Polity_Score)) +\n    labs(x = \"一人あたりGDP (USD)\",y = \"人間開発指数\", color = \"Polity IVスコア\") +\n    scale_x_log10() +\n    theme_bw(base_size = 12)\n\nScale_Fig2\n\n\n\n\n\n\n\n\n　これまではcolor引数を離散変数でしかマッピングしませんでしたが、このように連続変数でマッピングすることも可能です。ただし、この場合は値に応じてはっきりした色分けがされるのではなく、グラデーションで色分けされます。この例だと、青に近いほどPolity IVスコアが高く、黒に近いほど低いことが分かります。この場合、colorのスケール調整は最小値と最大値における色を指定するだけです。その間の色については{ggplot2}が自動的に計算してくれます。\n　今回使用する関数はscale_color_gradient()です。これまでの例だとscale_color_continuous()かと思う方も多いでしょう。実際、scale_color_continuous()関数も提供されており、使い方もほぼ同じです。ただし、scale_color_continuous()を使う際は引数としてtype = \"gradient\"を指定する必要があります。scale_color_gradient()の場合、最小値における色をlow、最大値のそれをhighで指定します。数値は\"red\"や\"blue\"なども可能であり、\"#132B43\"のような書き方も使えます。たとえば、先ほどのScale_Fig2においてPolity IVスコアが高いほどbrown3、低いほどcornflowerblueになるようにする場合は以下のように書きます。\n\nScale_Fig2 +\n    scale_color_gradient(high = \"brown3\", low = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n　このような書き方だとどのような色名で使えるかを事前に知っておく必要があります。使える色名のリストはcolors()から確認できます。全部で657種類がありますが、ここでは最初の50個のみを出力します。\n\nhead(colors(), 50)\n\n [1] \"white\"          \"aliceblue\"      \"antiquewhite\"   \"antiquewhite1\" \n [5] \"antiquewhite2\"  \"antiquewhite3\"  \"antiquewhite4\"  \"aquamarine\"    \n [9] \"aquamarine1\"    \"aquamarine2\"    \"aquamarine3\"    \"aquamarine4\"   \n[13] \"azure\"          \"azure1\"         \"azure2\"         \"azure3\"        \n[17] \"azure4\"         \"beige\"          \"bisque\"         \"bisque1\"       \n[21] \"bisque2\"        \"bisque3\"        \"bisque4\"        \"black\"         \n[25] \"blanchedalmond\" \"blue\"           \"blue1\"          \"blue2\"         \n[29] \"blue3\"          \"blue4\"          \"blueviolet\"     \"brown\"         \n[33] \"brown1\"         \"brown2\"         \"brown3\"         \"brown4\"        \n[37] \"burlywood\"      \"burlywood1\"     \"burlywood2\"     \"burlywood3\"    \n[41] \"burlywood4\"     \"cadetblue\"      \"cadetblue1\"     \"cadetblue2\"    \n[45] \"cadetblue3\"     \"cadetblue4\"     \"chartreuse\"     \"chartreuse1\"   \n[49] \"chartreuse2\"    \"chartreuse3\"   \n\n\nscale_color_gradient()から派生した関数としてscale_color_gradient2()というものもあります。これは最小値と最大値だけでなく、mid引数を使って中間における色も指定可能な関数です。例えば、先ほどの例で真ん中をseagreenにしてみましょう。\n\nScale_Fig2 +\n    scale_color_gradient2(high = \"brown3\", mid = \"seagreen\", low = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n　色は\"seagreen\"、\"red\"でなく、\"#00AC97\"、\"#FF0000\"のように具体的なRGB値で指定することもできます。これは色を赤（R）、緑（G）、青（B）の3つの原色を混ぜて様々な色を表現する方法です。\"#FF0000\"の場合、最初の#はRGB表記であることを意味し、FFは赤が255であることの16進法表記、次の00と最後の00は緑と青が0であることの16進法表記です。各原色は0から255までの値を取ります。{ggplot2}でよく見る色としては#F8766D、#00BFC4、#C77CFF、#7CAE00があります。他にもGoogleなどで「RGB color list」などを検索すれば様々な色を見ることができます。\n他にも{ggplot2}は様々なユーザー指定のパレットが使用可能であり、実際、パッケージの形式として提供される場合もあります。たとえば、ジブリ風のカラーパレットが{ghibli}パッケージとして提供されており、install.packages(\"ghibli\")でインストール可能です。\n\npacman::p_load(ghibli)\n\nScale_Fig2 +\n    # 連続変数（_c）用の「もののけ姫」パレット（medium）\n    scale_color_ghibli_c(\"MononokeMedium\")\n\n\n\n\n\n\n\n\n\n\ncolor引数が順序付き離散変数でマッピングされている場合\n　連続変数のようにグラデーションではあるものの、それぞれの値に対して具体的な色が指定されます。まずは先ほど作成しましたScale_Fig2の色分けを連続変数であるPolity_Scoreでなく、順序付き離散変数であるPolity_Typeにします。Polity_Typeの順序は独裁（Autocracy）、閉じられたアノクラシー（Closed Anocracy）、開かれたアノクラシー（Open Anocracy）、民主主義（Democracy）、完全な民主主義（Full Democracy）にします。また、Polity_Typeが定義されていない国もデータセットに含まれているため、drop_na(Polity_Type)を追加し、Polity_Typeが欠損している行を除去します。\n\nScale_Fig3 <- Country_df %>%\n  mutate(Polity_Type = factor(Polity_Type, ordered = TRUE,\n                              levels  = c(\"Autocracy\",\n                                          \"Closed Anocracy\",\n                                          \"Open Anocracy\",\n                                          \"Democracy\",\n                                          \"Full Democracy\"))) %>%\n  drop_na(Polity_Type) %>%\n  ggplot() +\n  geom_point(aes(x = GDP_per_capita, y = HDI_2018, color = Polity_Type)) +\n  labs(x = \"一人あたりGDP (USD)\",y = \"人間開発指数\", color = \"Polity IVタイプ\") +\n  scale_x_log10() +\n  theme_bw(base_size = 12)\n\nScale_Fig3\n\n\n\n\n\n\n\n\n　今回は紫（独裁）から黄色（完全な民主主義）の順で色分けがされ、その間のカテゴリーも紫と黄色の間の値をとります。実は順序付き離散変数の場合、色のスケールを調整することはあまりありませんし、これまでの方法に比べてやや複雑です。ここでは色相（Hue）の範囲と強度、明るさを調整する方法について紹介します。\n　まずは、色相について知る必要があります。{ggplot2}において色相の範囲は0から360です。そして、色には強度（intensity）、または彩度という概念があり、0に近いほどグレイへ近づき、色間の区別がしにくくなります。{ggplot2}では彩度のデフォルト値は100であり、我々が普段{ggplot2}で見る図の色です。最後に明るさ（luminance）があり、0から100までの値を取ります。値が大きいほど明るくなり、{ggplot2}のデフォルト値は65です。重要なのは色相のところであり、Hueの具体的な数値がどの色なのかを確認する必要があります。そのためには、{scales}パッケージのhue_pal()とshow_col()関数を使用します。\n\n\n\n\n\n\n\n\n\n　左上が0、右下が360の色を意味します。順序変数でマッピングされた色スケールを色相に基づいて調整する際は、scale_color_hue()レイヤーを追加し、色相の範囲、彩度、明るさを指定します。たとえば、0から300までの範囲の色を使用し2、再度と明るさはデフォルトにしたい場合、以下のように書きます。\n\nScale_Fig3 +\n  scale_color_hue(h = c(0, 360), c = 100, l = 65)\n\n\n\n\n\n\n\n\n　また、direction = -1を追加することで、色の順番を逆にすることも可能です。\n\nScale_Fig3 +\n  scale_color_hue(h = c(0, 360), c = 100, l = 65, direction = -1)\n\n\n\n\n\n\n\n\n　他にもmpl colormapsというカラーマップを使うことも可能です。この場合はscale_color_hue()ではなく、scale_color_viridis_d()を使用します。具体的な色の情報はmpl colormapsを参照してください。必要な引数は色のスタート地点（begin）と終了地点（end）です。そして、optionの引数のデフォルト値は\"D\"であり、これはVIRIDIS colormapを意味します。実はscale_color_*()を付けなかった場合の色分けがこれです。たとえば、VIRIDIS colormapでなく、PLASMA colormapを使うならoption = \"C\"を付けます。\n\nScale_Fig3 +\n  # PLASMA colormapを使用する（option = \"C\"）\n  scale_color_viridis_d(begin = 0, end = 1, option = \"C\")\n\n\n\n\n\n\n\n\n\n\ncolor引数が順序なし離散変数でマッピングされている場合\n　最後に順序なし離散変数の場合について解説します。ここではscale_color_manual()関数を使用し、マッピングされている変数のそれぞれ値に対して具体的な色を指定する方法です。以下で説明する方法は、順序付き離散変数でも使用可能であるため、Scale_Fig3の図をそのまま利用してみたいと思います。\n　scale_color_manual()の核心となる引数はvaluesであり、ここに\"変数の値\" = \"色\"で指定します。この色は\"red\"のような具体的な色名でも、\"#FF0000\"のようなRGB表記でも構いません。\n\nggplot2オブジェクト +\n  scale_color_manual(values = c(\"変数の値1\" = \"色1\",\n                                \"変数の値2\" = \"色2\",\n                                \"変数の値3\" = \"色3\",\n                                ...))\n\n　それではPolity_Typeの値が\"Autocracy\"の場合はdarkred、\"Closed Anocracy\"はseagreen、\"Open Anocracy\"はcornflowerblue、\"Democracy\"はorchid、\"Full Democracy\"はorangeにしてみましょう。\n\nScale_Fig3 +\n  scale_color_manual(values = c(\"Autocracy\"       = \"darkred\",\n                                \"Closed Anocracy\" = \"seagreen\",\n                                \"Open Anocracy\"   = \"cornflowerblue\",\n                                \"Democracy\"       = \"orchid\",\n                                \"Full Democracy\"  = \"orange\"))\n\n\n\n\n\n\n\n\n　色を決める際は色覚多様性に気をつけるべきです。誰にとっても見やすい図にはUniversal Designが必要です。特によくある例が「緑と赤」の組み合わせです。緑も赤も暖色系であり、P型およびD型色弱の場合、両者の区別が難しいと言われています。しかも、色弱の方は意外と多いです。日本の場合、男性の5%、女性の0.2%と言われております。これには人種差もありまして、フランスや北欧の男性の場合は約10%です。一通り図を作成しましたら、色覚シミュレーターなどを使用して、誰にとっても見やすい色であるかを確認することも良いでしょう。また、{ggplot2}がデフォルトで採用しているVIRIDISは色弱に優しいカラーパレットと言われています。一般的に二色の組み合わせの場合、最も区別しやすい色は青とオレンジと言われいます。\n　{ggplot2}におけるcolorスケールは非常に細かく調整可能であり、関連関数やパッケージも多く提供されています。本書では全てを紹介することはできませんが、ここまで紹介してきました例だけでも、自分好みに色を調整できるでしょう。\n\n\n\nalphaスケールの調整\n　次は点・線・面の透明度を指定するalphaスケールの調整です。もし、alphaに連続変数をマッピングすれば、マッピングした変数の値に応じて透明度が変わります。一般的に、値が大きいほど不透明となり、小さいほど透明になります。しかし、このような使い方はあまり見られません。プロット上のオブジェクトを透明度を指定するのは\n\n特定箇所にオブジェクトが密集し、重なるオブジェクトの区別がつきにくい場合\n特定の点・線・面を強調するため\n\n　以上の2ケースでしょう。また、ケース1の場合、alpha引数をaes()の内部ではなく、外側に指定し、具体的な数値を指定することになります。ここで注目するのはケース2です。たとえば、横軸に一人あたりGDP、縦軸に人間開発指数をマッピングした散布図を作成し、アジアの国のみハイライトしたいとします。この場合、アジアとその他の国で色分けしたり、丸と四角といった形で分けるのも可能ですが、「強調」が目的であれば、その他の国をやや透明にすることも可能でしょう。まず、Asiaという変数を作成し、Continentの値が\"Asia\"なら\"Asia\"、その他の値なら\"Other\"の値を入れます。そして、geom_point()のaes()内にalpha引数を追加し、Asia変数でマッピングします。\n\nCountry_df %>%\n  mutate(Asia = if_else(Continent == \"Asia\", \"Asia\", \"Other\")) %>%\n  ggplot() +\n  geom_point(aes(x = GDP_per_capita, y = HDI_2018, alpha = Asia), size = 2) +\n  labs(x = \"一人あたりGDP (USD)\",y = \"人間開発指数\", alpha = \"大陸\") +\n  scale_x_log10() +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　Asiaの値によって透明度が異なりますが、私たちの目的はAsiaを不透明にし、その他の点を透明にすることです。ここで登場するのがscale_alpha_manual()です。使い方はこれまで見てきたscale_*_manual()と非常に似ています。values引数にそれぞれの値と透明度を指定するだけです。透明度は1が不透明、0が透明です。アジアの透明度を1.0、その他の透明度を0.15とするなら、以下のように書きます。\n\nCountry_df %>%\n  mutate(Asia = if_else(Continent == \"Asia\", \"Asia\", \"Other\")) %>%\n  ggplot() +\n  geom_point(aes(x = GDP_per_capita, y = HDI_2018, alpha = Asia), size = 2) +\n  labs(x = \"一人あたりGDP (USD)\",y = \"人間開発指数\", alpha = \"大陸\") +\n  scale_x_log10() +\n  scale_alpha_manual(values = c(\"Asia\" = 1.0, \"Other\" = 0.15)) +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　これでアジアの国々がプロット上で強調されました。透明度スケールは連続変数（scale_alpha_continuous()）や離散変数（scale_alpha_discrete()）に使うことも可能ですが、透明度を「区別」でなく「強調」の目的で使うならば、scale_alpha_manual()でも十分だと考えられます。\n　一つ注意して頂きたいのは、図をPDFで出力する際、半透明なオブジェクトが見えなかったり、逆に透明度が全く適用されない場合があります。PNGなどのビットマップ画像ならこのような問題は生じませんが、PDFの場合は注意が必要です3。この場合、画像をPNGなどの形式にするか、半透明でなく「グレーと黒」のような組み合わせで作図した方が良いかも知れません。\n\n\nsizeスケールの調整\n　大きさに関するマッピングは幾何オブジェクトのaes()内にsize引数を使用します。ここでの大きさというのは点の大きさと線の太さを意味します。geom_point()内のsizeは点の大きさ、geom_line()やgeom_segment()のsizeは線の太さ、geom_pointrange()では両方を意味します。ただし、実質的に変数の値に応じて線の太さを変えることは強調4を除けばあまり見られません。ここでは点の大きさに焦点を当てて解説します。\n　それでは、横軸はフリーダム・ハウスのスコア（FH_Total）、縦軸は2018年人間開発指数（HDI_2018）として散布図を作成し、一人当たり購買力平価GDP（PPP_per_capita）に応じて点の大きさを指定します。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, size = PPP_per_capita)) +\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         size = \"一人あたり購買力平価GDP（USD）\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") # 凡例の図の下段にする\n\n\n\n\n\n\n\n\n　フリーダム・ハウススコアと人間開発指数は全般的には正の相関を示しています。そして、両指標が高い国（図の右上）は所得水準も高いことが分かります。ただし、フリーダム・ハウススコアが低くても人間開発指数が高い国（図の左上）もかなり見られますが、これらの国の共通点は所得水準が高いことです。つまり、人間開発指数と所得水準には強い相関関係があると考えられます（人間開発指数には所得水準も含まれるため、当たり前です）。\n　大きさをマッピングすると、点が重なる箇所が広くなりますので、alpha引数で半透明にすることも有効でしょう。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, size = PPP_per_capita),\n               alpha = 0.5) + # 透明度の指定\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         size = \"一人あたり購買力平価GDP（USD）\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n　この大きさのスケール調整はマッピングされた変数の尺度によって、scale_size_continuous()、sclae_size_discrete()、scale_size_ordinal()などを使用し、すべてマニュアルで調整したい場合はscale_size_manual()を使います。使い方はこれまでのスケール調整とほぼ同様です。たとえば、上記の図だと、大きさの凡例が3万、6万、9万となっていますが、これを1000, 10000, 100000といった対数スケールに変更してみましょう。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, size = PPP_per_capita),\n               alpha = 0.5) +\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         size = \"一人あたり購買力平価GDP（USD）\") +\n    scale_size_continuous(breaks = c(1000, 10000, 100000),\n                          labels = c(\"1000\", \"10000\", \"100000\")) +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n　この場合、図内の点の大きさに変化はありません。変わるのは凡例のみであり、値に応じた点のサイズに調整されます。連続変数でマッピングされている場合、一つ一つの値に応じてサイズを指定するのは非現実的であります。この場合、点の大きさ調整は{ggplot2}に任せて、凡例のサイズを調整するのが無難でしょう。\n　ただし、点の最小サイズと最大サイズを調整したいケースもあるでしょう。最も小さい点のサイズを0.1に、最も大きい点のサイズを10にする場合、range引数を指定します。range引数の実引数は長さ2の数値型ベクトルです。\n\nCountry_df %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, size = PPP_per_capita),\n               alpha = 0.5) +\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         size = \"一人あたり購買力平価GDP（USD）\") +\n    scale_size_continuous(breaks = c(1000, 10000, 100000),\n                          labels = c(\"1000\", \"10000\", \"100000\"),\n                          range = c(0.1, 10)) +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n　このように範囲が広がるほど、所得水準の差がより見やすくなります。他にもsizeは離散変数でマッピングさることも可能ですが、あまり相性は良くありません。離散変数でのマッピングはこれまで紹介しましたcolorやsizeの方を参照してください。使い方は同じです。\n\n\nshape、linetypeスケールの調整\n　sizeは連続変数と相性が良いですが、点や線の形であるshapeとlinetypeは離散変数、とりわけ順序なし離散変数（名目変数）と相性が良いです。なぜなら、点や線の形は高低・大小の情報を持たないからです。たとえば、丸の点は四角の点より大きいとか実線は破線より小さいといった情報はありません。したがって、sizeやlinetypeは名目変数に使うのが一般的です。たとえば、フリーダムハウスのスコアを横軸、人間開発指数を縦軸とし、G20加盟有無によって点の形が異なる散布図を作成するとします。G20をcharacter型、あるいはfactor型に変換し、geom_point()の幾何オブジェクトのaes()内にshape引数を指定します。\n\nCountry_df %>%\n    mutate(G20 = if_else(G20 == 1, \"加盟国\", \"非加盟国\")) %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, shape = G20),\n               size = 2) +\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         shape = \"G20\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n　加盟国を丸（16または19）、非加盟国をダイヤモンド型（18）にするにはscale_shape_manual()を使います。\n\nCountry_df %>%\n    mutate(G20 = if_else(G20 == 1, \"加盟国\", \"非加盟国\")) %>%\n    ggplot() +\n    geom_point(aes(x = FH_Total, y = HDI_2018, shape = G20),\n               size = 2) +\n    scale_shape_manual(values = c(\"加盟国\" = 19, \"非加盟国\" = 18)) +\n    labs(x = \"フリーダムハウススコア\",y = \"2018年人間開発指数\", \n         shape = \"G20\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n　ただし、各数字がどの形に対応しているかを事前に知っておく必要があります。よく使うのは0から25までであり、それぞれ対応するshapeを示したのが以下の図です。必要に応じてこのページを参照しても良いですし、Rコンソール上で?pchを入力しても0から23までの例を見ることが出来ます。\n\n\n\n\n\n\n\n\n\n　注意していただきたいのは、shapeのマッピングが有効でない状況があるという点です。それが先ほどの例です。先ほどの散布図の場合、次元を増やすことは、新しい次元で条件づけた場合の変数の関係性を調べることとなります。しかし、新しい次元による条件付き関連性があまり見られない場合、あるいは二種類以上の点の形があまり分離されていない場合は、次元の追加がもたらす恩恵が感じにくくなるでしょう。例えば以下のような散布図を比較してみましょう。図(A)の場合、縦軸の変数が閾値を超えるともう一つの変数との関係が弱まるということが分かります。たとえば、三角の点において両変数は正の相関を持ち、丸の点においては無相関に近いことが分かります。この場合、点の形は非常に有用な情報を含んでいると判断できます。一方、図(B)の場合、点の形から読み取れる情報が少ないですね。あえて言えば、グループ間の違いがあまりないことくらいでしょう。\n\n\n\n\n\n\n\n\n\n　白黒のグラフの場合、色分けが出来ないため、次元拡張には点の形を変えることになります。しかし、色に来れば形は読み手にとって認知の負荷がかかりやすいです。下の図を見てください。100個の点がありますが、三角の点はいくつでしょうか。\n\n\n\n\n\n\n\n\n\n　正解は5つです。あまり難しい問題ではないでしょう。一方、下の図はいかがでしょうか。\n\n\n\n\n\n\n\n\n\n　どれも正解は5つです。本質的には同じ問題ですが、どの図の方が読みやすかったでしょうか。個人差はあるかも知れませんが、多くの方にとって後者の方が読みやすかったでしょう。最近、海外のジャーナルはカラーの図を使うことも可能ですので、色分けを優先的に考えましょう。白黒のみ受け付けられる場合でも、グループ数やサンプルサイズによっては点の形より、彩度や明るさの方が効果的な場合もあります。\n　続きまして、linetypeについて解説します。線の形も色分けができない場合、よく使われる次元の増やし方です。ここでは日中韓台における人口1万人あたりCOVID-19新規感染者数の折れ線グラフを作成します。COVID19_dfには人口のデータがないため、left_join()を使ってCountry_dfと結合します。left_join()の使い方に関してはdplyr入門を参照してください。続いて、Character型であるDate変数をas.Date()関数を使ってDate型へ変換します。1万人あたり新規感染者数は新規感染者数（Confirmed_Day）を人口（Population）で割り、1万をかけます。最後にCountry列を基準にfilter()を使用し、日中韓台のデータのみ残します。後は折れ線グラフを作成しますが、geom_line()内のaes()内にlinetype引数をCountry変数でマッピングします。\n\nleft_join(COVID19_df, Country_df, by = \"Country\") %>% \n  mutate(Date                 = as.Date(Date),\n         Confirmed_per_capita = Confirmed_Day / Population * 10000) %>%\n  filter(Country %in% c(\"Japan\", \"China\", \"Taiwan\", \"South Korea\")) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_per_capita, linetype = Country)) +\n  labs(x = \"月\", y = \"1万人当たり新規感染者数\",linetype = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　いかがでしょうか。linetypeはcolorよりも識別性が非常に低いことが分かるでしょう。個人差もあるかも知れませんが、shapeよりも低いのではないでしょうか。実線の中国を除けば、日本、韓国、台湾の線はなかなか区別できません。したがって、linetypeは2つ、3つまでが限界だと考えられます。3つまででしたら、実線、破線、点線に分けることができるでしょう。ここでは、日本のみを実線とし、他の3カ国は「その他」として破線にしてみましょう。そのためには、日本か否かを示すJapan変数を作成します。また、geom_line()のaes()内にはgroups引数を追加し、linetypeはJapan変数でマッピングします。\n\nleft_join(COVID19_df, Country_df, by = \"Country\") %>% \n  mutate(Date                 = as.Date(Date),\n         Confirmed_per_capita = Confirmed_Day / Population * 10000,\n         Japan                = if_else(Country == \"Japan\", \"日本\", \"その他\")) %>%\n  filter(Country %in% c(\"Japan\", \"China\", \"Taiwan\", \"South Korea\")) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_per_capita, \n                group = Country, linetype = Japan)) +\n  labs(x = \"月\", y = \"1万人当たり新規感染者数\", linetype = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　その他の国が実線となっているので、scale_linetype_manual()でJapanの値ごとに線のタイプを指定します。\n\nleft_join(COVID19_df, Country_df, by = \"Country\") %>% \n  mutate(Date                 = as.Date(Date),\n         Confirmed_per_capita = Confirmed_Day / Population * 10000,\n         Japan                = if_else(Country == \"Japan\", \"日本\", \"その他\")) %>%\n  filter(Country %in% c(\"Japan\", \"China\", \"Taiwan\", \"South Korea\")) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_per_capita, \n                group = Country, linetype = Japan)) +\n  scale_linetype_manual(values = c(\"日本\" = 1, \"その他\" = 2)) +\n  labs(x = \"月\", y = \"1万人当たり新規感染者数\", linetype = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　できれば、日本とその他の順番も逆にしたいですね。こちらはJapan変数をfactor化することで対応可能です。\n\nleft_join(COVID19_df, Country_df, by = \"Country\") %>% \n  mutate(Date                 = as.Date(Date),\n         Confirmed_per_capita = Confirmed_Day / Population * 10000,\n         Japan                = if_else(Country == \"Japan\", \"日本\", \"その他\"),\n         Japan                = factor(Japan, levels = c(\"日本\", \"その他\"))) %>%\n  filter(Country %in% c(\"Japan\", \"China\", \"Taiwan\", \"South Korea\")) %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = Confirmed_per_capita, \n                group = Country, linetype = Japan)) +\n  scale_linetype_manual(values = c(\"日本\" = 1, \"その他\" = 2)) +\n  labs(x = \"月\", y = \"1万人当たり新規感染者数\", linetype = \"国\") +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nこれで完成ですがいかがでしょうか。複数の線を識別するという意味では色分け（color）が優れていますし、ハイライトなら透明度（alpha）か線の太さ（size）の方が良いでしょう。筆者（SONG）としましてはlinetypeによる次元の追加はあまりオススメしませんが、知っといて損はないでしょう。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html#visual3-theme",
    "href": "tutorial/R/ggplot_intro3.html#visual3-theme",
    "title": "可視化[応用]",
    "section": "theme_*()とtheme(): テーマの指定",
    "text": "theme_*()とtheme(): テーマの指定\n　続いて図全体の雰囲気を決めるtheme_*()レイヤーについて解説します。これらの使い方は非常に簡単であり、ggplotオブジェクトにtheme_*()レイヤーを+で繋ぐだけです。もし、こちらのレイヤーを追加しない場合、デフォルトテーマとしてtheme_gray()が適用されます。{ggplot2}はいくつかのテーマを提供しており、以下がその例です。\n\n\n\n\n\n\n\n\n\n　他にも{ggplot2}用のテーマをパッケージとしてまとめたものもあります。興味のある方はggthemeやggthemrページを確認してみてください。\n　theme_*()内部ではいくつかの引数を指定することができます。最もよく使われるのがbase_family引数であり、図で使用するフォントを指定する引数です。macOSユーザーだとヒラギノ角ゴジックW3が良く使われており、base_family = \"HiraginoSans-W3\"で設定可能です。他にも全体の文字サイズを指定するbase_sizeなどがあります。\n　テーマの微調整は主にtheme()レイヤーで行います。こちらでは図の見た目に関する細かい調整ができます。実はtheme_*()関数群は調整済みtheme()レイヤーとして捉えることも出来ます。theme_*()とtheme()を同時に使うことも可能であり、「全般的にはminimalテーマ（theme_minimal()）が好きだけど、ここだけはちょっと修正したい」場合に使用します。theme()では図の見た目に関する全ての部分が設定可能であるため、引数も膨大です。詳しくはコンソール上で?themeを入力し、ヘルプを確認してください。ここではいくつかの例のみを紹介します。\n　まずは実習用データとして任意の棒グラフを作成し、Theme_Figという名のオブジェクトとして保存しておきます。\n\nTheme_Fig <- Country_df %>%\n  # Freedom HouseのStatus、Continentでグループ化\n  group_by(FH_Status, Continent) %>%\n  # 欠損値のある行を除き、PPP_per_capitaの平均値を計算\n  summarise(PPP     = mean(PPP_per_capita, na.rm = TRUE),\n            .groups = \"drop\") %>%\n  # 欠損値の行を除去\n  drop_na() %>%\n  # Freedom HouseのStatusを再ラベリングし、factor化\n  mutate(FH_Status = case_when(FH_Status == \"F\"  ~ \"Free\",\n                               FH_Status == \"PF\" ~ \"Partially Free\",\n                               FH_Status == \"NF\" ~ \"Not Free\"),\n         FH_Status = factor(FH_Status, levels = c(\"Free\", \"Partially Free\",\n                                                  \"Not Free\"))) %>%\n  ggplot() +\n  geom_bar(aes(x = FH_Status, y = PPP, fill = Continent),\n           stat = \"identity\", position = position_dodge(width = 1)) +\n  labs(x = \"Polity IV Score\", y = \"PPP per capita (USD)\")\n\nTheme_Fig\n\n\n\n\n\n\n\n\n\n文字の大きさ\n　図全体における文字の大きさはtheme_*()レイヤーのbase_sizeから調整することもできますが、theme()からも可能です。文字に関する調整はtext引数に対してelement_text()実引数を指定します。大きさの場合、element_text()内にsize引数を指定します。\n\nTheme_Fig +\n  theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n　element_text()では文字の大きさ以外にも色（color）、回転の度合い（angle）などを指定することもできます。詳しくはRコンソール上で?element_textを入力し、ヘルプを確認してください。\n\n\n背景のグリッド\n　背景のグリッドを調整する際はpanel.gird.*引数を使います。主に使用する場面はグリッドの除去する場合ですが、この場合は実引数としてelement_blank()を使います。もし、グリッドの色や太さなどを変更したい場合はelement_line()を使用します。ここではグリッドを除去する方法について紹介します。\n　グリッドを除去する場合、どのグリッドを除去するかを決めないといけません。例えば、すべてのグリッドを除去するためにはpanel.grid = element_blank()を使います。また、メジャーグリッドの除去にはpanel.grid.major（すべて）、panel.grid.major.x（横軸のみ）、panel.grid.major.y（縦軸のみ）を指定します。マイナーグリッドの場合はmajorをminorに替えてください。以下にはいくつかの例をお見せします。\n\n# すべてのグリッドを除去\nTheme_Fig +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n# x軸のメジャーグリッドを除去\nTheme_Fig +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n# y軸のマイナーグリッドのみ除去\nTheme_Fig +\n  theme(panel.grid.minor.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n目盛りラベルの回転\n　横軸の目盛りラベルが長すぎるか大きすぎると、ラベルが重なる場合があります。この場合の対処方法としてはラベルの長さを短くしたり、改行（\\n）を入れることが考えられますが、ラベルを若干回転することでも対処可能です。たとえば、横軸の目盛りラベルを調整する場合はaxis.text.x = element_text()を指定し、element_text()内にangle引数を指定します。例えば、 反時計回りで25度回転させる場合はangle = 25と指定します。\n\nTheme_Fig +\n  theme(text = element_text(size = 16),\n        axis.text.x = element_text(angle = 25))\n\n\n\n\n\n\n\n\n　この場合、ラベルは目盛りのすぐ下を基準に回転することになります。もし、ラベルの最後の文字を目盛りの下に移動させる場合はhjust = 1を追加します。\n\nTheme_Fig +\n  theme(text = element_text(size = 16),\n        axis.text.x = element_text(angle = 25, hjust = 1))\n\n\n\n\n\n\n\n\n　ちなみに、angle = 90などで指定するとラベルが重なる問題はほぼ完全に解決されますが、かなり読みづらくなるので、できればangleの値は小さめにした方が読みやすくなります。\n\n\nscale_*_*()を用いたラベル重複の回避\n　ラベルの重複を回避するもう一つの方法は「ラベルの位置をずらす」ことです。これは{ggplot2}3.3.0以降追加された機能であり、theme()でなく、scale_*_*()関数のguide引数で指定することが出来ます。使い方は以下の通りです。たとえば、横軸（x軸）のラベルを2行構成にしたい場合は以下のように指定します。\n\nggplot2オブジェクト名 +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2))\n\n　実際の結果を確認してみましょう。\n\nTheme_Fig +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n　横軸のラベルが2行構成になりました。左から最初のラベルは1行目に、2番目のラベルは2行目に、3番目のラベルは1行目になります。実際、ラベルをずらすだけならn.dodge = 2で十分ですが、この引数の挙動を調べるためにn.dodge = 3に指定してみましょう。\n\nTheme_Fig +\n  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +\n  theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n　3番目のラベルが3行目に位置することになります。もし、4つ目のラベルが存在する場合、それは1行目に位置するでしょう。\n\n\n凡例の表示/非表示\n　凡例を無くす方法はいくつかありますが、まずはすべての凡例を非表示する方法について紹介します。それは後ほど紹介しますlegend.positionの実引数として\"none\"を指定する方法です。\n\n# 凡例を非表示にする\nTheme_Fig +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n　Theme_Figはxとy以外にfillにマッピングをしたため、凡例は一つのみとなります。ただし、場合によってはもっと次元を増やすことによって2つ以上の凡例が表示されるケースがあります。別途の説明なくても図だけで理解するのが理想なので凡例は出来る限り温存させた方が良いでしょう。しかし、実例はあまり多く見られないと思いますが、凡例がなくても理解に問題がないと判断される場合は一部の凡例を非表示することも考えられます。\n　たとえば、以下のようなTheme_Fig2の例を考えてみましょう。\n\nTheme_Fig2 <- Country_df %>%\n  mutate(OECD = if_else(OECD == 1, \"OECD\", \"non-OECE\")) %>%\n  ggplot(aes(x = FH_Total, y = GDP_per_capita)) +\n  geom_point(aes(color = OECD)) +\n  stat_ellipse(aes(fill = OECD), geom = \"polygon\", level = 0.95, alpha = 0.2) +\n  labs(x = \"Freedom House Score\", y = \"GDP per capita (USD)\", \n       color = \"OECD\", fill = \"Cluster\")\n\nTheme_Fig2\n\n\n\n\n\n\n\n\n　colorとfillがそれぞれ別の凡例として独立しています。この例の場合、fillの凡例はなくても、図を理解するのは難しくないかも知れません。ここで考えられる一つの方法はcolorとfillの凡例をオーバラップさせる方法です。{ggplot2}の場合、同じ変数がマッピングされていれば凡例をオーバーラップさせることも可能です。ただし、凡例のタイトルが同じである必要があります。ここではcolorとfillのタイトルを\"OECD\"に統一してみましょう。\n\nTheme_Fig2 +\n  labs(color = \"OECD\", fill = \"OECD\")\n\n\n\n\n\n\n\n\n　これで十分でしょう。しかし、凡例からfillの情報を完全に消したい場合はどうすれば良いでしょうか。その時に登場するのがguides()関数です。関数の中でマッピング要素 = \"none\"を指定すると、当該凡例が非表示となります。\n\nTheme_Fig2 +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n　guides()関数は凡例を細かく調整できる様々な機能を提供しています。興味のある方はヘルプ（?guides）を参照してください。\n\n\n凡例の位置\n　凡例を図の下段に移動させる場合はlegend.position引数に\"bottom\"を指定するだけです。他にも\"top\"や\"left\"も可能ですが、凡例は一般的に図の右か下段に位置しますので、デフォルトのままに置くか、\"bottom\"くらいしか使わないでしょう。\n\n# 凡例を図の下段へ移動\nTheme_Fig +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　もし、凡例を図の内部に置く場合は、長さ2のnumeric型ベクトルを指定します。図の左下ならc(0, 0)、左上ならc(0, 1)、右上はc(1, 1)、右下はc(1, 0)となります。これは図の大きさを横縦それぞれ1とした場合の位置を意味します。ここでは凡例を右上へ置いてみましょう。\n\n# 凡例をプロットの右上へ\nTheme_Fig +\n  theme(legend.position = c(1, 1))\n\n\n\n\n\n\n\n\n　これは凡例の中央が右上に来るようになります。これを是正するためにはlegend.justification引数を更に指定する必要があります。これにも長さ2のnumeric型ベクトルを指定しますが、これは凡例の中心をどこにするかを意味します。今回の例だと、凡例の右上をc(1, 1)に位置させたいので、ここもc(1, 1)と指定します。基本的にlegend.positionとlegend.justificationは同じ値にすれば問題ないでしょう。\n\n# 凡例の中心を凡例の右上と指定\nTheme_Fig +\n  theme(legend.position = c(1, 1),\n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\n\n　また、凡例の背景を透明にしたい場合はlegend.background = element_blank()を指定します。\n\n# 凡例を背景を透明に\nTheme_Fig +\n  theme(legend.position = c(1, 1),\n        legend.justification = c(1, 1),\n        legend.background = element_blank())\n\n\n\n\n\n\n\n\n　theme()関数が提供している昨日は非常に多く、それぞれの実引数として用いられるelement_text()やelement_line()、element_rect()にも様々な引数が提供されております。図を自分好みに微調整したい方はそれぞれの関数のヘルプを参照してください。"
  },
  {
    "objectID": "tutorial/R/ggplot_intro3.html#visual3-merge",
    "href": "tutorial/R/ggplot_intro3.html#visual3-merge",
    "title": "可視化[応用]",
    "section": "図の結合",
    "text": "図の結合\n　{ggplot2}で作成した複数の図を一つの図としてまとめる場合、昔は{gridExtra}一択でした。しかし、今はもっと使いやすいパッケージがいくつか公開されており、ここでは{ggpubr}のggarrange()を紹介します。使い方を紹介する前に、結合する図をいくつか用意し、それぞれGrid_Fig1、Gird_Fig2、…Grid_Fig4と名付けます。\n\n# Grid_Fig1: 散布図\n# X軸: フリーダムハウス・スコア / Y軸: 一人あたり購買力平価GDP（対数）\nGrid_Fig1 <- Country_df %>%\n  ggplot() +\n  geom_point(aes(x = FH_Total, y = PPP_per_capita)) +\n  scale_y_log10() +\n  labs(x = \"フリーダムハウススコア\", \n       y = \"一人あたり購買力平価GDP（対数）\") +\n  theme_bw(base_size = 12)\n\n# Grid_Fig2: 散布図\n# X軸: フリーダムハウス・スコア / Y軸: 2018年人間開発指数\nGrid_Fig2 <- Country_df %>%\n  ggplot() +\n  geom_point(aes(x = FH_Total, y = HDI_2018)) +\n  labs(x = \"フリーダムハウススコア\", \n       y = \"人間開発指数（2018年）\") +\n  theme_bw(base_size = 12)\n\n# Grid_Fig4: ヒストグラム\n# X軸: フリーダムハウス・スコア\nGrid_Fig3 <- Country_df %>%\n  ggplot() +\n  geom_histogram(aes(x = FH_Total), color = \"white\",\n                 binwidth = 5, boundary = 0) +\n  labs(x = \"フリーダムハウススコア\", y = \"国数\") +\n  theme_bw(base_size = 12)\n\n# Grid_Fig4: ヒストグラム\n# Y軸: 一人あたり購買力平価GDP（対数）\n# 時計回りで90度回転したヒストグラムを作成するために、yにマッピング\nGrid_Fig4 <- Country_df %>%\n  ggplot() +\n  geom_histogram(aes(y = PPP_per_capita), color = \"white\",\n                 boundary = 0, bins = 15) +\n  scale_y_log10() +\n  labs(x = \"国数\", y = \"一人あたり購買力平価GDP（対数）\") +\n  theme_bw(base_size = 12)\n\n　それぞれの図は以下の通りです。左上、右上、左下、右下の順でそれぞれGrid_Fig1、Grid_Fig2、Grid_Fig3、Grid_Fig4です。\n\n\n\n\n\n\n\n\n\n　まずはGrid_Fig1とGrid_Fig2を横に並べてみましょう。まずは{ggpubr}を読み込みます。\n\npacman::p_load(ggpubr)\n\n　図を並べる際はggarrange()関数を使用し、まず、結合したい図のオブジェクトを入力します。また、2つの図を横に並べることは1行2列のレイアウトであることを意味するため、nrowとncol引数の実引数としてそれぞれ1と2を指定します。\n\nggarrange(Grid_Fig1, Grid_Fig2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n　右側にあるGrid_Fig2の幅が若干広いような気がします。この場合、aling = \"hv\"を入れると綺麗に揃えられます。\n\nggarrange(Grid_Fig1, Grid_Fig2, nrow = 1, ncol = 2, align = \"hv\")\n\n\n\n\n\n\n\n\n　続きまして、3つの図を以下のように並べるとします。\n\n\n\n\n\n  \n    Grid_Fig3 \n     \n  \n  \n    Grid_Fig1 \n    Grid_Fig4 \n  \n\n\n\n\n\n　今回は3つの図オブジェクトを入れるだけでは不十分です。なぜなら、ggarrange()関数は左上から右下の順番へ一つずつ図を入れるからです。もし、3つの図オブジェクトのみを入れると、以下のように配置されます。\n\n\n\n\n\n  \n    Grid_Fig3 \n    Grid_Fig1 \n  \n  \n    Grid_Fig4 \n     \n  \n\n\n\n\n\n　これを回避するためには空欄とするグリッドにNULLを指定する必要があります。\n\nggarrange(Grid_Fig3, # 1行1列目\n          NULL,      # 1行2列目\n          Grid_Fig1, # 2行1列目\n          Grid_Fig4, # 2行2列目\n          nrow = 2, ncol = 2, align = \"hv\")\n\n\n\n\n\n\n\n\n　次はヒストグラムを小さく調整します。左上のGrid_Fig3の上下の幅を、右下のGrid_Fig4は左右の幅を狭くします。このためにはwidthsとheights引数が必要です。たとえば、heights = c(0.3, 0.7)だと1行目は全体の30%、2行目は全体の70%になります。今回は1行目と2行目の比率は3:7に、1列目と2列目の比は7:3とします。また、それぞれの図に(a)、(b)、(c)を付けます。空欄となるグリッドのラベルはNAか\"\"にします。NULLではないことに注意してください。\n\nggarrange(Grid_Fig3, \n          NULL, \n          Grid_Fig1, \n          Grid_Fig4,\n          nrow = 2, ncol = 2, align = \"hv\",\n          # グリットの大きさを調整\n          widths = c(0.7, 0.3), heights = c(0.3, 0.7),\n          # 各図にラベルを付ける\n          labels = c(\"(a)\", NA, \"(b)\", \"(c)\"))\n\n\n\n\n\n\n\n\n　左上のGrid_Fig3と左下のGrid_Fig1は横軸のラベルを共有しており、左下のGrid_Fig1と右下のGrid_Fig4は縦軸のラベルを共有しています。以下ではGrid_Fig3の横軸ラベル、Grid_Fig4の縦軸ラベルを消します。ggplotオブジェクトにtheme(axis.title.x = element_blank())を+で繋ぐと横軸のラベルが表示されなくなります。ggarrange()に入れるオブジェクトに直接アクセスし、それぞれの軸ラベルを消してみましょう5。\n\nggarrange(Grid_Fig3 + theme(axis.title.x = element_blank()), \n          NULL, \n          Grid_Fig1, \n          Grid_Fig4 + theme(axis.title.y = element_blank()),\n          nrow = 2, ncol = 2, align = \"hv\",\n          widths = c(0.7, 0.3), heights = c(0.3, 0.7),\n          labels = c(\"(a)\", NA, \"(b)\", \"(c)\"))\n\n\n\n\n\n\n\n\n　これで完成です。ggarrange()には他にもカスタマイズ可能な部分がいっぱいあります。詳細はコンソール上で?ggarrangeを入力し、ヘルプを参照してください。\n　他にも{egg}パッケージのggarrange()、{cowplot}のplot_grid()、{patchwork}の+、|、/演算子などがあります。それぞれ強みがあり、便利なパッケージです。興味のある読者は以下のページで使い方を確認してみてください。\n\negg\ncowplot\npatchwork"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html",
    "href": "tutorial/R/dplyr_intro.html",
    "title": "dplyr入門",
    "section": "",
    "text": "以下の内容は現在執筆中の内容の一部となります。\n\nSong Jaehyun・矢内勇生『私たちのR: ベストプラクティスの探求』(E-book)\n\nいきなりオブジェクト、関数、引数といった馴染みのない概念が出てきます。これらの概念に馴染みのない方は、予め「Rプログラミング入門の入門」をご一読ください。\n実習用データ:  Ramen.csv Ramen2.csv"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#パッケージと実習用データの読み込み",
    "href": "tutorial/R/dplyr_intro.html#パッケージと実習用データの読み込み",
    "title": "dplyr入門",
    "section": "パッケージと実習用データの読み込み",
    "text": "パッケージと実習用データの読み込み\n　パッケージは{dplyr}でも、{tidyverse}でもどれでも構いません。ここではデータをtibble型として読み込むため、{tidyverse}を読み込んでおきます。\n\nlibrary(tidyverse)\n\n　それでは今回の実習用データを読み込みましょう。Ramen.csvには「ぐるなび」から取得したラーメン屋6292店舗の情報が入っています。具体的には東京、神奈川、千葉、埼玉、大阪、京都、兵庫、奈良、和歌山それぞれ都府県にあるラーメン屋の中から最大1000店舗の情報を抽出したものです。東京都は、ぐるなびに登録したラーメン屋が3000店舗以上ですが、1000店舗の基準はぐるなびの「おすすめ」の順で上位1000店舗となります。また、店側またはぐるなびが登録したカテゴリを基準に抽出したため、実際はラーメン屋ではないにもかかわらずラーメン屋としてデータ内に含まれている可能性があります。\n　まず、このデータを読み込み、dfという名付けます。R内蔵関数であるread.csv()を使ってデータを読み込んでも以下の内容を実習するにあたって全く問題はございません。read.csv()から読み込まれたデータのクラスはデータフレーム、read_csv()の場合はtibbleです。tibbleはデータフレームの拡張版であり、データフレームで可能な操作は全てtibbleにおいても可能です。ここではtibbleを使いますが、こちらの方が、結果が読みやすく出力されるからです。\n\n# ファイルのパスは適宜修正してください\ndf <- read_csv(\"Data/Ramen.csv\")\n\n　本サンプルデータはUTF-8で保存されており、文字化けが生じる場合、以下のように対処してください。\n\n# readrパッケージのread_csv()を使う場合\ndf <- read_csv(\"Data/Ramen.csv\", locale = locale(encoding = \"utf8\"))\n\n　データの中身を確認してみましょう。\n\ndf\n\n# A tibble: 6,292 × 14\n   ID     Name  Pref  Zipcode Latitude Longitude Line  Station  Walk   Bus   Car\n   <chr>  <chr> <chr>   <dbl>    <dbl>     <dbl> <chr> <chr>   <dbl> <dbl> <dbl>\n 1 e5396… 居酒… 東京… 1040031     35.7      140. 地下… 銀座一…     3    NA    NA\n 2 gfeb6… 本格… 東京… 1100005     35.7      140. 地下… 仲御徒…     1    NA    NA\n 3 ggt59… 食べ… 東京… 1250041     35.8      140. ＪＲ… 金町駅      2    NA    NA\n 4 g1813… 博多… 東京… 1920904     35.7      139. ＪＲ  八王子…     1    NA    NA\n 5 ggww1… まさ… 東京… 1500042     35.7      140. 地下… 渋谷駅      7    NA    NA\n 6 gdzk5… 完全… 東京… 1000013     35.7      140. 地下… 虎ノ門…     3    NA    NA\n 7 ga2g2… 鶏そ… 東京… 1760006     35.7      140. 西武… 江古田…     2    NA    NA\n 8 gg9m1… 宴会… 東京… 1010021     35.7      140. ＪＲ  秋葉原…     4    NA    NA\n 9 gdvk2… 中国… 東京… 1000006     35.7      140. ＪＲ  有楽町…     1    NA    NA\n10 gggb2… 中国… 東京… 1140002     35.8      140. 地下… 王子駅      2    NA    NA\n# … with 6,282 more rows, and 3 more variables: Budget <dbl>, ScoreN <dbl>,\n#   Score <dbl>\n\n\n　1行目の# A tibble: 6,292 x 14から、ケース数 (店舗数)は6292、変数は14個あることが分かります。各変数の詳細は以下の通りです。\n\n\n\n\n\n\n  \n  \n    \n      変数名\n      説明\n    \n  \n  \n    ID\n店舗ID\n    Name\n店舗名\n    Pref\n店舗の所在地 (都府県)\n    Zipcode\n店舗の郵便番号\n    Latitude\n緯度\n    Longitude\n経度\n    Line\n最寄りの駅の路線\n    Station\n最寄りの駅\n    Walk\n最寄りの駅からの距離 (徒歩; 分)\n    Bus\n最寄りの駅からの距離 (バス; 分)\n    Car\n最寄りの駅からの距離 (車; 分)\n    Budget\n平均予算 (円)\n    ScoreN\n口コミの数\n    Score\n口コミ評価の平均値\n  \n  \n  \n\n\n\n\n　それではここからはdfを用いた{dplyr}の様々な機能を紹介していきます。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#パイプ演算子",
    "href": "tutorial/R/dplyr_intro.html#パイプ演算子",
    "title": "dplyr入門",
    "section": "パイプ演算子 (%>%)",
    "text": "パイプ演算子 (%>%)\n\n\n\n\n\n\n新しいパイプ演算子|>\n\n\n\nこれまでのパイプ演算子 (%>%) は{magrittr}パッケージから提供されましたが、R 4.1からはR内蔵のパイプ演算子 (|>) が追加されました。使い方はやや異なりますが、本ページの内容であれば、%>%の代わりに|>を使って頂いても問題有りません。\n\n\n　{dplyr}パッケージを利用する前にパイプ演算子について説明します。パイプ演算子は{dplyr}に含まれている演算子ではなく、{magrittr}という別のパッケージから提供される演算子ですが、{tidyverse}パッケージを読み込むと自動的に読み込まれます。パイプ演算子はx %>% y()のような書き方となりますが、これは「xをy()の第一引数として渡す」ことを意味します。xの部分はベクトルやデータフレームのようなオブジェクトでも、関数でも構いません。なぜなら、関数から得られた結果もまたベクトルやデータフレームといったものになるからです。つまり、x() %>% y()という使い方も可能です。そして、パイプは無限に繋ぐこともできます。「データdfを関数x()で処理をし、その結果をまた関数y()で処理する」ことは、パイプを使うとdf %>% x() %>% y()のような書き方となります。\n　たとえば、「paste(3, \"+\", 5, \"=\", 8)を実行し、その結果をrep()関数を使って3回複製し、それをprint()を使って出力する」コードを考えてみましょう。方法としては2つ考えられます。まずは、それぞれの処理を別途のオブジェクトに格納する方法です。そして二つ目は関数の中に関数を使う方法です。\n\n# 方法1: 一関数一オブジェクト\nResult1 <- paste(3, \"+\", 5, \"=\", 8)\nResult2 <- rep(Result1, 3)\nprint(Result2)\n\n[1] \"3 + 5 = 8\" \"3 + 5 = 8\" \"3 + 5 = 8\"\n\n# 方法2: 関数の中に関数の中に関数\nprint(rep(paste(3, \"+\", 5, \"=\", 8), 3))\n\n[1] \"3 + 5 = 8\" \"3 + 5 = 8\" \"3 + 5 = 8\"\n\n\n　どれも結果は同じです。コードを書く手間を考えれば、後者の方が楽かも知れませんが、可読性があまりよくありません。一方、前者は可読性は良いものの、コードも長くなり、オブジェクトを2つも作ってしまうのでメモリの無駄遣いになります。\n　コードの可読性と書く手間、両方を満足する書き方がパイプ演算子%>%です。まずは、例から見ましょう。\n\n# %>%を使う\npaste(3, \"+\", 5, \"=\", 8) %>% rep(3) %>% print()\n\n[1] \"3 + 5 = 8\" \"3 + 5 = 8\" \"3 + 5 = 8\"\n\n\n　まず、結果は先ほどと同じです。それではコードの説明をしましょう。まずは、paste(3, \"+\", 5, \"=\", 8)を実行します。そしてその結果をそのままrep()関数の第一引数として渡されます。つまり、rep(paste(3, \"+\", 5, \"=\", 8), 3)になるわけです。ここではrep(3)と書きましたが、第一引数が渡されたため、3は第二引数扱いになります (パイプ演算子前のオブジェクトを第二、三引数として渡す方法は適宜説明します。)。そして、これをまたprint()関数に渡します。結果としてはprint(rep(paste(3, \"+\", 5, \"=\", 8), 3))となります。\n　関数を重ねると読む順番は「カッコの内側から外側へ」になりますが、パイプ演算子を使うと「左 (上)から右 (下)へ」といったより自然な読み方が可能になります。また、以下のコードのように、パイプ演算子後に改行を行うことでより読みやすいコードになります。これからはパイプ演算子の後は必ず改行をします。\n\n# 改行 (+字下げ)したらもっと読みやすくなる\npaste(3, \"+\", 5, \"=\", 8) %>% \n    rep(3) %>% \n    print()\n\n　パイプ演算子を使わない方法 図 1 のようにイメージできます。一回の処理ごとに結果を保存し、それをまた次の処理時においてデータとして使うイメージです。\n\n\n\n\n\n図 1: パイプ演算子を使わない場合\n\n\n\n\n　一方、 図 2 はパイプ演算子を使う場合のプロセスです。処理後の結果を保存せず、すぐに次のプロセスに渡すことで、メモリ (図だとボウル)や時間、コードの無駄を減らすことができます。むろん、 図 1 の結果1を使って色々試してみたい場合は、一旦結果1までは格納し、適宜引き出して使った方が効率的でしょう。パイプ演算子はたしかに便利で、「今どき」のRの書き方を象徴するようなものですが、一つの結果を出すまであまりにも多くのパイプ演算子を使うことはあ望ましくありません。\n\n\n\n\n\n図 2: パイプ演算子を使う場合\n\n\n\n\n　データハンドリングもこれど同様に、様々な作業を順に沿って行う必要があります。例えば、「(1) 列を選択して、(2) 欠損値を含む列を除去して、 (3) ある変数の値を100倍にして、(4) ある変数の値がが小さい行から大きい順へ並び替える」といった手順です。これらの作業はパイプ演算子を使えば、スムーズに行うことが可能です。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#列の抽出",
    "href": "tutorial/R/dplyr_intro.html#列の抽出",
    "title": "dplyr入門",
    "section": "列の抽出",
    "text": "列の抽出\n\n特定の列を抽出する\n　まずは、データフレーム (または、tibble)から特定の列のみを残す、除去する方法について紹介します。たとえば、dfからID、Name、Pref、Scoreのみを残すとします。{dplyr}を使わない方法と{dplyr}のselect()関数を使った方法を紹介します。\n\n# dplyrを使わない方法\ndf[, c(\"ID\", \"Name\", \"Pref\", \"Score\")]\n\n# A tibble: 6,292 × 4\n   ID      Name                                                     Pref   Score\n   <chr>   <chr>                                                    <chr>  <dbl>\n 1 e539604 居酒屋 龍記 京橋店                                       東京都 NA   \n 2 gfeb600 本格上海料理 新錦江 上野御徒町本店                       東京都  4.5 \n 3 ggt5900 食べ飲み放題×中華ビストロ NOZOMI（のぞみ）               東京都 NA   \n 4 g181340 博多餃子軒 八王子店 タピオカ店 Bull Pulu（ブルプル）併設 東京都 NA   \n 5 ggww100 まさ屋 渋谷店                                            東京都 NA   \n 6 gdzk500 完全個室 上海レストラン 檸檬 霞ヶ関ビル内店              東京都 NA   \n 7 ga2g202 鶏そば きらり                                            東京都 NA   \n 8 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店                    東京都  3.33\n 9 gdvk200 中国料理 宝龍                                            東京都  2.5 \n10 gggb200 中国料理 天安門                                          東京都 NA   \n# … with 6,282 more rows\n\n# dplyr::select()を使う方法\n# select(df, ID, Name, Pref, Score)でもOK\ndf %>%\n  select(ID, Name, Pref, Score)\n\n# A tibble: 6,292 × 4\n   ID      Name                                                     Pref   Score\n   <chr>   <chr>                                                    <chr>  <dbl>\n 1 e539604 居酒屋 龍記 京橋店                                       東京都 NA   \n 2 gfeb600 本格上海料理 新錦江 上野御徒町本店                       東京都  4.5 \n 3 ggt5900 食べ飲み放題×中華ビストロ NOZOMI（のぞみ）               東京都 NA   \n 4 g181340 博多餃子軒 八王子店 タピオカ店 Bull Pulu（ブルプル）併設 東京都 NA   \n 5 ggww100 まさ屋 渋谷店                                            東京都 NA   \n 6 gdzk500 完全個室 上海レストラン 檸檬 霞ヶ関ビル内店              東京都 NA   \n 7 ga2g202 鶏そば きらり                                            東京都 NA   \n 8 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店                    東京都  3.33\n 9 gdvk200 中国料理 宝龍                                            東京都  2.5 \n10 gggb200 中国料理 天安門                                          東京都 NA   \n# … with 6,282 more rows\n\n\n　どれも結果は同じですが、select()関数を使った方がより読みやすいコードになっているでしょう。むろん、select()関数を使わない方がスッキリする方も知るかも知れません。実際、自分でパッケージなどを作成する際はselect()を使わない場合が多いです。ただし、一般的な分析の流れではselect()の方がコードも意味も明確となり、パイプ演算子でつなぐのも容易です。\n　select()関数の使い方は非常に簡単です。第一引数はデータフレーム (または、tibble)ですが、パイプ演算子を使う場合は省略可能です。第二引数以降の引数はデータフレーム/tibble内の変数名です。つまり、ここには残す変数名のみを書くだけで十分です。\n　また、select()関数を使って列の順番を変えることもできます。たとえば、ID、Pref、Name、Scoreの順で列を残すなら、この順番で引数を書くだけです。\n\ndf %>%\n  select(ID, Pref, Name)\n\n# A tibble: 6,292 × 3\n   ID      Pref   Name                                                    \n   <chr>   <chr>  <chr>                                                   \n 1 e539604 東京都 居酒屋 龍記 京橋店                                      \n 2 gfeb600 東京都 本格上海料理 新錦江 上野御徒町本店                      \n 3 ggt5900 東京都 食べ飲み放題×中華ビストロ NOZOMI（のぞみ）              \n 4 g181340 東京都 博多餃子軒 八王子店 タピオカ店 Bull Pulu（ブルプル）併設\n 5 ggww100 東京都 まさ屋 渋谷店                                           \n 6 gdzk500 東京都 完全個室 上海レストラン 檸檬 霞ヶ関ビル内店             \n 7 ga2g202 東京都 鶏そば きらり                                           \n 8 gg9m100 東京都 宴会個室×餃子酒場 北京飯店 秋葉原本店                   \n 9 gdvk200 東京都 中国料理 宝龍                                           \n10 gggb200 東京都 中国料理 天安門                                         \n# … with 6,282 more rows\n\n\n\n\n特定の列を抽出し、列名を変更する\n　また、特定の列を残す際、変数名を変更することも可能です。今回もID、Name、Pref、Scoreのみを残しますが、Pref列はPrefectureに変えてみましょう。\n\ndf %>%\n  select(ID, Name, Prefecture = Pref, Score)\n\n# A tibble: 6,292 × 4\n   ID      Name                                                Prefecture Score\n   <chr>   <chr>                                               <chr>      <dbl>\n 1 e539604 居酒屋 龍記 京橋店                                  東京都     NA   \n 2 gfeb600 本格上海料理 新錦江 上野御徒町本店                  東京都      4.5 \n 3 ggt5900 食べ飲み放題×中華ビストロ NOZOMI（のぞみ）          東京都     NA   \n 4 g181340 博多餃子軒 八王子店 タピオカ店 Bull Pulu（ブルプル… 東京都     NA   \n 5 ggww100 まさ屋 渋谷店                                       東京都     NA   \n 6 gdzk500 完全個室 上海レストラン 檸檬 霞ヶ関ビル内店         東京都     NA   \n 7 ga2g202 鶏そば きらり                                       東京都     NA   \n 8 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店               東京都      3.33\n 9 gdvk200 中国料理 宝龍                                       東京都      2.5 \n10 gggb200 中国料理 天安門                                     東京都     NA   \n# … with 6,282 more rows\n\n\n　抽出する際、変数を新しい変数名 = 既存の変数名にするだけで、変数名が簡単に変更できました。もし、特定の列は抽出しないものの、変数名を変えるにはどうすれば良いでしょうか。ここではdfのPrefをPrefectureに、WalkをDistanceに変更してみます。{dplyr}を使わない場合と{dplyr}のrename()関数を使う場合を両方紹介します。\n　まずは、name()関数についてですが、これはデータフレーム (または、tibble)の変数名をベクトルとして出力する関数です。\n\nnames(df)\n\n [1] \"ID\"        \"Name\"      \"Pref\"      \"Zipcode\"   \"Latitude\"  \"Longitude\"\n [7] \"Line\"      \"Station\"   \"Walk\"      \"Bus\"       \"Car\"       \"Budget\"   \n[13] \"ScoreN\"    \"Score\"    \n\n\n　察しの良い読者は気づいたかも知れませんが、names(データフレーム/tibble名)の結果はベクトルであり、上書きも可能です。つまり、names(df)の3番目と9番目の要素を\"Prefecture\"と\"Distance\"に上書きすることができるということです。\n\n# dplyrを使わずに列名を変更する方法\nnames(df)[c(3, 9)] <- c(\"Prefecture\", \"Distance\")\n\n# dfの中身を出力\ndf\n\n# A tibble: 6,292 × 14\n   ID      Name     Prefecture Zipcode Latitude Longitude Line  Station Distance\n   <chr>   <chr>    <chr>        <dbl>    <dbl>     <dbl> <chr> <chr>      <dbl>\n 1 e539604 居酒屋 … 東京都     1040031     35.7      140. 地下… 銀座一…        3\n 2 gfeb600 本格上…  東京都     1100005     35.7      140. 地下… 仲御徒…        1\n 3 ggt5900 食べ飲…  東京都     1250041     35.8      140. ＪＲ… 金町駅         2\n 4 g181340 博多餃…  東京都     1920904     35.7      139. ＪＲ  八王子…        1\n 5 ggww100 まさ屋 … 東京都     1500042     35.7      140. 地下… 渋谷駅         7\n 6 gdzk500 完全個…  東京都     1000013     35.7      140. 地下… 虎ノ門…        3\n 7 ga2g202 鶏そば … 東京都     1760006     35.7      140. 西武… 江古田…        2\n 8 gg9m100 宴会個…  東京都     1010021     35.7      140. ＪＲ  秋葉原…        4\n 9 gdvk200 中国料…  東京都     1000006     35.7      140. ＪＲ  有楽町…        1\n10 gggb200 中国料…  東京都     1140002     35.8      140. 地下… 王子駅         2\n# … with 6,282 more rows, and 5 more variables: Bus <dbl>, Car <dbl>,\n#   Budget <dbl>, ScoreN <dbl>, Score <dbl>\n\n\n　簡単に変数名の変更ができました。続いて、{dplyr}のrename()関数を使った方法です。今回は、PrefectureをPrefに、DistanceをWalkに戻して見ましょう。そして、出力するだけにとどまらず、dfに上書きしましょう。\n\n# dfのPrefectureをPrefに、DistanceをWalkに変更し、上書きする\ndf <- df %>%\n  rename(Pref = Prefecture, Walk = Distance)\n\n　これで終わりです。実はselect()関数と使い方がほぼ同じです。ただし、残す変数名を指定する必要がなく、名前を変更する変数名と新しい変数名を入れるだけです。変数が少ないデータならselect()でもあまり不便は感じないかも知れませんが、変数が多くなるとrename()関数は非常に便利です。\n\n\n特定の列を除外する\n　逆に、一部の変数をデータフレーム (または、tibble)から除去したい場合もあるでしょう。たとえば、緯度 (Latitude)と経度 (Longitude)はラーメン屋の情報としては不要かもしれません。この2つの変数を除外するためにはどうすれば良いでしょうか。まず考えられるのは、この2つの変数を除いた変数を指定・抽出する方法です。\n\ndf %>%\n  select(ID, Name, Pref, Zipcode, \n         Line, Station, Walk, Bus, Car, Budget, ScoreN, Score)\n\n# A tibble: 6,292 × 12\n   ID    Name  Pref  Zipcode Line  Station  Walk   Bus   Car Budget ScoreN Score\n   <chr> <chr> <chr>   <dbl> <chr> <chr>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n 1 e539… 居酒… 東京… 1040031 地下… 銀座一…     3    NA    NA   3000      0 NA   \n 2 gfeb… 本格… 東京… 1100005 地下… 仲御徒…     1    NA    NA   2000      2  4.5 \n 3 ggt5… 食べ… 東京… 1250041 ＪＲ… 金町駅      2    NA    NA   2980      0 NA   \n 4 g181… 博多… 東京… 1920904 ＪＲ  八王子…     1    NA    NA   2000      0 NA   \n 5 ggww… まさ… 東京… 1500042 地下… 渋谷駅      7    NA    NA    380      0 NA   \n 6 gdzk… 完全… 東京… 1000013 地下… 虎ノ門…     3    NA    NA   2980      0 NA   \n 7 ga2g… 鶏そ… 東京… 1760006 西武… 江古田…     2    NA    NA    850      0 NA   \n 8 gg9m… 宴会… 東京… 1010021 ＪＲ  秋葉原…     4    NA    NA   2000      3  3.33\n 9 gdvk… 中国… 東京… 1000006 ＪＲ  有楽町…     1    NA    NA   1000      2  2.5 \n10 gggb… 中国… 東京… 1140002 地下… 王子駅      2    NA    NA   2000      0 NA   \n# … with 6,282 more rows\n\n\n　かなり長いコードになりましたね。しかし、もっと簡単な方法があります。それは-を使う方法です。\n\ndf %>%\n  select(-Latitude, -Longitude) # select(-c(Latitude, Longitude))\n\n# A tibble: 6,292 × 12\n   ID    Name  Pref  Zipcode Line  Station  Walk   Bus   Car Budget ScoreN Score\n   <chr> <chr> <chr>   <dbl> <chr> <chr>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n 1 e539… 居酒… 東京… 1040031 地下… 銀座一…     3    NA    NA   3000      0 NA   \n 2 gfeb… 本格… 東京… 1100005 地下… 仲御徒…     1    NA    NA   2000      2  4.5 \n 3 ggt5… 食べ… 東京… 1250041 ＪＲ… 金町駅      2    NA    NA   2980      0 NA   \n 4 g181… 博多… 東京… 1920904 ＪＲ  八王子…     1    NA    NA   2000      0 NA   \n 5 ggww… まさ… 東京… 1500042 地下… 渋谷駅      7    NA    NA    380      0 NA   \n 6 gdzk… 完全… 東京… 1000013 地下… 虎ノ門…     3    NA    NA   2980      0 NA   \n 7 ga2g… 鶏そ… 東京… 1760006 西武… 江古田…     2    NA    NA    850      0 NA   \n 8 gg9m… 宴会… 東京… 1010021 ＪＲ  秋葉原…     4    NA    NA   2000      3  3.33\n 9 gdvk… 中国… 東京… 1000006 ＪＲ  有楽町…     1    NA    NA   1000      2  2.5 \n10 gggb… 中国… 東京… 1140002 地下… 王子駅      2    NA    NA   2000      0 NA   \n# … with 6,282 more rows\n\n\n　除外したい変数名の前に-を付けただけです。また、-Latitudeと-Longitudeをそれぞれ指定せず、-c(Latitude, Longitude)のようにc()でまとめるのも可能です。\n\n\n隣接した列を指定する\n　先ほど、dfから緯度 (Latitude)と経度 (Longitude)を除外する例を考えてみましょう。-を使うと簡単ですが、場合によっては残す変数名を指定する必要もあります。\n\ndf %>%\n  select(ID, Name, Pref, Zipcode, \n         Line, Station, Walk, Bus, Car, Budget, ScoreN, Score)\n\n　よく考えてみれば、IDからZipcodeは隣接した列ですし、LineからScoreまでもそうです。これはnames()関数で確認できます。\n\nnames(df)\n\n [1] \"ID\"        \"Name\"      \"Pref\"      \"Zipcode\"   \"Latitude\"  \"Longitude\"\n [7] \"Line\"      \"Station\"   \"Walk\"      \"Bus\"       \"Car\"       \"Budget\"   \n[13] \"ScoreN\"    \"Score\"    \n\n\n　ここで便利な演算子が:です。これまで、xからyまでの公差1の等差数列を作成する際にx:yを使って来ましたが、これに非常に似ています。データフレーム (または、tibble)の「x列からy列まで」の表記もselect()関数内では:と書くことができます。したがって、上記のコードは以下のように短縮可能です。\n\ndf %>%\n  select(ID:Zipcode, Line:Score)\n\n　「dfのIDからZipcodeまで、そしてLineからScoreまでの列を選択する」という意味です。非常に便利な演算子ですので、-と合わせて覚えておきましょう。\n\n\n一部の列の順番だけを変える\n　ある列の位置を替えたいとします。たとえば、ScoreとScoreNをそれぞれ1列目、2列目にしたい場合、どうすれば良いでしょうか。これまで勉強したことを考えると、以下のようなコードで問題ないでしょう。\n\ndf %>%\n  select(Score, ScoreN, ID:Budget)\n\n# A tibble: 6,292 × 14\n   Score ScoreN ID    Name  Pref  Zipcode Latitude Longitude Line  Station  Walk\n   <dbl>  <dbl> <chr> <chr> <chr>   <dbl>    <dbl>     <dbl> <chr> <chr>   <dbl>\n 1 NA         0 e539… 居酒… 東京… 1040031     35.7      140. 地下… 銀座一…     3\n 2  4.5       2 gfeb… 本格… 東京… 1100005     35.7      140. 地下… 仲御徒…     1\n 3 NA         0 ggt5… 食べ… 東京… 1250041     35.8      140. ＪＲ… 金町駅      2\n 4 NA         0 g181… 博多… 東京… 1920904     35.7      139. ＪＲ  八王子…     1\n 5 NA         0 ggww… まさ… 東京… 1500042     35.7      140. 地下… 渋谷駅      7\n 6 NA         0 gdzk… 完全… 東京… 1000013     35.7      140. 地下… 虎ノ門…     3\n 7 NA         0 ga2g… 鶏そ… 東京… 1760006     35.7      140. 西武… 江古田…     2\n 8  3.33      3 gg9m… 宴会… 東京… 1010021     35.7      140. ＪＲ  秋葉原…     4\n 9  2.5       2 gdvk… 中国… 東京… 1000006     35.7      140. ＪＲ  有楽町…     1\n10 NA         0 gggb… 中国… 東京… 1140002     35.8      140. 地下… 王子駅      2\n# … with 6,282 more rows, and 3 more variables: Bus <dbl>, Car <dbl>,\n#   Budget <dbl>\n\n\n　しかし、{dplyr}にはrelocate()というより便利な専用関数を提供しています。relocate()には変数名を指定するだけですが、ここで指定した変数がデータフレーム (または、tibble)の最初列の方に移動します。\n\ndf %>%\n  relocate(Score, ScoreN)\n\n# A tibble: 6,292 × 14\n   Score ScoreN ID    Name  Pref  Zipcode Latitude Longitude Line  Station  Walk\n   <dbl>  <dbl> <chr> <chr> <chr>   <dbl>    <dbl>     <dbl> <chr> <chr>   <dbl>\n 1 NA         0 e539… 居酒… 東京… 1040031     35.7      140. 地下… 銀座一…     3\n 2  4.5       2 gfeb… 本格… 東京… 1100005     35.7      140. 地下… 仲御徒…     1\n 3 NA         0 ggt5… 食べ… 東京… 1250041     35.8      140. ＪＲ… 金町駅      2\n 4 NA         0 g181… 博多… 東京… 1920904     35.7      139. ＪＲ  八王子…     1\n 5 NA         0 ggww… まさ… 東京… 1500042     35.7      140. 地下… 渋谷駅      7\n 6 NA         0 gdzk… 完全… 東京… 1000013     35.7      140. 地下… 虎ノ門…     3\n 7 NA         0 ga2g… 鶏そ… 東京… 1760006     35.7      140. 西武… 江古田…     2\n 8  3.33      3 gg9m… 宴会… 東京… 1010021     35.7      140. ＪＲ  秋葉原…     4\n 9  2.5       2 gdvk… 中国… 東京… 1000006     35.7      140. ＪＲ  有楽町…     1\n10 NA         0 gggb… 中国… 東京… 1140002     35.8      140. 地下… 王子駅      2\n# … with 6,282 more rows, and 3 more variables: Bus <dbl>, Car <dbl>,\n#   Budget <dbl>\n\n\n　relocate()を使うとID:Budgetが省略可能となり、より短いコードになります。もう一つの例は、最初に持ってくるのではなく、「ある変数の前」または「ある変数の後」に移動させるケースです。これもrelocate()で可能ですが、もう一つの引数が必要です。PrefとZipcdoeの順番を変えるなら、まずは以下のような方法が考えられます。\n\ndf %>%\n  select(ID:Name, Zipcode, Pref, Latitude:Score)\n\n# A tibble: 6,292 × 14\n   ID     Name  Zipcode Pref  Latitude Longitude Line  Station  Walk   Bus   Car\n   <chr>  <chr>   <dbl> <chr>    <dbl>     <dbl> <chr> <chr>   <dbl> <dbl> <dbl>\n 1 e5396… 居酒… 1040031 東京…     35.7      140. 地下… 銀座一…     3    NA    NA\n 2 gfeb6… 本格… 1100005 東京…     35.7      140. 地下… 仲御徒…     1    NA    NA\n 3 ggt59… 食べ… 1250041 東京…     35.8      140. ＪＲ… 金町駅      2    NA    NA\n 4 g1813… 博多… 1920904 東京…     35.7      139. ＪＲ  八王子…     1    NA    NA\n 5 ggww1… まさ… 1500042 東京…     35.7      140. 地下… 渋谷駅      7    NA    NA\n 6 gdzk5… 完全… 1000013 東京…     35.7      140. 地下… 虎ノ門…     3    NA    NA\n 7 ga2g2… 鶏そ… 1760006 東京…     35.7      140. 西武… 江古田…     2    NA    NA\n 8 gg9m1… 宴会… 1010021 東京…     35.7      140. ＪＲ  秋葉原…     4    NA    NA\n 9 gdvk2… 中国… 1000006 東京…     35.7      140. ＪＲ  有楽町…     1    NA    NA\n10 gggb2… 中国… 1140002 東京…     35.8      140. 地下… 王子駅      2    NA    NA\n# … with 6,282 more rows, and 3 more variables: Budget <dbl>, ScoreN <dbl>,\n#   Score <dbl>\n\n\n　これをrelocate()で書き換えるなら、.afterまたは.before引数が必要になります。relocate(変数名1, .after = 変数名2)は「変数1を変数2の直後に移動させる」 ことを意味します。\n\ndf %>%\n  relocate(Pref, .after = Zipcode)\n\n# A tibble: 6,292 × 14\n   ID     Name  Zipcode Pref  Latitude Longitude Line  Station  Walk   Bus   Car\n   <chr>  <chr>   <dbl> <chr>    <dbl>     <dbl> <chr> <chr>   <dbl> <dbl> <dbl>\n 1 e5396… 居酒… 1040031 東京…     35.7      140. 地下… 銀座一…     3    NA    NA\n 2 gfeb6… 本格… 1100005 東京…     35.7      140. 地下… 仲御徒…     1    NA    NA\n 3 ggt59… 食べ… 1250041 東京…     35.8      140. ＪＲ… 金町駅      2    NA    NA\n 4 g1813… 博多… 1920904 東京…     35.7      139. ＪＲ  八王子…     1    NA    NA\n 5 ggww1… まさ… 1500042 東京…     35.7      140. 地下… 渋谷駅      7    NA    NA\n 6 gdzk5… 完全… 1000013 東京…     35.7      140. 地下… 虎ノ門…     3    NA    NA\n 7 ga2g2… 鶏そ… 1760006 東京…     35.7      140. 西武… 江古田…     2    NA    NA\n 8 gg9m1… 宴会… 1010021 東京…     35.7      140. ＪＲ  秋葉原…     4    NA    NA\n 9 gdvk2… 中国… 1000006 東京…     35.7      140. ＪＲ  有楽町…     1    NA    NA\n10 gggb2… 中国… 1140002 東京…     35.8      140. 地下… 王子駅      2    NA    NA\n# … with 6,282 more rows, and 3 more variables: Budget <dbl>, ScoreN <dbl>,\n#   Score <dbl>\n\n\n　.beforeを使うことできます。この場合は「ZipcodeをPrefの直前に移動させる」 ことを指定する必要があります。結果は省略しますが、自分でコードを走らせ、上と同じ結果が得られるかを確認してみてください。\n\ndf %>%\n  relocate(Zipcode, .before = Pref)\n\n\n\nselect()の便利な機能\n　select()関数は他にも便利な機能がいくつかあります。ここではいくつの機能を紹介しますが、より詳しい内容は?dplyr::selectを参照してください。\nstarts_with()とends_with()、contains()、num_range(): 特定の文字を含む変数を選択する\n　まずは、特定の文字を含む変数名を指定する方法です。starts_with(\"X\")、ends_with(\"X\")、contains(\"X\")は変数名が\"X\"で始まるか、\"X\"で終わるか、\"X\"を含むかを判断し、条件に合う変数名を返す関数です。実際の例を見ましょう。\n\n# ID、Nameに続いて、Scoreで始まる変数名を抽出\ndf %>%\n  select(ID, Name, starts_with(\"Score\"))\n\n# A tibble: 6,292 × 4\n   ID      Name                                                     ScoreN Score\n   <chr>   <chr>                                                     <dbl> <dbl>\n 1 e539604 居酒屋 龍記 京橋店                                            0 NA   \n 2 gfeb600 本格上海料理 新錦江 上野御徒町本店                            2  4.5 \n 3 ggt5900 食べ飲み放題×中華ビストロ NOZOMI（のぞみ）                    0 NA   \n 4 g181340 博多餃子軒 八王子店 タピオカ店 Bull Pulu（ブルプル）併設      0 NA   \n 5 ggww100 まさ屋 渋谷店                                                 0 NA   \n 6 gdzk500 完全個室 上海レストラン 檸檬 霞ヶ関ビル内店                   0 NA   \n 7 ga2g202 鶏そば きらり                                                 0 NA   \n 8 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店                         3  3.33\n 9 gdvk200 中国料理 宝龍                                                 2  2.5 \n10 gggb200 中国料理 天安門                                               0 NA   \n# … with 6,282 more rows\n\n# eで終わる変数名を除去\ndf %>%\n  select(-ends_with(\"e\")) # !ends_with(\"e\")も可能\n\n# A tibble: 6,292 × 8\n   ID      Pref   Station       Walk   Bus   Car Budget ScoreN\n   <chr>   <chr>  <chr>        <dbl> <dbl> <dbl>  <dbl>  <dbl>\n 1 e539604 東京都 銀座一丁目駅     3    NA    NA   3000      0\n 2 gfeb600 東京都 仲御徒町駅       1    NA    NA   2000      2\n 3 ggt5900 東京都 金町駅           2    NA    NA   2980      0\n 4 g181340 東京都 八王子駅         1    NA    NA   2000      0\n 5 ggww100 東京都 渋谷駅           7    NA    NA    380      0\n 6 gdzk500 東京都 虎ノ門駅         3    NA    NA   2980      0\n 7 ga2g202 東京都 江古田駅         2    NA    NA    850      0\n 8 gg9m100 東京都 秋葉原駅         4    NA    NA   2000      3\n 9 gdvk200 東京都 有楽町駅         1    NA    NA   1000      2\n10 gggb200 東京都 王子駅           2    NA    NA   2000      0\n# … with 6,282 more rows\n\n# reを含む変数名を抽出するが、ScoreNは除去する\ndf %>%\n  select(contains(\"re\"), -ScoreN)\n\n# A tibble: 6,292 × 2\n   Pref   Score\n   <chr>  <dbl>\n 1 東京都 NA   \n 2 東京都  4.5 \n 3 東京都 NA   \n 4 東京都 NA   \n 5 東京都 NA   \n 6 東京都 NA   \n 7 東京都 NA   \n 8 東京都  3.33\n 9 東京都  2.5 \n10 東京都 NA   \n# … with 6,282 more rows\n\n\n　他の使い方としてはX1、X2のような「文字+数字」の変数を選択する際、starts_with()が活躍します。たとえば、以下のようなmyDF1があるとします。\n\n# tibble()の代わりにdata.frame()も使用可能\nmyDF1 <- tibble(\n  ID  = 1:5,\n  X1  = c(2, 4, 6, 2, 7),\n  Y1  = c(3, 5, 1, 1, 0),\n  X1D = c(4, 2, 1, 6, 9),\n  X2  = c(5, 5, 6, 0, 2),\n  Y2  = c(3, 3, 2, 3, 1),\n  X2D = c(8, 9, 5, 0, 1),\n  X3  = c(3, 0, 3, 0, 2),\n  Y3  = c(1, 5, 9, 1, 3),\n  X3D = c(9, 1, 3, 3, 8)\n)\n\nmyDF1\n\n# A tibble: 5 × 10\n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     2     3     4     5     3     8     3     1     9\n2     2     4     5     2     5     3     9     0     5     1\n3     3     6     1     1     6     2     5     3     9     3\n4     4     2     1     6     0     3     0     0     1     3\n5     5     7     0     9     2     1     1     2     3     8\n\n\n　このmyDF1からID、Y1、Y2、Y3を抽出するにはどうすれば良いでしょうか。これらの変数は隣接していないため、:も使えませんが、starts_with()を使えば簡単です。\n\nmyDF1 %>%\n  select(ID, starts_with(\"Y\"))\n\n# A tibble: 5 × 4\n     ID    Y1    Y2    Y3\n  <int> <dbl> <dbl> <dbl>\n1     1     3     3     1\n2     2     5     3     5\n3     3     1     2     9\n4     4     1     3     1\n5     5     0     1     3\n\n\n　それでは、ID、X1、X2、X3はどうでしょうか。starts_with(\"X\")だと、X1cなども選択されてしまいますね。ここで-ends_with()の出番です。つまり、「まずはstarts_with(\"X\")でXで始まる変数を選択し、続いて、Dで終わるものを除外すればいいじゃん？」です。それでは、やってみましょうか。\n\nmyDF1 %>%\n  select(ID, starts_with(\"X\"), -ends_with(\"D\"))\n\n# A tibble: 5 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1     2     5     3\n2     4     5     0\n3     6     6     3\n4     2     0     0\n5     7     2     2\n\n\n　あらら、IDも同時になくなりましたね1。実はこのような時のために用意された関数があり、それがnum_range()です。num_range()の第一引数はstarts_with()関数と同じですが、第二引数も必要です。この第二引数にはnumeric型のベクトルが必要です。1:3でも、c(1, 2, 3)でも構いません。たとえば、ID、X1、X2、X3するには以下のように書きます。\n\nmyDF1 %>%\n  select(ID, num_range(\"X\", 1:3))\n\n# A tibble: 5 × 4\n     ID    X1    X2    X3\n  <int> <dbl> <dbl> <dbl>\n1     1     2     5     3\n2     2     4     5     0\n3     3     6     6     3\n4     4     2     0     0\n5     5     7     2     2\n\n\nall_of()とany_of(): 文字型ベクトルを用いた変数の選択\n　all_of()とany_of()はselect()内の変数名として文字型ベクトルを使う際に用いる関数です。これは抽出したい列名が既にcharacter型ベクトルとして用意されている場合、便利な関数です。たとえば、以下のName_Vecを考えてみましょう。\n\nName_Vec <- c(\"X1\", \"X2\", \"X3\")\n\n　このName_Vecの要素と同じ列名を持つ列とID列をmyDF1から抽出する方法は以下の2通りです。\n\nmyDF1[, c(\"ID\", Name_Vec)]\n\n# A tibble: 5 × 4\n     ID    X1    X2    X3\n  <int> <dbl> <dbl> <dbl>\n1     1     2     5     3\n2     2     4     5     0\n3     3     6     6     3\n4     4     2     0     0\n5     5     7     2     2\n\nmyDF1 %>%\n  select(ID, all_of(Name_Vec))\n\n# A tibble: 5 × 4\n     ID    X1    X2    X3\n  <int> <dbl> <dbl> <dbl>\n1     1     2     5     3\n2     2     4     5     0\n3     3     6     6     3\n4     4     2     0     0\n5     5     7     2     2\n\n\n　今の例だと、select()を使わない前者の方が便利かも知れませんが、select()内に外の変数名も指定する場合も多いので、後者の方が汎用性は高いです。私から見れば、今の例でも後者の方が読みやすく、使いやすいと思います。\n　それでは以下のようなName_Vecはどうでしょう。今回は、myDF1に含まれていないX4とX5もあります。\n\nName_Vec <- c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")\n\nmyDF1 %>%\n  select(all_of(Name_Vec))\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Columns `X4` and `X5` don't exist.\n\n\n　このようにエラーが出てしまします。つまり、all_of()の場合、引数の要素全てがデータフレーム (または、tibble)に存在する必要があります。もし、ないものは無視して、合致する列だけ取り出したいはどうすれば良いでしょうか。そこで登場するのがany_of()です。\n\nmyDF1 %>%\n  select(any_of(Name_Vec))\n\n# A tibble: 5 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1     2     5     3\n2     4     5     0\n3     6     6     3\n4     2     0     0\n5     7     2     2\n\n\n　any_of()の方がより使いやすいと思う方も多いでしょうが、必ずしもそうとは限りません。たとえば、Name_Vecに誤字などが含まれる場合、any_of()だと誤字が含まれている変数は取り出しません。この場合はむしろちゃんとエラーを表示してくれた方が嬉しいですね。\nlast_col(): 最後の列を選択する\n　普段あまり使わない機能ですが、最後の列を選択するlast_col()という関数もあります。たとえば、last_col(0)にすると最後の列を選択し、last_col(1)なら最後から2番目の列を選択します。たとえば、dfからIDと最後の列を取り出してみましょう。\n\n# IDと最後の列のみを抽出\ndf %>%\n  select(ID, last_col(0))\n\n# A tibble: 6,292 × 2\n   ID      Score\n   <chr>   <dbl>\n 1 e539604 NA   \n 2 gfeb600  4.5 \n 3 ggt5900 NA   \n 4 g181340 NA   \n 5 ggww100 NA   \n 6 gdzk500 NA   \n 7 ga2g202 NA   \n 8 gg9m100  3.33\n 9 gdvk200  2.5 \n10 gggb200 NA   \n# … with 6,282 more rows\n\n\n　最後の2行分を取り出すことも可能です。この場合はlast_col()の引数を長さ1ベクトルでなく、長さ2以上のベクトルにします。最後の行が0、その手前の行が1ですから、中の引数は1:0となります。0:1でも可能ですが、結果が若干異なります。\n\n# IDと最後の2列分を抽出 (引数を1:0と設定)\ndf %>%\n  select(ID, last_col(1:0))\n\nWarning in offset && n <= offset: 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\nWarning in offset && n <= offset: 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\n# A tibble: 6,292 × 3\n   ID      ScoreN Score\n   <chr>    <dbl> <dbl>\n 1 e539604      0 NA   \n 2 gfeb600      2  4.5 \n 3 ggt5900      0 NA   \n 4 g181340      0 NA   \n 5 ggww100      0 NA   \n 6 gdzk500      0 NA   \n 7 ga2g202      0 NA   \n 8 gg9m100      3  3.33\n 9 gdvk200      2  2.5 \n10 gggb200      0 NA   \n# … with 6,282 more rows\n\n\n\n# IDと最後の2列分を抽出 (引数を0:1と設定)\ndf %>%\n  select(ID, last_col(0:1))\n\nWarning in offset && n <= offset: 'length(x) = 2 > 1' in coercion to\n'logical(1)'\n\n\n# A tibble: 6,292 × 3\n   ID      Score ScoreN\n   <chr>   <dbl>  <dbl>\n 1 e539604 NA         0\n 2 gfeb600  4.5       2\n 3 ggt5900 NA         0\n 4 g181340 NA         0\n 5 ggww100 NA         0\n 6 gdzk500 NA         0\n 7 ga2g202 NA         0\n 8 gg9m100  3.33      3\n 9 gdvk200  2.5       2\n10 gggb200 NA         0\n# … with 6,282 more rows\n\n\n　last_col()の引数を1:0にするか0:1にするかによって抽出される順番が異なります。1:0はc(1, 0)、0:1はc(0, 1)と同じであることを考えると理由は簡単です。c(1, 0)の場合、last_col(1), last_col(0)の順番で処理をし、c(0, 1)はlast_col(0)、last_col(1)の順番で処理を行うからです。\n　このlast_col()の引数を空っぽにするとそれは最後の列を意味します。これを利用すれば、「ある変数の最後の列へ移動させる」こともできます。たとえば、IDを最後の列に移動させたい場合、relocate(ID, .after = last_col())のように書きます。\nwhere(): データ型から変数を選択する\n　最後に、「numeric型の列のみ抽出したい」、「character型の列だけほしい」場合に便利なwhere()関数を紹介します。where()の中に入る引数は一つだけであり、データ型を判定する関数名が入ります。たとえば、numeric型か否かを判断する関数はis.numericです。dfからnumeric型の変数のみを抽出したい場合は以下のように書きます。\n\n# numeric型の列を抽出する\ndf %>%\n  select(where(is.numeric))\n\n# A tibble: 6,292 × 9\n   Zipcode Latitude Longitude  Walk   Bus   Car Budget ScoreN Score\n     <dbl>    <dbl>     <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n 1 1040031     35.7      140.     3    NA    NA   3000      0 NA   \n 2 1100005     35.7      140.     1    NA    NA   2000      2  4.5 \n 3 1250041     35.8      140.     2    NA    NA   2980      0 NA   \n 4 1920904     35.7      139.     1    NA    NA   2000      0 NA   \n 5 1500042     35.7      140.     7    NA    NA    380      0 NA   \n 6 1000013     35.7      140.     3    NA    NA   2980      0 NA   \n 7 1760006     35.7      140.     2    NA    NA    850      0 NA   \n 8 1010021     35.7      140.     4    NA    NA   2000      3  3.33\n 9 1000006     35.7      140.     1    NA    NA   1000      2  2.5 \n10 1140002     35.8      140.     2    NA    NA   2000      0 NA   \n# … with 6,282 more rows\n\n\n　!を使って条件に合致する列を除外することも可能です。もし、character型の列を除外する場合は以下のように!where(is.character)を指定します。\n\n# character型でない列を抽出する\ndf %>%\n  select(!where(is.character))\n\n# A tibble: 6,292 × 9\n   Zipcode Latitude Longitude  Walk   Bus   Car Budget ScoreN Score\n     <dbl>    <dbl>     <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n 1 1040031     35.7      140.     3    NA    NA   3000      0 NA   \n 2 1100005     35.7      140.     1    NA    NA   2000      2  4.5 \n 3 1250041     35.8      140.     2    NA    NA   2980      0 NA   \n 4 1920904     35.7      139.     1    NA    NA   2000      0 NA   \n 5 1500042     35.7      140.     7    NA    NA    380      0 NA   \n 6 1000013     35.7      140.     3    NA    NA   2980      0 NA   \n 7 1760006     35.7      140.     2    NA    NA    850      0 NA   \n 8 1010021     35.7      140.     4    NA    NA   2000      3  3.33\n 9 1000006     35.7      140.     1    NA    NA   1000      2  2.5 \n10 1140002     35.8      140.     2    NA    NA   2000      0 NA   \n# … with 6,282 more rows\n\n\n　&を使って複数の条件を使うことも可能です。たとえば、ID変数に加えて「\"L\"で始まる変数の中でnumeric型の列を抽出」するコードは以下のようになります。\n\n# IDと、Lで始まるnumeric型の列を抽出する\ndf %>%\n  select(ID, starts_with(\"L\") & where(is.numeric))\n\n# A tibble: 6,292 × 3\n   ID      Latitude Longitude\n   <chr>      <dbl>     <dbl>\n 1 e539604     35.7      140.\n 2 gfeb600     35.7      140.\n 3 ggt5900     35.8      140.\n 4 g181340     35.7      139.\n 5 ggww100     35.7      140.\n 6 gdzk500     35.7      140.\n 7 ga2g202     35.7      140.\n 8 gg9m100     35.7      140.\n 9 gdvk200     35.7      140.\n10 gggb200     35.8      140.\n# … with 6,282 more rows"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#行の抽出",
    "href": "tutorial/R/dplyr_intro.html#行の抽出",
    "title": "dplyr入門",
    "section": "行の抽出",
    "text": "行の抽出\n\n指定した行を抽出する\n　他にも特定の行を抽出する場合があります。たとえば、「dfの最初の5行」や「dfの8行目のケース」といった場合です。この操作には{dplyr}のslice_*()関数群が便利です。それではそれぞれの関数の使い方について紹介していきます。その前に、実習用データとしてdfから一部の列のみを抽出したselelct.dfを作成します。\n\nselect.df <- df %>% \n  select(ID, Name, Pref, Budget, Score)\n\nslice(): 指定した番号の行のみ抽出する\n　select.dfから2, 8, 9行目の行を抽出したいとします。このような簡単な操作はパッケージを使わず、以下のように抽出することができます。\n\n# select.dfから2, 8, 9行目の行を抽出し、出力する\nselect.df[c(2, 8, 9),]\n\n# A tibble: 3 × 5\n  ID      Name                                  Pref   Budget Score\n  <chr>   <chr>                                 <chr>   <dbl> <dbl>\n1 gfeb600 本格上海料理 新錦江 上野御徒町本店    東京都   2000  4.5 \n2 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店 東京都   2000  3.33\n3 gdvk200 中国料理 宝龍                         東京都   1000  2.5 \n\n\n　しかし、以下のslice()関数を使うとパイプ演算子を前後に付けることが可能であり2、コードの可読性も高いです。slice()関数には以下のように抽出したい行の番号を入れるだけです。\n\n# select.dfから2, 8, 9行目の行を抽出し、出力する\nselect.df %>% \n  slice(2, 8, 9) # slice(c(2, 8, 9))もOK\n\n# A tibble: 3 × 5\n  ID      Name                                  Pref   Budget Score\n  <chr>   <chr>                                 <chr>   <dbl> <dbl>\n1 gfeb600 本格上海料理 新錦江 上野御徒町本店    東京都   2000  4.5 \n2 gg9m100 宴会個室×餃子酒場 北京飯店 秋葉原本店 東京都   2000  3.33\n3 gdvk200 中国料理 宝龍                         東京都   1000  2.5 \n\n\n　slice(2, 8, 9)でもslice(c(2, 8, 9))でも構いません。また、隣接した行でしたら:を使うことも可能です。たとえば、10行目から15行目まで抽出する場合はslice(10:15)のような書き方も出来ます。\nslice_head(): 最初のn行を抽出する\n\n# select.dfから最初の3行抽出し、出力する\nselect.df %>% \n  slice_head(n = 3)\n\n# A tibble: 3 × 5\n  ID      Name                                       Pref   Budget Score\n  <chr>   <chr>                                      <chr>   <dbl> <dbl>\n1 e539604 居酒屋 龍記 京橋店                         東京都   3000  NA  \n2 gfeb600 本格上海料理 新錦江 上野御徒町本店         東京都   2000   4.5\n3 ggt5900 食べ飲み放題×中華ビストロ NOZOMI（のぞみ） 東京都   2980  NA  \n\n\n　これはhead(データ名, n = 出力する個数)と同じ動きをする関数です。注意点としては引数n =を必ず付ける点です。たとえば、slice_head(3)にすると、select.dfの3行目のみ抽出されます。\nslice_tail(): 最後のn行を抽出する\n\n# select.dfから最後の7行を抽出し、出力する\nselect.df %>% \n  slice_tail(n = 7)\n\n# A tibble: 7 × 5\n  ID      Name                    Pref     Budget Score\n  <chr>   <chr>                   <chr>     <dbl> <dbl>\n1 5508852 場鶴                    和歌山県     NA    NA\n2 7113351 来来亭 橋本店           和歌山県     NA    NA\n3 6364939 ばり馬 和歌山紀三井寺店 和歌山県     NA    NA\n4 7103349 ramen BIRDMAN           和歌山県     NA    NA\n5 7315303 薩摩ラーメン 斗天王     和歌山県     NA    NA\n6 7703472 まるしげ                和歌山県     NA    NA\n7 6395035 暴豚製麺所              和歌山県     NA    NA\n\n\n　これはtail(データ名, n = 出力する個数)と同じ動きをする関数です。ちなみに、このn引数もn =を明記する必要があります。\nslice_max(): 指定した変数が大きい順でn行抽出する\n　slice_max()は指定した変数が大きい順でn行抽出する関数です。たとえば、Budgetが高い順で4店舗を抽出する場合は以下のように書きます。\n\n# select.dfからScoreの値が高い順で5行を抽出し、出力する\nselect.df %>% \n  slice_max(Budget, n = 4)\n\n# A tibble: 4 × 5\n  ID      Name                                              Pref    Budget Score\n  <chr>   <chr>                                             <chr>    <dbl> <dbl>\n1 g670609 横浜ベイシェラトン ホテル＆タワーズ 中国料理 彩龍 神奈川…   8000    NA\n2 g910420 JASMINE 憶江南                                    東京都    7000    NA\n3 7176666 赤坂焼鳥 鳳                                       東京都    7000    NA\n4 b612800 羽衣 銀座本店                                     東京都    6000    NA\n\n\nslice_min(): 指定した変数が小さい順でn行抽出する\n　一方、slice_min()関数が小さい順で抽出します。\n\n# select.dfからScoreの値が低い順で3行を抽出し、出力する\nselect.df %>% \n  slice_min(Score, n = 3)\n\n# A tibble: 4 × 5\n  ID      Name                        Pref   Budget Score\n  <chr>   <chr>                       <chr>   <dbl> <dbl>\n1 6384909 葛西大勝軒                  東京都     NA     1\n2 6929243 由丸 アトレヴィ大塚店       東京都     NA     1\n3 5816075 ラーメン戯拉戯拉            千葉県     NA     1\n4 5495086 らあめん花月嵐 坂戸わかば店 埼玉県     NA     1\n\n\n　ただし、n = 3と指定したはずなのに、4行が抽出されました。これは同点のケースがあるからです。実際、select.dfにはScoreが1のケースが4つあります。もし、同点の存在によりnに収まらない場合、slice_max()、slice_min()関数はnを超える行を出力します。これを強制的にn行に合わせるためにはwith_ties = FALSE引数を付けます。この場合、データで格納されている順でn個のみ出力されます。\n\nselect.df %>% \n  slice_min(Score, n = 3, with_ties = FALSE)\n\n# A tibble: 3 × 5\n  ID      Name                  Pref   Budget Score\n  <chr>   <chr>                 <chr>   <dbl> <dbl>\n1 6384909 葛西大勝軒            東京都     NA     1\n2 6929243 由丸 アトレヴィ大塚店 東京都     NA     1\n3 5816075 ラーメン戯拉戯拉      千葉県     NA     1\n\n\nslice_sample(): 無作為にn行を抽出する\n　最後に無作為にn行を抽出するslice_sample()関数です。引数はnであり、抽出したい行数を指定します。たとえば、select.dfから無作為に10行抽出したい場合は、\n\n# select.dfから無作為に5行を抽出し、出力する\nselect.df %>% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n   ID      Name                                 Pref   Budget Score\n   <chr>   <chr>                                <chr>   <dbl> <dbl>\n 1 6514273 てらッちょ柏店                       千葉県     NA  NA  \n 2 7648761 中華そば あおい                      埼玉県     NA  NA  \n 3 5495849 台方ラーメン甚兵衛渡し               千葉県     NA  NA  \n 4 7685211 らーめん工房 えびそば屋 東大阪吉田店 大阪府     NA  NA  \n 5 5553355 ラーメンまこと屋 イオンタウン佐用店  兵庫県     NA  NA  \n 6 7698760 極厚焼豚らーめん天翔                 大阪府    850  NA  \n 7 7055835 AFURI 六本木交差点                   東京都     NA  NA  \n 8 7207113 麺屋Ryoma 石橋本店                   大阪府     NA  NA  \n 9 7104601 大治飯店                             東京都     NA  NA  \n10 7356411 らーめんZoot                         東京都   1000   3.5\n\n\n　のように書きます。ブートストラップ法や機械学習における交差検証 (cross-validation)の際に有用な関数ですが、ブートストラップや機械学習のパッケージの多くはサンプル分割の関数を提供しているため、あまり使う機会はないでしょう。また、slice_sample()関数をブートストラップ法のために用いる場合は、ケースを反復抽出する必要があり、replace = TRUEを付けると反復抽出を行います。デフォルト値はFALSEです。\n\n\n条件に合致する行を抽出する\n　これまで見てきたslice()を用いる行の抽出は、実際あまり使う機会がありません。多くの場合、「何かの条件と合致するケースのみ抽出する」または、「何かの条件と合致しないケースのみを抽出する」やこれらの組み合わせで行の抽出を行います。そこで登場するのがdplyr()パッケージのfilter()関数です。filter()関数の使い方は以下の通りです。\n# dplyr::filter()の使い方\nfilter(データフレーム/tibble名, 条件1, 条件2, ...)\n　むろん、第一引数がデータですから、%>%を使うことも可能です。\n# dplyr::filter()の使い方 (パイプを使う方法)\nデータフレーム/tibble名 %>%\n  filter(条件1, 条件2, ...)\n　まずは、条件が一つの場合を考えてみましょう。ここでは「Prefが\"京都府\"であるケースのみに絞り、NameとStation、Score列のみを出力する」ケースを考えてみましょう。まず、filter()関数で行を抽出し、続いてselect()関数で抽出する列を指定します。むろん、今回の場合、filter()とselect()の順番は替えても構いません。\n\n# dfからPrefが\"京都府\"であるケースのみ残し、df2という名で保存\ndf2 <- df %>%\n  filter(Pref == \"京都府\")\n\n# df2からName, Station, Score列を抽出\ndf2 %>%\n  select(Name, Station, Score)\n\n# A tibble: 414 × 3\n   Name                                                    Station    Score\n   <chr>                                                   <chr>      <dbl>\n 1 中国料理 鳳麟                                           くいな橋駅 NA   \n 2 黒毛和牛一頭買い焼肉と 炊き立て土鍋ご飯 市場小路 烏丸店 四条駅      3.19\n 3 京の中華 ハマムラ みやこみち店                          京都駅     NA   \n 4 焼肉処 真 桂店                                          桂駅       NA   \n 5 祇園京都ラーメン                                        祇園四条駅 NA   \n 6 創作料理 串カツ トンカツ jiro                           新田辺駅   NA   \n 7 祇園 晩餐のあと                                         祇園四条駅 NA   \n 8 DETAIL                                                  東山駅     NA   \n 9 めんや龍神                                              北大路駅   NA   \n10 無尽蔵 京都八条家                                       京都駅      3.5 \n# … with 404 more rows\n\n\n　これはdfからPref == \"京都府\"のケースのみ残したものをdf2として格納し、それをまたselect()関数を使って列を抽出するコードです。これでも問題ありませんが、これだとパイプ演算子の便利さが分かりません。パイプ演算子は複数使うことが可能です。\n\ndf %>%\n  filter(Pref == \"京都府\") %>%\n  select(Name, Station, Score)\n\n# A tibble: 414 × 3\n   Name                                                    Station    Score\n   <chr>                                                   <chr>      <dbl>\n 1 中国料理 鳳麟                                           くいな橋駅 NA   \n 2 黒毛和牛一頭買い焼肉と 炊き立て土鍋ご飯 市場小路 烏丸店 四条駅      3.19\n 3 京の中華 ハマムラ みやこみち店                          京都駅     NA   \n 4 焼肉処 真 桂店                                          桂駅       NA   \n 5 祇園京都ラーメン                                        祇園四条駅 NA   \n 6 創作料理 串カツ トンカツ jiro                           新田辺駅   NA   \n 7 祇園 晩餐のあと                                         祇園四条駅 NA   \n 8 DETAIL                                                  東山駅     NA   \n 9 めんや龍神                                              北大路駅   NA   \n10 無尽蔵 京都八条家                                       京都駅      3.5 \n# … with 404 more rows\n\n\n　全く同じ結果ですが、無駄にdf2というデータフレーム (または、tibble)を作らず済むので、メモリの観点からも嬉しいですし、何よりコードが短く、しかも可読性も上がりました。\n　今回は==を使って合致するものに絞りましたが、!=を使って合致しないものに絞ることも可能です。または、比較演算子 (<、>、>=、<=など)を使うことも可能です。それでは、組み込み数 (ScoreN)が0ではないケースを取り出し、Name、Station、ScoreN、Score列を出力させてみましょう。\n\ndf %>%\n  filter(ScoreN != 0) %>%\n  select(Name, Station, starts_with(\"Score\"))\n\n# A tibble: 1,344 × 4\n   Name                                              Station       ScoreN Score\n   <chr>                                             <chr>          <dbl> <dbl>\n 1 本格上海料理 新錦江 上野御徒町本店                仲御徒町駅         2  4.5 \n 2 宴会個室×餃子酒場 北京飯店 秋葉原本店             秋葉原駅           3  3.33\n 3 中国料理 宝龍                                     有楽町駅           2  2.5 \n 4 麺達 うま家                                       高田馬場駅         2  3   \n 5 刀削麺・火鍋・西安料理 XI’AN（シーアン） 後楽園店 後楽園駅           1 NA   \n 6 七志らーめん 渋谷道玄坂店                         渋谷駅             7  4.5 \n 7 永楽                                              京成小岩駅         6  4.42\n 8 よってこや お台場店                               お台場海浜公…      1  4   \n 9 ラーメン武藤製麺所                                竹ノ塚駅           4  3.5 \n10 桂花ラーメン 新宿末広店                           新宿三丁目駅       8  3   \n# … with 1,334 more rows\n\n\n　これで口コミ数が1以上の店舗のみに絞ることができました。ただし、店によっては口コミはあっても、評価 (Score)が付いていないところもあります。たとえば、「刀削麺・火鍋・西安料理 XI’AN（シーアン） 後楽園店」の場合、口コミはありますが、評価はありません。したがって、今回は評価が付いている店舗に絞ってみましょう。\n\ndf %>%\n  filter(Score != NA) %>%\n  select(Name, Station, starts_with(\"Score\"))\n\n# A tibble: 0 × 4\n# … with 4 variables: Name <chr>, Station <chr>, ScoreN <dbl>, Score <dbl>\n\n\n　あらら、何の結果も表示されませんでした。これはfilter()内の条件に合致するケースが存在しないことを意味します。しかし、先ほどの結果を見ても、評価が付いている店はいっぱいありましたね。これはなぜでしょう。\n　察しの良い読者さんは気づいているかと思いますが、NAか否かを判定する際は==や!=は使えません。is.na()を使います。filter(is.na(Score))なら「ScoreがNAであるケースに絞る」ことを意味しますが、今回は「ScoreがNAでないケースに絞る」ことが目的ですので、is.na()の前に!を付けます。\n\ndf %>%\n  filter(!is.na(Score)) %>%\n  select(Name, Station, starts_with(\"Score\"))\n\n# A tibble: 1,134 × 4\n   Name                                  Station          ScoreN Score\n   <chr>                                 <chr>             <dbl> <dbl>\n 1 本格上海料理 新錦江 上野御徒町本店    仲御徒町駅            2  4.5 \n 2 宴会個室×餃子酒場 北京飯店 秋葉原本店 秋葉原駅              3  3.33\n 3 中国料理 宝龍                         有楽町駅              2  2.5 \n 4 麺達 うま家                           高田馬場駅            2  3   \n 5 七志らーめん 渋谷道玄坂店             渋谷駅                7  4.5 \n 6 永楽                                  京成小岩駅            6  4.42\n 7 よってこや お台場店                   お台場海浜公園駅      1  4   \n 8 ラーメン武藤製麺所                    竹ノ塚駅              4  3.5 \n 9 桂花ラーメン 新宿末広店               新宿三丁目駅          8  3   \n10 北斗 新橋店                           新橋駅                4  2.5 \n# … with 1,124 more rows\n\n\n　これで口コミ評価が登録された店舗に絞ることができました。\n　続いて、複数の条件を持つケースを考えてみましょう。例えば、「京都府内の店舗で、口コミ評価が3.5以上の店舗」を出力したい場合、以下のようなコードとなります。\n\ndf %>%\n  filter(Pref == \"京都府\", Score >= 3.5) %>%\n  select(Name, Station, ScoreN, Score)\n\n# A tibble: 53 × 4\n   Name               Station    ScoreN Score\n   <chr>              <chr>       <dbl> <dbl>\n 1 無尽蔵 京都八条家  京都駅          2  3.5 \n 2 一蘭 京都河原町店  河原町駅        2  3.75\n 3 ミスター・ギョーザ 西大路駅        8  4.06\n 4 一蘭 京都八幡店    樟葉駅          3  4   \n 5 中華料理 清華園    京都駅          3  5   \n 6 まがり             <NA>            2  4   \n 7 魁力屋 北山店      北大路駅        2  4.25\n 8 大中BAL横店        <NA>            7  4.1 \n 9 こうちゃん         西舞鶴駅        1  5   \n10 大黒ラーメン       伏見桃山駅      4  4.25\n# … with 43 more rows\n\n\n　条件をfilter()内に追加するだけです。今回は!is.na(Score)は不要です。なぜなら、Score >= 3.5という条件で既に欠損値は対象外になるからです。条件文が複数ある場合、ANDかORかを指定する必要があります。つまり、条件文AとBがある場合、「AとB両方満たすものを出力する」か「AとBどちらかを満たすものを出力するか」を指定する必要があります。今の結果ってANDでしたよね。filter()関数は、別途の指定がない場合、全てAND扱いになります。RのAND演算子は&ですので、以上のコードは以下のコードと同じです。\n\ndf %>%\n  filter(Pref == \"京都府\" & Score >= 3.5) %>%\n  select(Name, Station, ScoreN, Score)\n\n# A tibble: 53 × 4\n   Name               Station    ScoreN Score\n   <chr>              <chr>       <dbl> <dbl>\n 1 無尽蔵 京都八条家  京都駅          2  3.5 \n 2 一蘭 京都河原町店  河原町駅        2  3.75\n 3 ミスター・ギョーザ 西大路駅        8  4.06\n 4 一蘭 京都八幡店    樟葉駅          3  4   \n 5 中華料理 清華園    京都駅          3  5   \n 6 まがり             <NA>            2  4   \n 7 魁力屋 北山店      北大路駅        2  4.25\n 8 大中BAL横店        <NA>            7  4.1 \n 9 こうちゃん         西舞鶴駅        1  5   \n10 大黒ラーメン       伏見桃山駅      4  4.25\n# … with 43 more rows\n\n\n　AND演算子 (&)が使えるということはOR演算子 (|)も使えることを意味します。たとえば、Stationが\"高田馬場駅\"か\"三田駅\"の条件を指定したい場合、\n\ndf %>% \n  filter(Station == \"高田馬場駅\" | Station == \"三田駅\") %>%\n  select(Name, Station, ScoreN, Score)\n\n# A tibble: 14 × 4\n   Name                                 Station    ScoreN Score\n   <chr>                                <chr>       <dbl> <dbl>\n 1 麺達 うま家                          高田馬場駅      2  3   \n 2 らぁ麺 やまぐち                      高田馬場駅      7  4.08\n 3 博多一瑞亭 三田店                    三田駅          0 NA   \n 4 つけ麺屋 ひまわり                    高田馬場駅      4  2.75\n 5 石器ラーメン 高田馬場                高田馬場駅      0 NA   \n 6 旨辛らーめん 表裏                    高田馬場駅      0 NA   \n 7 三歩一                               高田馬場駅      8  4.56\n 8 えぞ菊 戸塚店                        高田馬場駅      4  3.62\n 9 麺屋　宗                             高田馬場駅      5  4.2 \n10 とんこつラーメン 博多風龍 高田馬場店 高田馬場駅      2  3   \n11 横浜家系ラーメン 馬場壱家            高田馬場駅      0 NA   \n12 らーめん よし丸                      高田馬場駅      1  5   \n13 札幌ラーメン どさん子 三田店         三田駅          0 NA   \n14 天下一品 三田店                      三田駅          0 NA   \n\n\n　のように書きます（ちなみに高田馬場の「やまぐち」は本当に美味しいです）。むろん、複数の変数を用いたORも可能です。たとえば、「Prefが\"京都府\"かScoreが3以上」のような条件も可能ですが (Pref == \"京都府\" | Score >= 3)、実際、このような例はあまりありません。よく使うのは「変数Xがaかbかcか」のような例です。ただし、この場合は|を使わないもっと簡単な方法があります。それは%in%演算子です。以下のコードは上のコードと同じものです。\n\ndf %>% \n  filter(Station %in% c(\"高田馬場駅\", \"三田駅\")) %>%\n  select(Name, Station, ScoreN, Score)\n\n# A tibble: 14 × 4\n   Name                                 Station    ScoreN Score\n   <chr>                                <chr>       <dbl> <dbl>\n 1 麺達 うま家                          高田馬場駅      2  3   \n 2 らぁ麺 やまぐち                      高田馬場駅      7  4.08\n 3 博多一瑞亭 三田店                    三田駅          0 NA   \n 4 つけ麺屋 ひまわり                    高田馬場駅      4  2.75\n 5 石器ラーメン 高田馬場                高田馬場駅      0 NA   \n 6 旨辛らーめん 表裏                    高田馬場駅      0 NA   \n 7 三歩一                               高田馬場駅      8  4.56\n 8 えぞ菊 戸塚店                        高田馬場駅      4  3.62\n 9 麺屋　宗                             高田馬場駅      5  4.2 \n10 とんこつラーメン 博多風龍 高田馬場店 高田馬場駅      2  3   \n11 横浜家系ラーメン 馬場壱家            高田馬場駅      0 NA   \n12 らーめん よし丸                      高田馬場駅      1  5   \n13 札幌ラーメン どさん子 三田店         三田駅          0 NA   \n14 天下一品 三田店                      三田駅          0 NA   \n\n\n　結局、|が使われるケースがかなり限定されます。あるとすれば、「変数Xがa以下か、b以上か」のようなケースですね。ただし、&と|を同時に使うケースは考えられます。たとえば、大阪駅と京都駅周辺のうまいラーメン屋を調べるとします。問題は美味しさの基準ですが、3.5点以上としましょう。ただし、京都府民はラーメンに非常に厳しく、3点以上なら美味しいと仮定します。この場合、「(Stationが\"大阪駅\"かつScore >= 3.5)、または(Stationが\"京都駅\"かつScore >= 3)」のような条件が必要になります。()は「()の中から判定せよ」という、普通の算数での使い方と同じです。それでは、実際に検索してみましょう。\n\ndf %>%\n  filter((Station == \"大阪駅\" & Score >= 3.5) | (Station == \"京都駅\" & Score >= 3)) %>%\n  select(Name, Station, Walk, ScoreN, Score)\n\n# A tibble: 6 × 5\n  Name                      Station  Walk ScoreN Score\n  <chr>                     <chr>   <dbl>  <dbl> <dbl>\n1 Lei can ting 大阪ルクア店 大阪駅      3      3  4   \n2 神座 ルクア大阪店         大阪駅      1     10  3.94\n3 みつか坊主 醸             大阪駅     10      4  5   \n4 無尽蔵 京都八条家         京都駅      5      2  3.5 \n5 中華料理 清華園           京都駅     10      3  5   \n6 ますたに 京都拉麺小路店   京都駅      9      3  3.67\n\n\n　Songが大好きな神座がヒットして嬉しいです。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#行のソート",
    "href": "tutorial/R/dplyr_intro.html#行のソート",
    "title": "dplyr入門",
    "section": "行のソート",
    "text": "行のソート\n　続いて、行のソートについて解説します。「食べログ」などのレビューサービスを利用する場合、口コミ評価が高い順で見るのが一般的でしょう3。また、サッカーのランキングも多くは1位から下の順位で掲載されるのが一般的です。ここではこのようにある変数の値順に行を並び替える方法について説明します。\n　ソートには{dplyr}パッケージのarrange()関数を使います。引数は変数名のみです。たとえば、奈良県のラーメン屋を検索してみましょう。並び替える順は駅から近い店舗を上位に、遠い店舗を下位に並べます。このような順は昇順 (ascending)と呼ばれ、ランキング表などでよく見ます。駅から近い順にソートするので、まず最寄りの駅情報が欠損でないことが必要です。また、ラーメン屋の評価も気になるので口コミが1つ以上付いている店舗に絞りましょう。表示する列は店舗名、最寄りの駅、徒歩距離、口コミ数、点数です。\n\ndf %>%\n  filter(Pref == \"奈良県\", !is.na(Station), ScoreN > 0) %>%\n  select(Name, Station, Walk, ScoreN, Score) %>%\n  arrange(Walk) %>%\n  print(n = Inf)\n\n# A tibble: 24 × 5\n   Name                                  Station             Walk ScoreN Score\n   <chr>                                 <chr>              <dbl>  <dbl> <dbl>\n 1 麺屋 あまのじゃく 本店                富雄駅                 2      2  4.5 \n 2 紀州和歌山らーめんきぶんや 奈良富雄店 富雄駅                 4      1  4   \n 3 ラーメン家 みつ葉                     富雄駅                 4      1  3.5 \n 4 天下一品 新大宮店                     新大宮駅               6      1  3   \n 5 麺屋 一徳                             天理駅                 7      1  3   \n 6 丸源ラーメン 橿原店                   金橋駅                 8      1  3.5 \n 7 らーめん食堂 よってこや 平群店        元山上口駅            10      1  4   \n 8 天理スタミナラーメン本店              櫟本駅                11      2  3.25\n 9 博多長浜らーめん夢街道 奈良土橋店     真菅駅                11      1  3.5 \n10 ぶ～け                                奈良駅                11      1  5   \n11 つけめん らーめん元喜神 押熊店        学研奈良登美ヶ丘駅    12      4  4.12\n12 彩華ラーメン 本店                     前栽駅                12      5  3.6 \n13 力皇                                  天理駅                13      1  3.5 \n14 らーめん きみちゃん                   京終駅                14      2  4.5 \n15 無鉄砲がむしゃら                      帯解駅                15      2  4   \n16 彩華ラーメン 田原本店                 石見駅                15      1  4   \n17 神座 大和高田店                       大和高田駅            17      2  3.75\n18 彩華ラーメン 奈良店                   尼ヶ辻駅              17      3  4.33\n19 彩華ラーメン 桜井店                   大福駅                18      1  3   \n20 天下一品 東生駒店                     東生駒駅              19      1  3.5 \n21 まりお流ラーメン                      新大宮駅              20      1  5   \n22 どうとんぼり神座 奈良柏木店           西ノ京駅              22      1  3   \n23 河童ラーメン本舗 押熊店               学研奈良登美ヶ丘駅    28      1  4   \n24 博多長浜らーめん 夢街道 四条大路店    新大宮駅              29      4  2.88\n\n\n　3行まではこれまで習ってきたもので、4行目がソートの関数、arrange()です。引数はソートの基準となる変数で、今回は最寄りの駅からの徒歩距離を表すWalkです。5行目は省略可能ですが、tibbleクラスの場合、10行までしか出力されないので、print(n = Inf)で「すべての行を表示」させます。nを指定することで出力される行数が調整可能です。奈良県のラーメン屋の中で最寄りの駅から最も近い店は「麺屋 あまのじゃく 本店」で徒歩2分でした。京田辺店も駅から約2分ですし、近いですね。ちなみにSongはここの塩とんこつが好きです。世界一こってりなラーメンとも言われる「チョモランマ」で有名な「まりお流ラーメン」は新大宮駅から徒歩20分でかなり遠いことが分かります。\n　続いて、駅からの距離ではなく、評価が高い順にしてみましょう。評価が高いほど上に来るので、今回は昇順でなく、降順 (descending)でソートする必要があります。arrange()関数は基本的に、指定された変数を基準に昇順でソートします。降順にするためにはdesc()関数を更に用います。たとえば、arrange(desc(変数名))のようにです。それでは実際にやってみましょう。上のコードの4行目をarange(Walk)からarrange(desc(Score))にちょっと修正するだけです。\n\ndf %>%\n  filter(Pref == \"奈良県\", !is.na(Station), ScoreN > 0) %>%\n  select(Name, Station, Walk, ScoreN, Score) %>%\n  arrange(desc(Score)) %>%\n  print(n = Inf)\n\n# A tibble: 24 × 5\n   Name                                  Station             Walk ScoreN Score\n   <chr>                                 <chr>              <dbl>  <dbl> <dbl>\n 1 まりお流ラーメン                      新大宮駅              20      1  5   \n 2 ぶ～け                                奈良駅                11      1  5   \n 3 麺屋 あまのじゃく 本店                富雄駅                 2      2  4.5 \n 4 らーめん きみちゃん                   京終駅                14      2  4.5 \n 5 彩華ラーメン 奈良店                   尼ヶ辻駅              17      3  4.33\n 6 つけめん らーめん元喜神 押熊店        学研奈良登美ヶ丘駅    12      4  4.12\n 7 河童ラーメン本舗 押熊店               学研奈良登美ヶ丘駅    28      1  4   \n 8 無鉄砲がむしゃら                      帯解駅                15      2  4   \n 9 紀州和歌山らーめんきぶんや 奈良富雄店 富雄駅                 4      1  4   \n10 彩華ラーメン 田原本店                 石見駅                15      1  4   \n11 らーめん食堂 よってこや 平群店        元山上口駅            10      1  4   \n12 神座 大和高田店                       大和高田駅            17      2  3.75\n13 彩華ラーメン 本店                     前栽駅                12      5  3.6 \n14 天下一品 東生駒店                     東生駒駅              19      1  3.5 \n15 力皇                                  天理駅                13      1  3.5 \n16 博多長浜らーめん夢街道 奈良土橋店     真菅駅                11      1  3.5 \n17 ラーメン家 みつ葉                     富雄駅                 4      1  3.5 \n18 丸源ラーメン 橿原店                   金橋駅                 8      1  3.5 \n19 天理スタミナラーメン本店              櫟本駅                11      2  3.25\n20 麺屋 一徳                             天理駅                 7      1  3   \n21 どうとんぼり神座 奈良柏木店           西ノ京駅              22      1  3   \n22 彩華ラーメン 桜井店                   大福駅                18      1  3   \n23 天下一品 新大宮店                     新大宮駅               6      1  3   \n24 博多長浜らーめん 夢街道 四条大路店    新大宮駅              29      4  2.88\n\n\n　よく考えてみれば、「評価が同点の場合、どうなるの?」と疑問を抱く方がいるかも知れません。たとえば、7行目の「河童ラーメン本舗 押熊店」と8行目の「無鉄砲がむしゃら」はどれも評価が4点ですが、「河童ラーメン本舗 押熊店」が先に表示されます。そのこれは簡単です。同点の場合、データセット内で上に位置する行が先に表示されます。これを確認するにはwhich()関数を使います。()内に条件文を指定することで、この条件に合致する要素の位置を返します。もし、条件に合致するものが複数あった場合は全ての位置を返します4。\n\nwhich(df$Name == \"河童ラーメン本舗 押熊店\")\n\n[1] 6021\n\nwhich(df$Name == \"無鉄砲がむしゃら\")\n\n[1] 6040\n\n\n　データ内に「河童ラーメン本舗 押熊店」がより上に位置することが分かります。「もし同点なら口コミ評価数が多いところにしたい」場合はどうすれば良いでしょうか。これはarrange()内に変数名を足すだけで十分です。\n\ndf %>%\n  filter(Pref == \"奈良県\", !is.na(Station), ScoreN > 0) %>%\n  select(Name, Station, Walk, ScoreN, Score) %>%\n  arrange(desc(Score), desc(ScoreN)) %>%\n  print(n = Inf)\n\n# A tibble: 24 × 5\n   Name                                  Station             Walk ScoreN Score\n   <chr>                                 <chr>              <dbl>  <dbl> <dbl>\n 1 まりお流ラーメン                      新大宮駅              20      1  5   \n 2 ぶ～け                                奈良駅                11      1  5   \n 3 麺屋 あまのじゃく 本店                富雄駅                 2      2  4.5 \n 4 らーめん きみちゃん                   京終駅                14      2  4.5 \n 5 彩華ラーメン 奈良店                   尼ヶ辻駅              17      3  4.33\n 6 つけめん らーめん元喜神 押熊店        学研奈良登美ヶ丘駅    12      4  4.12\n 7 無鉄砲がむしゃら                      帯解駅                15      2  4   \n 8 河童ラーメン本舗 押熊店               学研奈良登美ヶ丘駅    28      1  4   \n 9 紀州和歌山らーめんきぶんや 奈良富雄店 富雄駅                 4      1  4   \n10 彩華ラーメン 田原本店                 石見駅                15      1  4   \n11 らーめん食堂 よってこや 平群店        元山上口駅            10      1  4   \n12 神座 大和高田店                       大和高田駅            17      2  3.75\n13 彩華ラーメン 本店                     前栽駅                12      5  3.6 \n14 天下一品 東生駒店                     東生駒駅              19      1  3.5 \n15 力皇                                  天理駅                13      1  3.5 \n16 博多長浜らーめん夢街道 奈良土橋店     真菅駅                11      1  3.5 \n17 ラーメン家 みつ葉                     富雄駅                 4      1  3.5 \n18 丸源ラーメン 橿原店                   金橋駅                 8      1  3.5 \n19 天理スタミナラーメン本店              櫟本駅                11      2  3.25\n20 麺屋 一徳                             天理駅                 7      1  3   \n21 どうとんぼり神座 奈良柏木店           西ノ京駅              22      1  3   \n22 彩華ラーメン 桜井店                   大福駅                18      1  3   \n23 天下一品 新大宮店                     新大宮駅               6      1  3   \n24 博多長浜らーめん 夢街道 四条大路店    新大宮駅              29      4  2.88\n\n\n　ソートの基準はarrange()内において先に指定された変数の順番となります。「口コミ評価も評価数も同じなら、駅から近いところにしたい」場合は変数が3つとなり、Score、ScoreN、Walkの順で入れます。\n\ndf %>%\n  filter(Pref == \"奈良県\", !is.na(Station), ScoreN > 0) %>%\n  select(Name, Station, Walk, ScoreN, Score) %>%\n  arrange(desc(Score), desc(ScoreN), Walk) %>%\n  print(n = Inf)\n\n# A tibble: 24 × 5\n   Name                                  Station             Walk ScoreN Score\n   <chr>                                 <chr>              <dbl>  <dbl> <dbl>\n 1 ぶ～け                                奈良駅                11      1  5   \n 2 まりお流ラーメン                      新大宮駅              20      1  5   \n 3 麺屋 あまのじゃく 本店                富雄駅                 2      2  4.5 \n 4 らーめん きみちゃん                   京終駅                14      2  4.5 \n 5 彩華ラーメン 奈良店                   尼ヶ辻駅              17      3  4.33\n 6 つけめん らーめん元喜神 押熊店        学研奈良登美ヶ丘駅    12      4  4.12\n 7 無鉄砲がむしゃら                      帯解駅                15      2  4   \n 8 紀州和歌山らーめんきぶんや 奈良富雄店 富雄駅                 4      1  4   \n 9 らーめん食堂 よってこや 平群店        元山上口駅            10      1  4   \n10 彩華ラーメン 田原本店                 石見駅                15      1  4   \n11 河童ラーメン本舗 押熊店               学研奈良登美ヶ丘駅    28      1  4   \n12 神座 大和高田店                       大和高田駅            17      2  3.75\n13 彩華ラーメン 本店                     前栽駅                12      5  3.6 \n14 ラーメン家 みつ葉                     富雄駅                 4      1  3.5 \n15 丸源ラーメン 橿原店                   金橋駅                 8      1  3.5 \n16 博多長浜らーめん夢街道 奈良土橋店     真菅駅                11      1  3.5 \n17 力皇                                  天理駅                13      1  3.5 \n18 天下一品 東生駒店                     東生駒駅              19      1  3.5 \n19 天理スタミナラーメン本店              櫟本駅                11      2  3.25\n20 天下一品 新大宮店                     新大宮駅               6      1  3   \n21 麺屋 一徳                             天理駅                 7      1  3   \n22 彩華ラーメン 桜井店                   大福駅                18      1  3   \n23 どうとんぼり神座 奈良柏木店           西ノ京駅              22      1  3   \n24 博多長浜らーめん 夢街道 四条大路店    新大宮駅              29      4  2.88"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#記述統計量の計算",
    "href": "tutorial/R/dplyr_intro.html#記述統計量の計算",
    "title": "dplyr入門",
    "section": "記述統計量の計算",
    "text": "記述統計量の計算\n\nsummarise()による記述統計量の計算\n　ある変数の平均値や標準偏差、最小値、最大値などの記述統計量 (要約統計量)を計算することも可能です。これはsummarize()またはsummarise()関数を使いますが、この関数は後で紹介するgroup_by()関数と組み合わせることで力を発揮します。ここではグルーピングを考えずに、全データの記述統計量を計算する方法を紹介します。\n　summarise()関数の使い方は以下の通りです。\n# summarise()関数の使い方\nデータフレーム/tibble名 %>%\n  summarise(新しい変数名 = 関数名(計算の対象となる変数名))\n　もし、Score変数の平均値を計算し、その結果をMeanという列にしたい場合は以下のようなコードになります。\n\ndf %>%\n  summarise(Mean = mean(Score))\n\n# A tibble: 1 × 1\n   Mean\n  <dbl>\n1    NA\n\n\n　ただし、mean()関数は欠損値が含まれるベクトルの場合、NAを返します。この場合方法は2つ考えられます。\n\nfilter()関数を使ってScoreが欠損しているケースを予め除去する。\nna.rm引数を指定し、欠損値を除去した平均値を求める。\n\n　ここでは2番目の方法を使います。\n\ndf %>%\n  summarise(Mean = mean(Score, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   Mean\n  <dbl>\n1  3.66\n\n\n　dfのScore変数の平均値はNAであることが分かります。また、summarise()関数は複数の記述統計量を同時に計算することも可能です。以下はScore変数の平均値、中央値、標準偏差、最小値、最大値、第一四分位点、第三四分位点を計算し、Score.Descという名のデータフレーム (または、tibble)に格納するコードです。\n\nScore.Desc <- df %>%\n  summarize(Mean   =     mean(Score,       na.rm = TRUE),  # 平均値\n            Median =   median(Score,       na.rm = TRUE),  # 中央値\n            SD     =       sd(Score,       na.rm = TRUE),  # 標準偏差\n            Min    =      min(Score,       na.rm = TRUE),  # 最小値\n            Max    =      max(Score,       na.rm = TRUE),  # 最大値\n            Q1     = quantile(Score, 0.25, na.rm = TRUE),  # 第一四分位点\n            Q3     = quantile(Score, 0.75, na.rm = TRUE))  # 第三四分位点\n\nScore.Desc\n\n# A tibble: 1 × 7\n   Mean Median    SD   Min   Max    Q1    Q3\n  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  3.66   3.58 0.719     1     5     3     4\n\n\n　むろん、複数の変数に対して記述統計量を計算することも可能です。たとえば、平均予算 (Budget)、口コミ数 (ScoreN)、口コミ評価 (Score)の平均値を求めるとしたら、\n\ndf %>%\n  summarize(Budget_Mean = mean(Budget, na.rm = TRUE), # 平均予算の平均値\n            SocreN_Mean = mean(ScoreN, na.rm = TRUE), # 口コミ数の平均値\n            Score_Mean  = mean(Score,  na.rm = TRUE)) # 評価の平均値\n\n# A tibble: 1 × 3\n  Budget_Mean SocreN_Mean Score_Mean\n        <dbl>       <dbl>      <dbl>\n1       1232.       0.537       3.66\n\n\nのように書きます。実はsummarise()はこれくらいで十分便利です。ただし、以上の操作はもっと簡単なコードに置換できます。ただし、ラムダ式など、やや高度な内容になるため、以下の内容は飛ばして、次の節 (グルーピング)を読んでいただいても構いません。\n　まずは、複数の変数に対して同じ記述統計量を求める例を考えてみましょう。たとえば、Budget、ScoreN、Scoreに対して平均値を求める例です。これはacross()関数を使うとよりコードが短くなります。まずはacross()関数の書き方から見ましょう。\n# across()の使い方\nデータフレーム/tibble名 %>%\n  summarise(across(変数名のベクトル, 記述統計を計算する関数名, 関数の引数))\n　変数名のベクトルは長さ1以上のベクトルです。たとえば、Budget、ScoreN、Scoreの場合c(Budget, ScoreN, Score)になります。これはdf内で隣接する変数ですからBudget:Scoreの書き方も使えます。また、where()やany_of()、starts_with()のような関数を使って変数を指定することも可能です。関数名はmeanやsdなどの関数名です。ここは関数名()でななく、関数名であることに注意してください。引数は前の関数に必要な引数です。引数を必要としない関数なら省略可能ですが、na.rm = TRUEなどの引数が必要な場合は指定する必要があります。それではBudget、ScoreN、Scoreの平均値を計算してみましょう。\n\ndf %>%\n  summarize(across(Budget:Score, mean, na.rm = TRUE))\n\n# A tibble: 1 × 3\n  Budget ScoreN Score\n   <dbl>  <dbl> <dbl>\n1  1232.  0.537  3.66\n\n\n　across()使わない場合、4行必要だったコードが2行になりました。変数が少ない場合はacross()を使わない方が、可読性が高くなる場合もあります。しかし、変数が多くなる場合、可読性がやや落ちてもacross()を使った方が効率的でしょう。\n　次は、ある変数に対して複数の記述統計量を計算したい場合について考えます。Budget、ScoreN、Score変数の第一四分位点と第三四分位点をacross()を使わずに計算すると家のような7行のコードになります。\n\ndf %>%\n  summarize(Budget_Q1 = quantile(Budget, 0.25, na.rm = TRUE),\n            Budget_Q3 = quantile(Budget, 0.75, na.rm = TRUE),\n            ScoreN_Q1 = quantile(ScoreN, 0.25, na.rm = TRUE),\n            ScoreN_Q3 = quantile(ScoreN, 0.75, na.rm = TRUE),\n            Score_Q1  = quantile(Score,  0.25, na.rm = TRUE),\n            Score_Q3  = quantile(Score,  0.75, na.rm = TRUE))\n\n# A tibble: 1 × 6\n  Budget_Q1 Budget_Q3 ScoreN_Q1 ScoreN_Q3 Score_Q1 Score_Q3\n      <dbl>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1       800      1000         0         0        3        4\n\n\n　この作業もacross()を使ってより短縮することができます。ここではラムダ式の知識が必要になります。ラムダ関数とは関数名を持たない無名関数 (anonymous functions)を意味しますが、詳細は割愛します。興味のある読者はWikipediaなどを参照してください。簡単にいうとその場で即席に関数を作成し、計算が終わったら破棄する関数です。ただ、Rは基本的にラムダ式を提供しているのではなく、{purrr}パッケージのラムダ式スタイルを使用します。まずは、書き方から確認します。\n# ラムダ式を用いたacross()の使い方\nデータフレーム/tibble名 %>%\n  summarise(across(変数名のベクトル, list(結果の変数名 = ラムダ式)))\n　先ほどの書き方と似ていますが、関数を複数書く必要があるため、今回は関数名をlist型にまとめます。そして、結果の変数名は結果として出力されるデータフレーム (または、tibble)の列名を指定する引数です。たとえば、Meanにすると結果は元の変数名1_Mean、元の変数名2_Mean…のように出力されます。そして、ラムダ式が実際の関数が入る箇所です。とりあえず今回はコードを走らせ、結果から確認してみましょう。\n\ndf %>%\n  summarize(across(Budget:Score, list(Q1 = ~quantile(.x, 0.25, na.rm = TRUE),\n                                      Q3 = ~quantile(.x, 0.75, na.rm = TRUE))))\n\n# A tibble: 1 × 6\n  Budget_Q1 Budget_Q3 ScoreN_Q1 ScoreN_Q3 Score_Q1 Score_Q3\n      <dbl>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1       800      1000         0         0        3        4\n\n\n　結果の列名がBudget_Q1、Budget_Q3、ScoreN_Q1…のようになり、それぞれの変数の第一四分位点と第三四分位点が出力されます。問題はラムダ式の方ですが、普通の関数に非常に近いことが分かります。across()内のラムダ式は~関数名(.x, その他の引数)のような書き方になります。関数名の前に~が付いていることに注意してください。分位数を求める関数はquantile()であり、quantile(ベクトル, 分位数)であり、必要に応じてna.rmを付けます。この分位数が0.25なら第一四分位点、0.5なら第二四分位点 (=中央値)、0.75なら第三四分位点になります。それではラムダ式~quantile(.x, 0.25, na.rm = TRUE)はどういう意味でしょうか。これは.xの箇所にBudgetやScoreN、Scoreが入ることを意味します。.xという書き方は決まりです。.yとか.Song-san-Daisukiなどはダメです。そして、0.25を付けることによって第一四分位点を出力するように指定します。また、Budget、ScoreN、Scoreに欠損値がある場合、無視するようにna.rm = TRUEを付けます。\n　ラムダ式を自分で定義する関数で表現すると、以下のようになります。\n# 以下の3つは同じ機能をする関数である\n\n# ラムダ式\n~quantile(.x, 0.25, na.rm = TRUE)\n\n# 一般的な関数の書き方1\n名無し関数 <- function(x) {\n  quantile(x, 0.25, na.rm = TRUE)\n}\n\n# 一般的な関数の書き方2\n名無し関数 <- function(x) quantile(x, 0.25, na.rm = TRUE)\n　この3つは全て同じですが、ラムダ式は関数名を持たず、その場で使い捨てる関数です。むろん、ラムダ式を使わずに事前に第一四分位点と第三四分位点を求める関数を予め作成し、ラムダ式の代わりに使うことも可能です。まずは第一四分位点と第三四分位点を求める自作関数FuncQ1とFuncQ2を作成します。\n\n# ラムダ式を使わない場合は事前に関数を定義しておく必要がある\nFuncQ1 <- function(x) {\n  quantile(x, 0.25, na.rm = TRUE)\n}\nFuncQ3 <- function(x) {\n  quantile(x, 0.75, na.rm = TRUE)\n}\n\n　後は先ほどのほぼ同じ書き方ですが、今回はラムダ式を使わないため関数名に~を付けず、関数名のみで十分です。\n\n# やっておくと、summarise()文は簡潔になる\ndf %>%\n  summarize(across(Budget:Score, list(Q1 = FuncQ1, Q3 = FuncQ3)))\n\n# A tibble: 1 × 6\n  Budget_Q1 Budget_Q3 ScoreN_Q1 ScoreN_Q3 Score_Q1 Score_Q3\n      <dbl>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1       800      1000         0         0        3        4\n\n\n　事前に関数を用意するのが面倒ですが、across()の中身はかなりスッキリしますね。もし、このような作業を何回も行うなら、ラムダ式を使わず、自作関数を用いることも可能です。ただし、自作関数であっても引数が2つ以上必要な場合はラムダ式を使います。\n\n\nsummarise()に使える便利な関数\n　以下の内容は後で説明するgroup_by()関数を使っているため、まだgroup_by()に馴染みのない読者はまずはここを読み飛ばし、グルーピングの節にお進みください。\nIQR(): 四分位範囲を求める\n　四分位範囲は第三四分位点から第一四分位点を引いた値であり、Rの内蔵関数であるIQR()を使えば便利です。この関数はmeanやsd()関数と同じ使い方となります。\n\ndf %>%\n  filter(!is.na(Walk)) %>% # 予め欠損したケースを除くと、後でna.rm = TRUEが不要\n  group_by(Pref) %>%\n  summarise(Mean    = mean(Walk),\n            SD      = sd(Walk),\n            IQR     = IQR(Walk),\n            N       = n(),\n            .groups = \"drop\") %>%\n  arrange(Mean)\n\n# A tibble: 9 × 5\n  Pref      Mean    SD   IQR     N\n  <chr>    <dbl> <dbl> <dbl> <int>\n1 東京都    4.29  4.49     4   919\n2 大阪府    5.92  6.08     6   932\n3 神奈川県  8.21  7.91    10   878\n4 京都府    8.38  6.95     9   339\n5 兵庫県    8.52  7.27    10   484\n6 奈良県   10.6   6.59    10   123\n7 千葉県   10.6   8.21    12   776\n8 埼玉県   11.6   8.99    14   817\n9 和歌山県 12.8   6.83     9   107\n\n\nfirst()、last()、nth(): n番目の要素を求める\n　稀なケースかも知れませんが、データ内、またはグループ内のn番目の行を抽出する時があります。たとえば、市区町村の情報が格納されているデータセットで、人口が大きい順でデータがソートされているとします。各都道府県ごとに最も人口が大きい市区町村のデータ、あるいは最も少ない市区町村のデータが必要な際、first()とlast()関数が有効です。\n　それでは各都道府県ごとに「最も駅から遠いラーメン屋」の店舗名と最寄りの駅からの徒歩距離を出力したいとします。まずは、徒歩距離のデータが欠損しているケースを除去し、データを徒歩距離順でソートします。これはfilter()とarrange()関数を使えば簡単です。続いて、group_by()を使って都府県単位でデータをグループ化します。最後にsummarise()関数内にlast()関数を使います。データは駅から近い順に鳴っているため、各都府県内の最後の行は駅から最も遠い店舗になるからです。\n\ndf %>%\n  filter(!is.na(Walk)) %>%\n  arrange(Walk) %>%\n  group_by(Pref) %>%\n  summarise(Farthest  = last(Name),\n            Distance  = last(Walk),\n            .groups   = \"drop\")\n\n# A tibble: 9 × 3\n  Pref     Farthest                           Distance\n  <chr>    <chr>                                 <dbl>\n1 京都府   熱烈らぁめん                             30\n2 兵庫県   濃厚醤油 中華そば いせや 玉津店          43\n3 千葉県   札幌ラーメン どさん子 佐原51号店         59\n4 和歌山県 中華そば まる乃                          30\n5 埼玉県   札幌ラーメン どさん子 小鹿野店          116\n6 大阪府   河童ラーメン本舗 岸和田店                38\n7 奈良県   博多長浜らーめん 夢街道 四条大路店       29\n8 東京都   てんがら 青梅新町店                      30\n9 神奈川県 札幌ラーメン どさん子 中津店             73\n\n\n　このlast()をfirst()に変えると、最寄りの駅から最も近い店舗情報が表示されます。また、「n番目の情報」が必要な際はnth()関数を使います。nth(Name, 2)に変えることで2番目の店舗名が抽出できます。\nn_distinct(): ユニーク値の個数を求める\n　n_distinct()は何種類の要素が含まれているかを計算する関数であり、length(unique())関数と同じ機能をします。たとえば、以下のmyVec1に対して何種類の要素があるかを確認してみましょう。\n\nmyVec1 <- c(\"A\", \"B\", \"B\", \"D\", \"A\", \"B\", \"D\", \"C\", \"A\")\n\nunique(myVec1)\n\n[1] \"A\" \"B\" \"D\" \"C\"\n\n\n　myVec1は\"A\"、\"B\"、\"D\"、\"C\"の要素で構成されていることが分かります。これがmyVec1のユニーク値 (unique values)です。そして、このユニーク値の個数を調べるためにlength()を使います。\n\nlength(unique(myVec1))\n\n[1] 4\n\n\n　これでmyVec1は4種類の値が存在することが分かります。これと全く同じ機能をする関数がn_distinct()です。\n\nn_distinct(myVec1)\n\n[1] 4\n\n\n　この関数をsummarise()に使うことで、都府県ごとに駅の個数が分かります。あるいは「東京都内の選挙区に、これまでの衆院選において何人の候補者が存在したか」も分かります。ここではdf内の都府県ごとに駅の個数を計算してみましょう。最後の駅数が多い順でソートします。\n\ndf %>%\n  filter(!is.na(Station)) %>% # 最寄りの駅が欠損しているケースを除去\n  group_by(Pref) %>%\n  summarise(N_Station = n_distinct(Station),\n            .groups   = \"drop\") %>%\n  arrange(desc(N_Station))\n\n# A tibble: 9 × 2\n  Pref     N_Station\n  <chr>        <int>\n1 東京都         368\n2 大阪府         341\n3 千葉県         241\n4 神奈川県       240\n5 兵庫県         199\n6 埼玉県         185\n7 京都府         123\n8 奈良県          52\n9 和歌山県        46\n\n\n　当たり前かも知れませんが、駅数が最も多いのは東京都で次が大阪府であることが分かります。\nany()、all(): 条件に合致するか否かを求める\n　any()とall()はベクトル内の全要素に対して条件に合致するか否かを判定する関数です。ただし、any()は一つの要素でも条件に合致すればTRUEを、全要素が合致しない場合FALSEを返します。一方、all()は全要素に対して条件を満たせばTRUE、一つでも満たさない要素があればFALSEを返します。以下はany()とall()の例です。\n\nmyVec1 <- c(1, 2, 3, 4, 5)\nmyVec2 <- c(1, 3, 5, 7, 11)\n\nany(myVec1 %% 2 == 0) # myVec1を2で割った場合、一つでも余りが0か\n\n[1] TRUE\n\nall(myVec1 %% 2 == 0) # myVec1を2で割った場合、全ての余りが0か\n\n[1] FALSE\n\nall(myVec2 %% 2 != 0) # myVec2を2で割った場合、全ての余りが0ではないか\n\n[1] TRUE\n\n\n　それでは実際にdfに対してany()とall()関数を使ってみましょう。一つ目は「ある都府県に最寄りの駅から徒歩60分以上の店舗が一つでもあるか」であり、二つ目は「ある都府県の店舗は全て最寄りの駅から徒歩30分以下か」です。それぞれの結果をOver60とWithin30という列で出力してみましょう。\n\ndf %>%\n  group_by(Pref) %>%\n  summarise(Over60   = any(Walk >= 60, na.rm = TRUE),\n            Within30 = all(Walk <= 30, na.rm = TRUE),\n            .groups  = \"drop\")\n\n# A tibble: 9 × 3\n  Pref     Over60 Within30\n  <chr>    <lgl>  <lgl>   \n1 京都府   FALSE  TRUE    \n2 兵庫県   FALSE  FALSE   \n3 千葉県   FALSE  FALSE   \n4 和歌山県 FALSE  TRUE    \n5 埼玉県   TRUE   FALSE   \n6 大阪府   FALSE  FALSE   \n7 奈良県   FALSE  TRUE    \n8 東京都   FALSE  TRUE    \n9 神奈川県 TRUE   FALSE   \n\n\n　埼玉県と神奈川県において、最寄りの駅から徒歩60以上の店がありました。また、京都府、東京都、奈良県、和歌山県の場合、全店舗が最寄りの駅から徒歩30分以下ということが分かります。当たり前ですがOver60がTRUEならWithin30は必ずFALSEになりますね。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#グルーピング",
    "href": "tutorial/R/dplyr_intro.html#グルーピング",
    "title": "dplyr入門",
    "section": "グルーピング",
    "text": "グルーピング\n\ngroup_by()によるグループ化\n　先ほどのsummarise()関数は確かに便利ですが、特段に便利とも言いにくいです。dfのScoreの平均値を計算するだけなら、summarise()関数を使わない方が楽です。\n\n# これまでのやり方\ndf %>%\n  summarise(Mean = mean(Score, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   Mean\n  <dbl>\n1  3.66\n\n# 普通にこれでええんちゃう?\nmean(df$Score, na.rm = TRUE)\n\n[1] 3.663457\n\n\n　しかし、これをグループごとに計算するならどうでしょう。たとえば、Scoreの平均値を都府県ごとに計算するとします。この場合、以下のようなコードになります。\n\nmean(df$Score[df$Pref == \"東京都\"],   na.rm = TRUE)\n\n[1] 3.674256\n\nmean(df$Score[df$Pref == \"神奈川県\"], na.rm = TRUE)\n\n[1] 3.533931\n\nmean(df$Score[df$Pref == \"千葉県\"],   na.rm = TRUE)\n\n[1] 3.715983\n\nmean(df$Score[df$Pref == \"埼玉県\"],   na.rm = TRUE)\n\n[1] 3.641573\n\nmean(df$Score[df$Pref == \"大阪府\"],   na.rm = TRUE)\n\n[1] 3.765194\n\nmean(df$Score[df$Pref == \"京都府\"],   na.rm = TRUE)\n\n[1] 3.684976\n\nmean(df$Score[df$Pref == \"兵庫県\"],   na.rm = TRUE)\n\n[1] 3.543936\n\nmean(df$Score[df$Pref == \"奈良県\"],   na.rm = TRUE)\n\n[1] 3.854762\n\nmean(df$Score[df$Pref == \"和歌山県\"], na.rm = TRUE)\n\n[1] 3.96999\n\n\n　変わったのはdf$Scoreがdf$Score[df$Pref == \"東京都\"]に変わっただけです。df$Prefが\"東京都\"であるか否かをTRUEとFALSEで判定し、これを基準にdf$Scoreを抽出する仕組みです。df$Scoreとdf$Prefは同じデータフレーム (または、tibble)ですから、このような書き方で問題ありません。\n　これだけでもかなり書くのが面倒ですが、これが47都道府県なら、あるいは200ヶ国ならかなり骨の折れる作業でしょう。ここで大活躍するのが{dplyr}パッケージのgroup_by()関数です。引数はグループ化する変数名だけです。先ほどの作業を{dplyr}を使うならPref変数でグループ化し、summarise()関数で平均値を求めるだけです。今回はScoreだけでなく、ScoreNの平均値も求めてみましょう。そして、評価が高い順にソートもしてみます。\n\n# ScoreNとScoreの平均値をPrefごとに求める\ndf %>%\n  group_by(Pref) %>%\n  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE)) %>%\n  arrange(desc(Score_Mean))\n\n# A tibble: 9 × 3\n  Pref     ScoreN_Mean Score_Mean\n  <chr>          <dbl>      <dbl>\n1 和歌山県       0.593       3.97\n2 奈良県         0.306       3.85\n3 大阪府         0.516       3.77\n4 千葉県         0.259       3.72\n5 京都府         0.522       3.68\n6 東京都         1.16        3.67\n7 埼玉県         0.278       3.64\n8 兵庫県         0.389       3.54\n9 神奈川県       0.587       3.53\n\n\n　評判が最も高い都府県は和歌山県、最も低いのは神奈川県ですね。Songも和歌山ラーメンは井出系も車庫前系も好きです。しかし、大事なのは「井出系」と「車庫前系」といった分類が正しいかどうかではありません。コードが非常に簡潔となり、ソートなども自由自在であることです。都府県ごとにScoreNとScoreの平均値を求める場合、dplyr()を使わなかったら18行のコードとなり、ソートも自分でやる必要があります。一方、group_by()関数を使うことによってコードが5行になりました。\n　また、これは2020年6月に公開された{dplyr}1.0.0からの問題ですが、group_by()の後にsummarise()を使うと以下のようなメッセージが出力されます。\n## `summarise()` ungrouping output (override with `.groups` argument)\n　これはgroup_by()で指定された変数のグループ化が自動的に解除されたことを意味します。なぜならsummarise()をする際はPrefをグループ変数として使いましたが、出力された結果のPref変数はもはやグループとして機能できなくなるからです。元のdfにはPrefが\"東京都\"だったケースが1000行、\"京都府\"だったのが414行あったので、Pref変数でグループ化する意味がありました。しかし、summarise()から得られたデータフレーム (または、tibble)はPref == \"東京都\"の行が1つしかありません。これはグループ化する意味がなくなったことを意味します。したがって、自動的にグループを解除してくれます。自動的にやってくれるのはありがたいことですが、可能ならば関数内に自分で明記することが推奨されます。そこで使う引数が.groupsであり、\"drop\"を指定すると全てのグループ化変数を解除します。以下のようなコードだと先ほどのメッセージが表示されません。今後、意識的に入れるようにしましょう。\n\n# ScoreNとScoreの平均値をPrefごとに求める\ndf %>%\n  group_by(Pref) %>%\n  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE),\n            .groups     = \"drop\") %>%\n  arrange(desc(Score_Mean))\n\n# A tibble: 9 × 3\n  Pref     ScoreN_Mean Score_Mean\n  <chr>          <dbl>      <dbl>\n1 和歌山県       0.593       3.97\n2 奈良県         0.306       3.85\n3 大阪府         0.516       3.77\n4 千葉県         0.259       3.72\n5 京都府         0.522       3.68\n6 東京都         1.16        3.67\n7 埼玉県         0.278       3.64\n8 兵庫県         0.389       3.54\n9 神奈川県       0.587       3.53\n\n\n　続いて、一つ便利な関数を紹介します。それはグループのサイズを計算する関数、n()です。この関数をsummarise()内に使うと、各グループに属するケース数を出力します。先ほどのコードを修正し、各グループのサイズをNという名の列として追加してみましょう。そしてソートの順番はNを最優先とし、同じ場合はScore_Meanが高い方を上に出力させます。また、ScoreN_Meanの前に、口コミ数の合計も出してみましょう。\n\n# Prefごとに口コミ数の合計、口コミ数の平均値、評価の平均値、店舗数を求める\n# 店舗数-評価の平均値順でソートする\ndf %>%\n  group_by(Pref) %>%\n  summarise(ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),\n            ScoreN_Mean = mean(ScoreN, na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE),\n            N           = n(),\n            .groups     = \"drop\") %>%\n  arrange(desc(N), desc(Score_Mean))\n\n# A tibble: 9 × 5\n  Pref     ScoreN_Sum ScoreN_Mean Score_Mean     N\n  <chr>         <dbl>       <dbl>      <dbl> <int>\n1 大阪府          516       0.516       3.77  1000\n2 千葉県          259       0.259       3.72  1000\n3 東京都         1165       1.16        3.67  1000\n4 埼玉県          278       0.278       3.64  1000\n5 神奈川県        587       0.587       3.53  1000\n6 兵庫県          230       0.389       3.54   591\n7 京都府          216       0.522       3.68   414\n8 奈良県           45       0.306       3.85   147\n9 和歌山県         83       0.593       3.97   140\n\n\n　記述統計をグループごとに求めるのは普通にあり得るケースですし、実験データの場合はほぼ必須の作業でう。統制群と処置群間においてグループサイズが均一か、共変量のバラツキが十分に小さいかなどを判断する際にgroup_by()とsummarise()関数の組み合わせは非常に便利です。\n\n\n複数の変数を用いたグループ化\n　グループ化変数は2つ以上指定することも可能です。たとえば、都府県 (Pref)と最寄りの駅の路線 (Line)でグループ化することも可能です。それではPrefとLineでグループ化し、店舗数と口コミ数、評価の平均値を計算し、ソートの順番は店舗数、店舗数が同じなら評価の平均値が高い順にしましょう。今回もsummarise()内に.group = \"drop\"を指定し、グループ化を解除します。今回はTop 20まで出してみましょう。\n\n# ScoreNとScoreの平均値をPrefごとに求める\ndf %>%\n  filter(!is.na(Line)) %>% # Lineが欠損していないケースのみ残す\n  group_by(Pref, Line) %>% # PrefとLineでグループ化\n  summarise(N           = n(),\n            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE),\n            .groups     = \"drop\") %>%\n  arrange(desc(N), desc(Score_Mean)) %>%\n  print(n = 20)\n\n# A tibble: 523 × 5\n   Pref     Line                        N ScoreN_Sum Score_Mean\n   <chr>    <chr>                   <int>      <dbl>      <dbl>\n 1 埼玉県   東武東上線                122         27       3.68\n 2 東京都   ＪＲ                      104        231       3.56\n 3 神奈川県 小田急小田原線             96         31       3.59\n 4 埼玉県   東武伊勢崎線               96         18       3.51\n 5 神奈川県 横浜市営ブルーライン       82         77       3.66\n 6 千葉県   京成本線                   82         29       3.34\n 7 神奈川県 京急本線                   68         40       3.33\n 8 千葉県   東武野田線                 63          2       4.75\n 9 神奈川県 小田急江ノ島線             62          8       3.79\n10 大阪府   阪急京都本線               53         32       3.67\n11 大阪府   南海本線                   52         11       4.22\n12 兵庫県   阪神本線                   52         23       3.80\n13 埼玉県   JR高崎線                   51          5       4   \n14 兵庫県   山陽電鉄本線               51         15       2.98\n15 千葉県   JR総武本線（東京-銚子）    47          8       4   \n16 埼玉県   西武新宿線                 45          8       4.17\n17 埼玉県   秩父鉄道線                 43         10       3.82\n18 大阪府   京阪本線                   43         10       3.69\n19 千葉県   新京成電鉄                 43          6       3.6 \n20 京都府   阪急京都本線               43         27       3.5 \n# … with 503 more rows\n\n\n　ぐるなびに登録されているラーメン屋が最も多い路線は埼玉県内の東武東上線で122店舗があります。東武東上線は東京都と埼玉県をまたがる路線ですので、東武東上線だけならもっと多いかも知れませんね。\n　ここで一つ考えたいのはsummarise()内の.groups引数です。前回はグループ化に使った変数ごとに1行しか残っていなかったのでグループ化を全て解除しました。しかし、今回は状況がやや異なります。グループ化変数に使ったPrefを考えると、まだPref == \"東京都\"であるケースがいくつかあります。やろうとすればまだグループ化出来る状態です。これはLineについても同じです。Line == \"東武東上線\"の行はここには表示されていないものの、まだデータに残っています。もし、これ以上グループ化しないなら今のように.groups = \"drop\"が正しいですが、もしもう一回グループ化したい場合はどうすればよいでしょうか。方法は2つ考えられます。\n\nもう一度パイプ演算子を使ってgroup_by()関数を使う (以下の9行目)。\n\n結果を見ると## # Groups:   Pref, Line [523]で、ちゃんとグループ化されていることが分かります。\n\n\n\ndf %>%\n  filter(!is.na(Line)) %>% \n  group_by(Pref, Line) %>% \n  summarise(N           = n(),\n            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE),\n            .groups     = \"drop\") %>%\n  arrange(desc(N), desc(Score_Mean)) %>%\n  group_by(Pref, Line) %>% # group_by()、もう一度\n  print(n = 5)\n\n# A tibble: 523 × 5\n# Groups:   Pref, Line [523]\n  Pref     Line                     N ScoreN_Sum Score_Mean\n  <chr>    <chr>                <int>      <dbl>      <dbl>\n1 埼玉県   東武東上線             122         27       3.68\n2 東京都   ＪＲ                   104        231       3.56\n3 神奈川県 小田急小田原線          96         31       3.59\n4 埼玉県   東武伊勢崎線            96         18       3.51\n5 神奈川県 横浜市営ブルーライン    82         77       3.66\n# … with 518 more rows\n\n\n\n.groups引数を何とかする。\n\n　推奨される方法は2番です。具体的には.groups = \"keep\"を指定するだけであり、こっちの方が無駄なコードを省けることができます。\n\ndf %>%\n  filter(!is.na(Line)) %>% \n  group_by(Pref, Line) %>% \n  summarise(N           = n(),\n            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),\n            Score_Mean  = mean(Score,  na.rm = TRUE),\n            .groups     = \"keep\") %>%\n  arrange(desc(N), desc(Score_Mean)) %>%\n  print(n = 5)\n\n# A tibble: 523 × 5\n# Groups:   Pref, Line [523]\n  Pref     Line                     N ScoreN_Sum Score_Mean\n  <chr>    <chr>                <int>      <dbl>      <dbl>\n1 埼玉県   東武東上線             122         27       3.68\n2 東京都   ＪＲ                   104        231       3.56\n3 神奈川県 小田急小田原線          96         31       3.59\n4 埼玉県   東武伊勢崎線            96         18       3.51\n5 神奈川県 横浜市営ブルーライン    82         77       3.66\n# … with 518 more rows\n\n\n　.groups引数は\"drop\"と\"keep\"以外にも\"drop_last\"があります。実はsummarise()に.groups引数を指定したい場合のデフォルト値は.groups == \"drop_last\"または\"keep\"ですが、ここがややこしいです。主なケースにおいてデフォルト値は\"drop\"となりますとなります。.groups == \"drop_last\"これは最後のグループ化変数のみ解除する意味です。今回の例だと、2番目のグループ化変数であるLineがグループ化変数から外され、Prefのみがグループ化変数として残る仕組みです。\n　それではデフォルト値が\"keep\"になるのはいつでしょうか。それは記述統計量の結果が長さ2以上のベクトルである場合です。平均値を求めるmean()、標準偏差を求めるsd()などは、結果として長さ1のベクトルを返します。しかし、長さ2以上ののベクトルを返す関数もあります。たとえば、分位数を求めるquantile()関数があります。quantile(ベクトル名, 0.25)の場合、第一四分位点のみ返すため、結果は長さ1のベクトルです。しかし、quantile(ベクトル名, c(0.25, 0.5, 0.75))のように第一四分位点から第三四分位点を同時に計算し、長さ3のベクトルが返されるケースもありますし、第二引数を省略すると、最小値・第一四分位点・第二四分位点・第三四分位点・最大値、つまり、長さ5のベクトルが返される場合があります。\n\n# 第一四分位点のみを求める (長さ1のベクトル)\nquantile(df$Walk, 0.25, na.rm = TRUE)\n\n25% \n  2 \n\n# 引数を省略する (長さ5のベクトル)\nquantile(df$Walk, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n   1    2    5   12  116 \n\n\n　.groupsのデフォルト値が\"keep\"になるのは、このように長さ2以上のベクトルが返されるケースです。たとえば、都府県と最寄りの駅の路線でグループ化し、店舗までの徒歩距離の平均値を求めるとします。デフォルト値の変化を見るために、ここではあえて.groups引数を省略しました。\n\ndf %>%\n  filter(!is.na(Walk)) %>%\n  group_by(Pref, Line) %>%\n  summarise(Mean = mean(Walk))\n\n`summarise()` has grouped output by 'Pref'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 509 × 3\n# Groups:   Pref [9]\n   Pref   Line                       Mean\n   <chr>  <chr>                     <dbl>\n 1 京都府 ＪＲ                       4   \n 2 京都府 ＪＲ京都線                10   \n 3 京都府 JR奈良線                   3.33\n 4 京都府 ＪＲ奈良線                 8   \n 5 京都府 JR小浜線                  16.5 \n 6 京都府 ＪＲ小浜線                 9   \n 7 京都府 ＪＲ山陰本線              19   \n 8 京都府 JR山陰本線（京都-米子）    8.67\n 9 京都府 ＪＲ山陰本線（京都-米子）  9.23\n10 京都府 ＪＲ嵯峨野線               5   \n# … with 499 more rows\n\n\n　最初はPrefとLineでグループ化しましたが、summarise()の後、Lineがグループ化変数から外されました。つまり、引数が\"drop_last\"になっていることです。\n　それでは、平均値に加えて、第一四分位点と第三四分位点も計算し、Quantileという名で格納してみましょう。\n\ndf %>%\n  filter(!is.na(Walk)) %>%\n  group_by(Pref, Line) %>%\n  summarise(Mean     = mean(Walk),\n            Quantile = quantile(Walk, c(0.25, 0.75)))\n\n`summarise()` has grouped output by 'Pref', 'Line'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 1,018 × 4\n# Groups:   Pref, Line [509]\n   Pref   Line        Mean Quantile\n   <chr>  <chr>      <dbl>    <dbl>\n 1 京都府 ＪＲ        4        1.25\n 2 京都府 ＪＲ        4        5   \n 3 京都府 ＪＲ京都線 10       10   \n 4 京都府 ＪＲ京都線 10       10   \n 5 京都府 JR奈良線    3.33     2   \n 6 京都府 JR奈良線    3.33     5   \n 7 京都府 ＪＲ奈良線  8        4   \n 8 京都府 ＪＲ奈良線  8        9   \n 9 京都府 JR小浜線   16.5      9.75\n10 京都府 JR小浜線   16.5     23.2 \n# … with 1,008 more rows\n\n\n　同じPref、Lineのケースが2つずつ出来ています。最初に来る数値は第一四分位点、次に来るのが第三四分位点です。そして最初のグループ化変数であったPrefとLineが、summarise()後もグループ化変数として残っていることが分かります。\n　.groups引数は記述統計量だけを計算する意味ではあまり意識する必要がありません。しかし、得られた記述統計量から何らかの計算をしたり、さらにもう一回記述統計量を求めたりする際、予期せぬ結果が得られる可能性があるため注意する必要があります。出来る限り.groups引数は指定するようにしましょう。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#変数の計算",
    "href": "tutorial/R/dplyr_intro.html#変数の計算",
    "title": "dplyr入門",
    "section": "変数の計算",
    "text": "変数の計算\n\nmutate()関数の使い方\n　続いて、データフレーム (または、tibble)内の変数を用いて計算を行い、その結果を新しい列として格納するmutate()関数について紹介します。まず、mutate()関数の書き方からです。\n# mutate()関数の使い方\nデータフレーム/tibble名 %>%\n  mutate(新しい変数名 = 処理内容)\n　これは何らかの処理を行い、その結果を新しい変数としてデータフレーム (または、tibble)に追加することを意味します。新しく出来た変数は、基本的に最後の列になります。ここでは分単位であるWalkを時間単位に変換したWalk_Hour変数を作成するとします。処理内容はWalk / 60です。最後に、都府県名、店舗名、徒歩距離 (分)、徒歩距離 (時間)のみを残し、遠い順にソートします。\n\ndf %>%\n  filter(!is.na(Walk)) %>%\n  mutate(Walk_Hour = Walk / 60) %>%\n  select(Pref, Name, Walk, Walk_Hour) %>%\n  arrange(desc(Walk_Hour))\n\n# A tibble: 5,375 × 4\n   Pref     Name                               Walk Walk_Hour\n   <chr>    <chr>                             <dbl>     <dbl>\n 1 埼玉県   札幌ラーメン どさん子 小鹿野店      116     1.93 \n 2 神奈川県 札幌ラーメン どさん子 中津店         73     1.22 \n 3 千葉県   札幌ラーメン どさん子 佐原51号店     59     0.983\n 4 神奈川県 札幌ラーメン どさん子 山際店         50     0.833\n 5 千葉県   札幌ラーメン どさん子 関宿店         49     0.817\n 6 兵庫県   濃厚醤油 中華そば いせや 玉津店      43     0.717\n 7 大阪府   河童ラーメン本舗 岸和田店            38     0.633\n 8 埼玉県   ラーメン山岡家 上尾店                35     0.583\n 9 兵庫県   濃厚醤油 中華そば いせや 大蔵谷店    35     0.583\n10 大阪府   河童ラーメン本舗 松原店              31     0.517\n# … with 5,365 more rows\n\n\n　mutate()は3行目に登場しますが、これはWalkを60に割った結果をWalk_Hourとしてデータフレーム (または、tibble)の最後の列として格納することを意味します。もし、最後の列でなく、ある変数の前、または後にしたい場合は、.beforeまたは.after引数を追加します。これはselect()関数の.beforeと.afterと同じ使い方です。たとえば、新しく出来たWalk_HourをIDとNameの間に入れたい場合は\n\n# コードの3行名を修正 (.before使用)\nmutate(Walk_Hour = Walk / 60,\n       .before   = Name)\n\n# コードの3行名を修正 (.after使用)\nmutate(Walk_Hour = Walk / 60,\n       .after    = ID)\n\nのようにコードを修正します。\n　むろん、変数間同士の計算も可能です。たとえば、以下のようなdf2があり、1店舗当たりの平均口コミ数を計算し、ScoreN_Meanという変数名でScoreN_Sumの後に格納うするとします。この場合、ScoreN_Sum変数をNで割るだけです。\n\ndf2 <- df %>%\n  group_by(Pref) %>%\n  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),\n            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),\n            Score_Mean  = mean(Score, na.rm = TRUE),\n            N           = n(),\n            .groups     = \"drop\")\n\n\ndf2 %>%\n  mutate(ScoreN_Mean = ScoreN_Sum / N,\n         .after      = ScoreN_Sum)\n\n# A tibble: 9 × 6\n  Pref     Budget_Mean ScoreN_Sum ScoreN_Mean Score_Mean     N\n  <chr>          <dbl>      <dbl>       <dbl>      <dbl> <int>\n1 京都府         1399.        216       0.522       3.68   414\n2 兵庫県         1197.        230       0.389       3.54   591\n3 千葉県         1124.        259       0.259       3.72  1000\n4 和歌山県       1252          83       0.593       3.97   140\n5 埼玉県         1147.        278       0.278       3.64  1000\n6 大阪府         1203.        516       0.516       3.77  1000\n7 奈良県         1169.         45       0.306       3.85   147\n8 東京都         1283.       1165       1.16        3.67  1000\n9 神奈川県       1239.        587       0.587       3.53  1000\n\n\n　このように、データ内の変数を用いた計算結果を新しい列として追加する場合は、mutate()が便利です。これをmutate()を使わずに処理する場合、以下のようなコードになりますが、可読性が相対的に低いことが分かります。\n\ndf2$ScoreN_Mean <- df2$ScoreN_Sum / df2$N\ndf2 <- df2[, c(\"Pref\", \"Budget_Mean\", \"Walk_Mean\", \n               \"ScoreN_Sum\", \"ScoreN_Mean\", \"Score_Mean\", \"N\")]\n\n　むろん、計算には+や/のような演算子だけでなく、関数を使うことも可能です。たとえば、Budgetが1000円未満なら\"Cheap\"、1000円以上なら\"Expensive\"と示す変数Budget2を作成する場合はifelse()関数が使えます。\n\ndf %>% \n  mutate(Budget2 = ifelse(Budget < 1000, \"Cheap\", \"Expensive\")) %>%\n  filter(!is.na(Budget2)) %>% # Budget2が欠損した店舗を除外\n  group_by(Pref, Budget2) %>% # PrefとBudget2でグループ化\n  summarise(N = n(),          # 店舗数を表示\n            .groups = \"drop\")\n\n# A tibble: 18 × 3\n   Pref     Budget2       N\n   <chr>    <chr>     <int>\n 1 京都府   Cheap        22\n 2 京都府   Expensive    28\n 3 兵庫県   Cheap        39\n 4 兵庫県   Expensive    27\n 5 千葉県   Cheap        64\n 6 千葉県   Expensive    72\n 7 和歌山県 Cheap        10\n 8 和歌山県 Expensive     5\n 9 埼玉県   Cheap        37\n10 埼玉県   Expensive    45\n11 大阪府   Cheap       104\n12 大阪府   Expensive   115\n13 奈良県   Cheap        11\n14 奈良県   Expensive    10\n15 東京都   Cheap       206\n16 東京都   Expensive   236\n17 神奈川県 Cheap        66\n18 神奈川県 Expensive    54\n\n\n　これは各都府県ごとの予算1000円未満の店と以上の店の店舗数をまとめた表となります。もし、500円未満なら\"Cheap\"、500円以上~1000円未満なら\"Reasonable\"、1000円以上なら\"Expensive\"になるBudget3変数を作るにはどうすればよいでしょうか。これはifelse()を重ねることも出来ますが、ここではcase_when()関数が便利です。まずは、ifelse()を使ったコードは以下の通りです。\n\n# ifelse()を使う場合\ndf %>% \n  mutate(Budget3 = ifelse(Budget < 500, \"Cheap\", \n                          ifelse(Budget >= 500 & Budget < 1000, \"Reasonable\",\n                                 \"Expensive\"))) %>%\n  filter(!is.na(Budget3)) %>%\n  group_by(Pref, Budget3) %>%\n  summarise(N = n(),         \n            .groups = \"drop\")\n\n　case_when()を使うと以下のような書き方になります。\n\n# case_when()を使う場合\ndf %>% \n  mutate(Budget3 = case_when(Budget < 500                  ~ \"Cheap\",\n                             Budget >= 500 & Budget < 1000 ~ \"Reasonable\",\n                             Budget >= 1000                ~ \"Expensive\"),\n         # 新しく出来た変数をfactor型にその場で変換することも可能\n         Budget3 = factor(Budget3, \n                          levels = c(\"Cheap\", \"Reasonable\", \"Expensive\"))) %>%\n  filter(!is.na(Budget3)) %>%\n  group_by(Pref, Budget3) %>%\n  summarise(N = n(),         \n            .groups = \"drop\")\n\n　書く手間の観点ではcase_when()はifelse()と大きく違いはないかも知れませんが、コードが非常に読みやすくなっています。case_when()関数の書き方は以下の通りです。\n# case_when()の使い方\nデータフレーム/tibble名 %>%\n  mutate(新変数名 = case_when(条件1 ~ 条件1を満たす場合の結果値, \n                             条件2 ~ 条件2を満たす場合の結果値, \n                             条件3 ~ 条件3を満たす場合の結果値, \n                             ...))\n　似たような機能をする関数としてrecode()関数があります。これは変数の値を単純に置換したい場合に便利な関数です。たとえば、都府県名をローマ字に変換するケースを考えてみましょう。\n\n# recode()を使う場合\ndf2 %>% \n  mutate(Pref2 = recode(Pref,\n                        \"東京都\"   = \"Tokyo\",\n                        \"神奈川県\" = \"Kanagawa\",\n                        \"千葉県\"   = \"Chiba\",\n                        \"埼玉県\"   = \"Saitama\",\n                        \"大阪府\"   = \"Osaka\",\n                        \"京都府\"   = \"Kyoto\",\n                        \"兵庫県\"   = \"Hyogo\",\n                        \"奈良県\"   = \"Nara\",\n                        \"和歌山県\" = \"Wakayama\",\n                        .default   = \"NA\"))\n\n# A tibble: 9 × 6\n  Pref     Budget_Mean ScoreN_Sum Score_Mean     N Pref2   \n  <chr>          <dbl>      <dbl>      <dbl> <int> <chr>   \n1 京都府         1399.        216       3.68   414 Kyoto   \n2 兵庫県         1197.        230       3.54   591 Hyogo   \n3 千葉県         1124.        259       3.72  1000 Chiba   \n4 和歌山県       1252          83       3.97   140 Wakayama\n5 埼玉県         1147.        278       3.64  1000 Saitama \n6 大阪府         1203.        516       3.77  1000 Osaka   \n7 奈良県         1169.         45       3.85   147 Nara    \n8 東京都         1283.       1165       3.67  1000 Tokyo   \n9 神奈川県       1239.        587       3.53  1000 Kanagawa\n\n\n　使い方は非常に直感的です。\n# recode()の使い方\nデータフレーム/tibble名 %>%\n  mutate(新変数名 = recode(元の変数名,\n                            元の値1 =  新しい値1, \n                            元の値2 =  新しい値2, \n                            元の値3 =  新しい値3, \n                            ...,\n                            .default = 該当しない場合の値))\n　最後の.default引数は、もし該当する値がない場合に返す値を意味し、長さ1のベクトルを指定します。もし、指定しない場合はNAが表示されます。また、ここには紹介しておりませんでしたが、.missing引数もあり、これは欠損値の場合に返す値を意味します。\n　もう一つ注意すべきところは、今回はcharacter型変数をcharacter型へ変換したため、「\"東京都\" = \"Tokyo\"」のような書き方をしました。しかし、numeric型からcharacter型に変換する場合は数字の部分を`で囲む必要があります。たとえば、「`1` = \"Tokyo\"」といった形式です。ただし、character型からnumeric型への場合は「\"東京都\" = 1」で構いません。\n　recode()は値をまとめる際にも便利です。たとえば、EastJapanという変数を作成し、関東なら1を、それ以外なら0を付けるとします。そして、これはPref変数の後に位置づけます。\n\n# 都府県を関東か否かでまとめる\ndf2 %>% \n  mutate(EastJapan = recode(Pref,\n                            \"東京都\"   = 1,\n                            \"神奈川県\" = 1,\n                            \"千葉県\"   = 1,\n                            \"埼玉県\"   = 1,\n                            \"大阪府\"   = 0,\n                            \"京都府\"   = 0,\n                            \"兵庫県\"   = 0,\n                            \"奈良県\"   = 0,\n                            \"和歌山県\" = 0,\n                            .default  = 0),\n         .after = Pref)\n\n# A tibble: 9 × 6\n  Pref     EastJapan Budget_Mean ScoreN_Sum Score_Mean     N\n  <chr>        <dbl>       <dbl>      <dbl>      <dbl> <int>\n1 京都府           0       1399.        216       3.68   414\n2 兵庫県           0       1197.        230       3.54   591\n3 千葉県           1       1124.        259       3.72  1000\n4 和歌山県         0       1252          83       3.97   140\n5 埼玉県           1       1147.        278       3.64  1000\n6 大阪府           0       1203.        516       3.77  1000\n7 奈良県           0       1169.         45       3.85   147\n8 東京都           1       1283.       1165       3.67  1000\n9 神奈川県         1       1239.        587       3.53  1000\n\n\n　ただし、関東以外は全て0になるため、以下のように省略することも可能です。\n\n# .default引数を指定する場合\ndf3 <- df2 %>% \n  mutate(EastJapan = recode(Pref,\n                            \"東京都\"   = 1,\n                            \"神奈川県\" = 1,\n                            \"千葉県\"   = 1,\n                            \"埼玉県\"   = 1,\n                            .default  = 0),\n         .after = Pref)\n\ndf3\n\n# A tibble: 9 × 6\n  Pref     EastJapan Budget_Mean ScoreN_Sum Score_Mean     N\n  <chr>        <dbl>       <dbl>      <dbl>      <dbl> <int>\n1 京都府           0       1399.        216       3.68   414\n2 兵庫県           0       1197.        230       3.54   591\n3 千葉県           1       1124.        259       3.72  1000\n4 和歌山県         0       1252          83       3.97   140\n5 埼玉県           1       1147.        278       3.64  1000\n6 大阪府           0       1203.        516       3.77  1000\n7 奈良県           0       1169.         45       3.85   147\n8 東京都           1       1283.       1165       3.67  1000\n9 神奈川県         1       1239.        587       3.53  1000\n\n\n　新しく出来たEastJapanのデータ型はなんでしょうか。\n\nclass(df3$EastJapan)\n\n[1] \"numeric\"\n\n\n　EastJapanはnumeric型ですね。もし、これをfactor型にしたい場合はどうすればよいでしょうか。それはmutate()内でEastJapanを生成した後にfactor()関数を使うだけです。\n\n# EastJapan変数をfactor型にする\ndf3 <- df2 %>% \n  mutate(EastJapan = recode(Pref,\n                            \"東京都\"   = 1,\n                            \"神奈川県\" = 1,\n                            \"千葉県\"   = 1,\n                            \"埼玉県\"   = 1,\n                            .default  = 0),\n         EastJapan = factor(EastJapan, levels = c(0, 1)),\n         .after = Pref)\n\ndf3$EastJapan\n\n[1] 0 0 1 0 1 0 0 1 1\nLevels: 0 1\n\n\n　EastJapanがfactor型になりました。実は、recodeは再コーディングと同時にfactor化をしてくれる機能があります。ただし、recode()関数でなく、recode_factor()関数を使います。\n\n# recode_factor()を使う方法\ndf3 <- df2 %>% \n  mutate(EastJapan = recode_factor(Pref,\n                                   \"東京都\"   = 1,\n                                   \"神奈川県\" = 1,\n                                   \"千葉県\"   = 1,\n                                   \"埼玉県\"   = 1,\n                                   .default  = 0),\n         .after = Pref)\n\ndf3$EastJapan\n\n[1] 0 0 1 0 1 0 0 1 1\nLevels: 1 0\n\n\n　ただし、levelの順番はrecode_factor()内で定義された順番になることに注意してください。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#factor型の処理に便利な関数",
    "href": "tutorial/R/dplyr_intro.html#factor型の処理に便利な関数",
    "title": "dplyr入門",
    "section": "factor型の処理に便利な関数",
    "text": "factor型の処理に便利な関数\n\nfactor型の必要性\n　話がずれますが、可視化における名目変数の扱いについて考えたいと思います。横軸、または縦軸が気温、成績、身長のような連続変数ではなく、都道府県や国、企業のような名目変数になる場合があります。たとえば、棒グラフの横軸は 図 3 のように、一般的に名目変数になる場合が多いです。\n\n\n\n\n\n図 3: 横軸が名目変数の棒グラフ\n\n\n\n\n　ここでは横軸の順番に注目してください。京都府、埼玉県、神奈川県、…の順番になっていますね。「この順番で大満足だよ!」という方がいるかも知れませんが、そうでない方もおおいでしょう。普通考えられるものとしては、都道府県コードの順か、縦軸が高い順 (低い順)でしょう。都道府県コードの順だと、埼玉県、千葉県、東京都、神奈川県、京都府、大阪府、兵庫県、奈良県、和歌山県の順番になります。または、縦軸 (口コミ評価の平均値)が高い順なら和歌山県、奈良県、大阪府、…の順番になります。あるいは50音順も考えられるでしょう。アメリカの場合、州を並べる際、アルファベット順で並べます。\n　自分でこの順番をコントロールするには可視化の前の段階、つまりデータハンドリングの段階で順番を決めなくてはなりません。これを決めておかない場合、Rが勝手に順番を指定します。具体的にはロケール (locale)というパソコン内の空間に文字情報が含まれているわけですが、そこに保存されている文字の順番となります。たとえば、日本語ロケールには「京」が「埼」よりも先に保存されているわけです。\n　したがって、名目変数がグラフに含まれる場合は、名目変数の表示順番を決める必要があり、そこで必要なのがfactor型です。名目変数がcharacter型の場合、ロケールに保存されている順でソートされますが、factor型の場合、予め指定した順番でソートされます。\n　たとえば、dfを用いて、都道府県ごとの口コミ評価の平均値を計算し、その結果をScore_dfとして保存します。\n\nScore_df <- df %>%\n    group_by(Pref) %>%\n    summarise(Score   = mean(Score, na.rm = TRUE),\n              .groups = \"drop\")\n\nScore_df\n\n# A tibble: 9 × 2\n  Pref     Score\n  <chr>    <dbl>\n1 京都府    3.68\n2 兵庫県    3.54\n3 千葉県    3.72\n4 和歌山県  3.97\n5 埼玉県    3.64\n6 大阪府    3.77\n7 奈良県    3.85\n8 東京都    3.67\n9 神奈川県  3.53\n\n\n　この時点で勝手にロケール順になります。実際、表示されたScore_dfを見るとPrefの下に`<chr>と表記されており、Prefはcharacter型であることが分かります。これをこのまま棒グラフに出してみましょう。可視化の方法は執筆中のE-Bookで詳細に解説する予定ですので、ここでは結果だけに注目してください。\n\n\n\n\n\n図 4: Prefがcharacter型の場合 (1)\n\n\n\n\n　横軸の順番があまり直感的ではありませんね。それでは、Score_dfをScoreが高い順にソートし、Score_df2で保存してから、もう一回試してみます。\n\nScore_df2 <- Score_df %>%\n  arrange(desc(Score))\n\nScore_df2\n\n# A tibble: 9 × 2\n  Pref     Score\n  <chr>    <dbl>\n1 和歌山県  3.97\n2 奈良県    3.85\n3 大阪府    3.77\n4 千葉県    3.72\n5 京都府    3.68\n6 東京都    3.67\n7 埼玉県    3.64\n8 兵庫県    3.54\n9 神奈川県  3.53\n\n\n　ここでもPrefはcharacter型ですが、とりあえず、これで図を出してみます。\n\n\n\n\n\n図 5: Prefがcharacter型の場合 (2)\n\n\n\n\n　結果は全く変わっておりません。それでは、Score_dfのPref列をfactor型に変換し、順番は口コミ評価の平均値が高い順番にしてみましょう。結果はScore_df_f1という名で保存します。\n\nScore_df_f1 <- Score_df %>%\n  mutate(Pref = factor(Pref, levels = c(\"和歌山県\", \"奈良県\", \"大阪府\",\n                                        \"千葉県\", \"京都府\", \"東京都\",\n                                        \"埼玉県\", \"兵庫県\", \"神奈川県\")))\n\nScore_df_f1\n\n# A tibble: 9 × 2\n  Pref     Score\n  <fct>    <dbl>\n1 京都府    3.68\n2 兵庫県    3.54\n3 千葉県    3.72\n4 和歌山県  3.97\n5 埼玉県    3.64\n6 大阪府    3.77\n7 奈良県    3.85\n8 東京都    3.67\n9 神奈川県  3.53\n\n\n　表示される順番はScore_dfとScore_df_f1も同じですが、Prefのデータ型が<fct>、つまりfactor型であることが分かります。実際、Pref列だけ抽出した場合、factor型として、和歌山県から神奈川県の順になっていることが確認できます。\n\nScore_df_f1$Pref\n\n[1] 京都府   兵庫県   千葉県   和歌山県 埼玉県   大阪府   奈良県   東京都  \n[9] 神奈川県\n9 Levels: 和歌山県 奈良県 大阪府 千葉県 京都府 東京都 埼玉県 ... 神奈川県\n\n\n　このScore_df_f1データを使って、 図 4 と全く同じコードを実行した結果が 図 6 です。\n\n\n\n\n\n図 6: Prefがfactor型の場合\n\n\n\n\n　これまでの話をまとめるの以下の2点が分かります。\n\n変数がcharacter型である場合、自動的にロケール順でソートされる。\n変数がfactor型である場合、データ内の順番やロケール順と関係なく、指定されたレベル (水準)の順でソートされる。\n\n　特に2番目の点についてですが、これは必ずしも順序付きfactorである必要はありません。順序付きfactor型でなくても、factor()内で指定した順にソートされます。むろん、順序付きfactor型なら指定された順序でソートされます。\n　これからはfactor型変換の際に便利な関数をいくつか紹介しますが、その前に数値として表現された名目変数について話します。たとえば、Score_df_f1に関東地域なら1を、その他の地域なら0を付けたKantoという変数があるとします。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Kanto = ifelse(Pref %in% c(\"東京都\", \"神奈川県\", \"千葉県\", \"埼玉県\"), 1, 0))\n\nScore_df_f1\n\n# A tibble: 9 × 3\n  Pref     Score Kanto\n  <fct>    <dbl> <dbl>\n1 京都府    3.68     0\n2 兵庫県    3.54     0\n3 千葉県    3.72     1\n4 和歌山県  3.97     0\n5 埼玉県    3.64     1\n6 大阪府    3.77     0\n7 奈良県    3.85     0\n8 東京都    3.67     1\n9 神奈川県  3.53     1\n\n\n　Kanto変数のデータ型は、<dbl>、つまりnumeric型です。しかし、これは明らかに名目変数ですね。これをこのままKantoを横軸にした図を出すと 図 7 のようになります。\n\n\n\n\n\n図 7: Kantoがnumeric型の場合\n\n\n\n\n　この場合、図の横軸はKantoの値が小さい順でソートされます。ただし、このような図は非常に見にくいため、1に\"関東\"、0に\"関西\"とラベルを付けたfactor型に変換した方が望ましいです。numeric型をラベル付きのfactor型にするためには、levels引数には元の数値を、labels引数にはそれぞれの数値に対応したラベルを指定します。また、関東の方を先に出したいので、factor()内のlevels引数はc(0, 1)でなく、c(1, 0)にします。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Kanto = factor(Kanto, levels = c(1, 0), labels = c(\"関東\", \"その他\")))\n\nScore_df_f1\n\n# A tibble: 9 × 3\n  Pref     Score Kanto \n  <fct>    <dbl> <fct> \n1 京都府    3.68 その他\n2 兵庫県    3.54 その他\n3 千葉県    3.72 関東  \n4 和歌山県  3.97 その他\n5 埼玉県    3.64 関東  \n6 大阪府    3.77 その他\n7 奈良県    3.85 その他\n8 東京都    3.67 関東  \n9 神奈川県  3.53 関東  \n\n\n　Kanto変数がfactor型に変換されたことが分かります。\n\nScore_df_f1$Kanto\n\n[1] その他 その他 関東   その他 関東   その他 その他 関東   関東  \nLevels: 関東 その他\n\n\n　また、\"関東\"、\"その他\"の順になっていますね。これを図として出力した結果が 図 8 です。\n\n\n\n\n\n図 8: Kantoがfactor型の場合\n\n\n\n\n　このように数値型名目変数でも、factor化することによって、自由に横軸の順番を変えることができます。それでは、factor化に使える便利な関数をいくつか紹介します。\n\n\n{forcats}パッケージについて\n　実はfactor型への変換や、順番に変更などは全てR内蔵のfactor()関数で対応可能ですが、ここでは{forcats}パッケージが提供しているfct_*()関数を使用します。{forcats}パッケージは{tidyverse}を読み込む際、自動的に読み込まれるため、既に{tidyverse}を読み込んでいる場合、別途のコードは要りません。\n　実はfactor型への変換や、順番に変更などは全てR内蔵のfactor()関数で対応可能ですが、ここでは{forcats}パッケージが提供しているfct_*()関数を使用します。{forcats}パッケージは{tidyverse}を読み込む際、自動的に読み込まれるため、既に{tidyverse}を読み込んでいる場合、別途のコードは要りません。\nfct_relevel(): 水準の順番を変更する\n　Score_df_f1のf1はScoreが高い順になっています。これを50音順に変更する際、fct_relevel()関数を使います。\n# 新しい変数名と元となる変数名が一致すると上書きになる\nデータフレーム名 %>%\n  mutate(新しい変数名 = fct_releve(元となる変数名, \n                                    \"水準1\", \"水準2\", \"水準3\", ...))\n　ここでは、Pref変数を再調整したPref2変数を作ってみましょう。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Pref2 = fct_relevel(Pref, \"大阪府\", \"神奈川県\", \"京都府\", \n                             \"埼玉県\", \"千葉県\", \"東京都\", \n                             \"奈良県\", \"兵庫県\", \"和歌山県\"))\n\nScore_df_f1\n\n# A tibble: 9 × 4\n  Pref     Score Kanto  Pref2   \n  <fct>    <dbl> <fct>  <fct>   \n1 京都府    3.68 その他 京都府  \n2 兵庫県    3.54 その他 兵庫県  \n3 千葉県    3.72 関東   千葉県  \n4 和歌山県  3.97 その他 和歌山県\n5 埼玉県    3.64 関東   埼玉県  \n6 大阪府    3.77 その他 大阪府  \n7 奈良県    3.85 その他 奈良県  \n8 東京都    3.67 関東   東京都  \n9 神奈川県  3.53 関東   神奈川県\n\n\n　一見、PrefとPref2変数は同じように見えますが、水準はどうなっているでしょうか。\n\nlevels(Score_df_f1$Pref)  # Prefの水準\n\n[1] \"和歌山県\" \"奈良県\"   \"大阪府\"   \"千葉県\"   \"京都府\"   \"東京都\"   \"埼玉県\"  \n[8] \"兵庫県\"   \"神奈川県\"\n\nlevels(Score_df_f1$Pref2) # Pref2の水準\n\n[1] \"大阪府\"   \"神奈川県\" \"京都府\"   \"埼玉県\"   \"千葉県\"   \"東京都\"   \"奈良県\"  \n[8] \"兵庫県\"   \"和歌山県\"\n\n\n　問題なく50音順になっていることが分かります。他にもfct_relevel()には全ての水準名を指定する必要がありません。一部の水準名も可能です。たとえば、「関東が関西の先に来るなんでけしからん！」と思う読者もいるでしょう。この場合、関西の府県名を入れると、指定した水準が最初に位置するようになります。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Pref3 = fct_relevel(Pref, \"京都府\", \"大阪府\",\n                             \"兵庫県\", \"奈良県\", \"和歌山県\"))\n\nlevels(Score_df_f1$Pref3) # Pref3の水準\n\n[1] \"京都府\"   \"大阪府\"   \"兵庫県\"   \"奈良県\"   \"和歌山県\" \"千葉県\"   \"東京都\"  \n[8] \"埼玉県\"   \"神奈川県\"\n\n\n　一部の水準名のみを指定するとその水準が最初に移動されますが、after引数を指定すると、位置を調整することも可能です。after = 2の場合、元となる変数の1、3番目の水準は維持され、3番目以降に指定した水準、それに続いて指定されていない水準の順番になります。Prefは和歌山、奈良、大阪の順ですが、ここで京都と東京を、奈良と大阪の間に移動するなら、\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Pref4 = fct_relevel(Pref, \"京都府\", \"東京都\", after = 2))\n\nlevels(Score_df_f1$Pref4) # Pref4の水準\n\n[1] \"和歌山県\" \"奈良県\"   \"京都府\"   \"東京都\"   \"大阪府\"   \"千葉県\"   \"埼玉県\"  \n[8] \"兵庫県\"   \"神奈川県\"\n\n\nのように書きます。afterを指定しない場合のデフォルト値は0であるため、最初に移動します。\nfct_recode(): 水準のラベルを変更する\n　fct_recode()は水準のラベルを変更する時に使う関数で、以下のように使います。\n# 新しい変数名と元となる変数名が一致すると上書きになる\nデータフレーム名 %>%\n  mutate(新しい変数名 = fct_recode(元となる変数名, \n                                   新しいラベル1 = \"既存のラベル1\",\n                                   新しいラベル2 = \"既存のラベル2\",\n                                   新しいラベル3 = \"既存のラベル3\",\n                                   ...))\n　注意点としては新しいラベルは\"で囲まず、既存のラベルは\"で囲む点です。それでは、Prefのラベルをローマ字に変更してみましょう。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Pref5 = fct_recode(Pref,\n                            Saitama  = \"埼玉県\",\n                            Wakayama = \"和歌山県\",\n                            Kyoto    = \"京都府\",\n                            Osaka    = \"大阪府\",\n                            Tokyo    = \"東京都\",\n                            Nara     = \"奈良県\",\n                            Kanagawa = \"神奈川県\",\n                            Hyogo    = \"兵庫県\",\n                            Chiba    = \"千葉県\"))\n\nScore_df_f1\n\n# A tibble: 9 × 7\n  Pref     Score Kanto  Pref2    Pref3    Pref4    Pref5   \n  <fct>    <dbl> <fct>  <fct>    <fct>    <fct>    <fct>   \n1 京都府    3.68 その他 京都府   京都府   京都府   Kyoto   \n2 兵庫県    3.54 その他 兵庫県   兵庫県   兵庫県   Hyogo   \n3 千葉県    3.72 関東   千葉県   千葉県   千葉県   Chiba   \n4 和歌山県  3.97 その他 和歌山県 和歌山県 和歌山県 Wakayama\n5 埼玉県    3.64 関東   埼玉県   埼玉県   埼玉県   Saitama \n6 大阪府    3.77 その他 大阪府   大阪府   大阪府   Osaka   \n7 奈良県    3.85 その他 奈良県   奈良県   奈良県   Nara    \n8 東京都    3.67 関東   東京都   東京都   東京都   Tokyo   \n9 神奈川県  3.53 関東   神奈川県 神奈川県 神奈川県 Kanagawa\n\n\n　fct_recode()の中に指定する水準の順番は無視されます。つまり、水準の順番はそのまま維持されるため、好きな順番で結構です。また、全ての水準を指定せず、一部のみ変更することも可能です。それではPref5の順番がPrefの順番と同じかを確認してみましょう。\n\nlevels(Score_df_f1$Pref)  # Prefの水準\n\n[1] \"和歌山県\" \"奈良県\"   \"大阪府\"   \"千葉県\"   \"京都府\"   \"東京都\"   \"埼玉県\"  \n[8] \"兵庫県\"   \"神奈川県\"\n\nlevels(Score_df_f1$Pref5) # Pref5の水準\n\n[1] \"Wakayama\" \"Nara\"     \"Osaka\"    \"Chiba\"    \"Kyoto\"    \"Tokyo\"    \"Saitama\" \n[8] \"Hyogo\"    \"Kanagawa\"\n\n\nfct_rev(): 水準の順番を反転する\n　水準の順番を反転することは非常によくあります。たとえば、グラフの読みやすさのために、左右または上下を反転するケースがあります。既に何回も強調しましたように、名目変数は基本的にfactor型にすべきであり、ここでfct_rev()関数が非常に便利です。たとえば、Pref2の水準は50音順でありますが、これを反転し、Pref6という名の列として追加してみましょう。\n\nScore_df_f1 <- Score_df_f1 %>%\n  mutate(Pref6 = fct_rev(Pref2))\n\nlevels(Score_df_f1$Pref6)\n\n[1] \"和歌山県\" \"兵庫県\"   \"奈良県\"   \"東京都\"   \"千葉県\"   \"埼玉県\"   \"京都府\"  \n[8] \"神奈川県\" \"大阪府\"  \n\n\n　関数一つで水準の順番が反転されました。\nfct_infreq(): 頻度順に順番を変更する\n　続いて、水準の順番を頻度順に合わせるfct_infreq()関数です。たとえば、Scoreが欠損でないケースのみで構成されたdf2を考えてみましょう。\n\ndf2 <- df %>%\n  filter(!is.na(Score))\n\n　そして、都府県ごとのケース数を計算します。\n\ntable(df2$Pref)\n\n\n  京都府   兵庫県   千葉県 和歌山県   埼玉県   大阪府   奈良県   東京都 \n      79       85      108       24      118      175       28      298 \n神奈川県 \n     219 \n\n\n　ここでPrefをfactor化しますが、水準の順番を店舗数が多い方を先にするにはどうすれば良いでしょうか。fct_infreq()関数は指定された変数の各値の個数を計算し、多い順にfactorの水準を調整します。\n\ndf2 <- df2 %>%\n  # 多く出現した値順でfactor化する\n  mutate(Pref = fct_infreq(Pref))\n\nlevels(df2$Pref) # df2のPref変数の水準を出力\n\n[1] \"東京都\"   \"神奈川県\" \"大阪府\"   \"埼玉県\"   \"千葉県\"   \"兵庫県\"   \"京都府\"  \n[8] \"奈良県\"   \"和歌山県\"\n\n\n　\"東京都\"、\"神奈川県\"、\"大阪府\"、…の順で水準の順番が調整され、これはtable(df$Pref2)の順位とも一致します。\nfct_inorder(): データ内の出現順番に順番を変更する\n　続いて、fct_inorder()ですが、これは意外と頻繁に使われる関数です。たとえば、自分でデータフレームなどを作成し、ケースの順番も綺麗に整えたとします。しかし、既に指摘した通り、データフレーム (または、tibble)での順番とグラフにおける順番は一致するとは限りません。データフレームに格納された順番でfactorの水準が設定できれば非常に便利でしょう。そこで使うのがfct_inorder()です。\n　たとえば、dfのPrefは\"東京都\"が1000個並び、続いて\"神奈川県\"が1000個、\"千葉県\"が1000個、…の順番で格納されています。この順番をそのままfactorの順番にするには以下のように書きます。\n\ndf3 <- df %>%\n  # Pref変数をfactor化し、水準は出現順とする\n  # 変換後の結果はPrefに上書きする\n  mutate(Pref = fct_inorder(Pref))\n\nlevels(df3$Pref)\n\n[1] \"東京都\"   \"神奈川県\" \"千葉県\"   \"埼玉県\"   \"大阪府\"   \"京都府\"   \"兵庫県\"  \n[8] \"奈良県\"   \"和歌山県\"\n\n\nfct_shift(): 水準の順番をずらす\n　続いて、水準の順番をずらすfct_shift()関数を紹介します。たとえば、「1:そう思う」〜「5:そう思わない」、「9:答えたくない」の6水準で構成された変数があるとします。\n\ndf4 <- tibble(\n  ID = 1:10,\n  Q1 = c(1, 5, 3, 2, 9, 2, 4, 9, 5, 1)\n) \n\ndf4 <- df4 %>%\n  mutate(Q1 = factor(Q1, levels = c(1:5, 9),\n                     labels = c(\"そう思う\", \n                                \"どちらかと言えばそう思う\",\n                                \"どちらとも言えない\",\n                                \"どちらかと言えばそう思わない\",\n                                \"そう思わない\",\n                                \"答えたくない\")))\n\ndf4\n\n# A tibble: 10 × 2\n      ID Q1                          \n   <int> <fct>                       \n 1     1 そう思う                    \n 2     2 そう思わない                \n 3     3 どちらとも言えない          \n 4     4 どちらかと言えばそう思う    \n 5     5 答えたくない                \n 6     6 どちらかと言えばそう思う    \n 7     7 どちらかと言えばそう思わない\n 8     8 答えたくない                \n 9     9 そう思わない                \n10    10 そう思う                    \n\n\n　水準の順番も「そう思う」〜「答えたくない」順で綺麗に整っています。この水準を反転するにはfct_rev()関数が便利です。Q1の水準を反転した変数をQ1_Rという新しい列として追加し、水準を確認してみましょう。\n\ndf4 <- df4 %>%\n  mutate(Q1_R = fct_rev(Q1))\n\ndf4\n\n# A tibble: 10 × 3\n      ID Q1                           Q1_R                        \n   <int> <fct>                        <fct>                       \n 1     1 そう思う                     そう思う                    \n 2     2 そう思わない                 そう思わない                \n 3     3 どちらとも言えない           どちらとも言えない          \n 4     4 どちらかと言えばそう思う     どちらかと言えばそう思う    \n 5     5 答えたくない                 答えたくない                \n 6     6 どちらかと言えばそう思う     どちらかと言えばそう思う    \n 7     7 どちらかと言えばそう思わない どちらかと言えばそう思わない\n 8     8 答えたくない                 答えたくない                \n 9     9 そう思わない                 そう思わない                \n10    10 そう思う                     そう思う                    \n\nlevels(df4$Q1_R)\n\n[1] \"答えたくない\"                 \"そう思わない\"                \n[3] \"どちらかと言えばそう思わない\" \"どちらとも言えない\"          \n[5] \"どちらかと言えばそう思う\"     \"そう思う\"                    \n\n\n　「答えたくない」が最初の順番に来ましてね。できれば、「そう思わない」〜「そう思う」、「答えたくない」の順番にしたいところです。ここで使うのがfct_shift()ですが、書き方がややこしいので、噛み砕いて解説します。\n# fct_shift()の使い方\nデータ名 %>%\n  mutate(新しい変数名 = fct_shift(元の変数名, n = 左方向へずらす個数))\n　問題はn =引数ですが、その挙動については以下の表を参照してください。\n\n\n\n\n\n\n  \n  \n    \n      \n      1番目\n      2番目\n      3番目\n      4番目\n      5番目\n      6番目\n    \n  \n  \n    n = -2\n\"E\"\n\"F\"\n\"A\"\n\"B\"\n\"C\"\n\"D\"\n    n = -1\n\"F\"\n\"A\"\n\"B\"\n\"C\"\n\"D\"\n\"E\"\n    n = 0 (初期状態)\n\"A\"\n\"B\"\n\"C\"\n\"D\"\n\"E\"\n\"F\"\n    n = 1\n\"B\"\n\"C\"\n\"D\"\n\"E\"\n\"F\"\n\"A\"\n    n = 2\n\"C\"\n\"D\"\n\"E\"\n\"F\"\n\"A\"\n\"B\"\n  \n  \n  \n\n\n\n\n　具体的には水準は左方向へn個移動します。元の水準がA, B, C, …, Fの順で、n = 1の場合、AがFの後ろへ移動し、B, C, D, E, Fが前の方へ1つずつ移動します。逆に右側へ1つ移動したい場合はn = -1のように書きます。今回は最初の水準を最後に移動させたいので、n = 1と指定します。\n\ndf4 <- df4 %>%\n  # Q1_Rの水準を左方向で1ずらす\n  mutate(Q1_R = fct_shift(Q1_R, n = 1))\n\nlevels(df4$Q1_R)\n\n[1] \"そう思わない\"                 \"どちらかと言えばそう思わない\"\n[3] \"どちらとも言えない\"           \"どちらかと言えばそう思う\"    \n[5] \"そう思う\"                     \"答えたくない\"                \n\n\n　これで水準の反転が完了しました。fct_shift()はこのように世論調査データの処理に便利ですが、他にも曜日の処理に使えます。例えば、1週間の始まりを月曜にするか日曜にするかによって、fct_shift()を使うケースがあります。\nfct_shuffle(): 水準の順番をランダム化する\n　あまり使わない機能ですが、水準の順番をランダム化することも可能です。使い方は非常に簡単で、fct_shuffle()に元の変数名を入れるだけです。たとえば、Score_dfのPrefの順番をランダム化し、Pref2として追加します。同じことをもう2回繰り返し、それぞれPref3とPref4という名前で追加してみましょう。\n\nScore_df <- Score_df %>%\n  mutate(Pref2 = fct_shuffle(Pref),\n         Pref3 = fct_shuffle(Pref),\n         Pref4 = fct_shuffle(Pref))\n\nScore_df\n\n# A tibble: 9 × 5\n  Pref     Score Pref2    Pref3    Pref4   \n  <chr>    <dbl> <fct>    <fct>    <fct>   \n1 京都府    3.68 京都府   京都府   京都府  \n2 兵庫県    3.54 兵庫県   兵庫県   兵庫県  \n3 千葉県    3.72 千葉県   千葉県   千葉県  \n4 和歌山県  3.97 和歌山県 和歌山県 和歌山県\n5 埼玉県    3.64 埼玉県   埼玉県   埼玉県  \n6 大阪府    3.77 大阪府   大阪府   大阪府  \n7 奈良県    3.85 奈良県   奈良県   奈良県  \n8 東京都    3.67 東京都   東京都   東京都  \n9 神奈川県  3.53 神奈川県 神奈川県 神奈川県\n\nlevels(Score_df$Pref2)\n\n[1] \"埼玉県\"   \"京都府\"   \"千葉県\"   \"奈良県\"   \"東京都\"   \"大阪府\"   \"兵庫県\"  \n[8] \"和歌山県\" \"神奈川県\"\n\nlevels(Score_df$Pref3)\n\n[1] \"東京都\"   \"千葉県\"   \"埼玉県\"   \"兵庫県\"   \"大阪府\"   \"奈良県\"   \"和歌山県\"\n[8] \"京都府\"   \"神奈川県\"\n\nlevels(Score_df$Pref4)\n\n[1] \"京都府\"   \"奈良県\"   \"埼玉県\"   \"大阪府\"   \"兵庫県\"   \"東京都\"   \"神奈川県\"\n[8] \"和歌山県\" \"千葉県\"  \n\n\n　PrefからPref4まで同じように見えますが、水準の順番が異なります (Prefはcharacter型だから水準がありません)。\nfct_reorder(): 別の1変数の値を基準に水準の順番を変更する\n　fct_infreq()は出現頻度順に並び替える関数でしたが、それと似たような関数としてfct_reorder()があります。ただし、これは出現頻度を基準にするのではなく、ある変数の平均値が低い順、中央値が高い順などでソートされます。まずは使い方から確認します。\nデータ名 %>%\n  mutate(新しい変数名 = fct_reorder(元の変数名, 基準となる変数, \n                                   関数名, 関数の引数))\n　必要な引数が多いですね。解説よりも実際の例を見ながら説明します。今回もPrefをfactor変数にし、Pref_Rという列で格納しますが、平均予算が安い順でfactorの水準を決めたいと思います。\n\ndf <- df %>%\n  mutate(Pref_R = fct_reorder(Pref, Budget, mean, na.rm = TRUE))\n\nlevels(df$Pref_R)\n\n[1] \"千葉県\"   \"埼玉県\"   \"奈良県\"   \"兵庫県\"   \"大阪府\"   \"神奈川県\" \"和歌山県\"\n[8] \"東京都\"   \"京都府\"  \n\n\n　Pref_Rの水準は千葉県、埼玉県、奈良県、…の順ですが、本当にそうでしょうか。group_by()とsummarise()などを使って確認してみましょう。\n\ndf %>% \n  group_by(Pref) %>%\n  summarise(Budget  = mean(Budget, na.rm = TRUE),\n            .groups = \"drop\") %>%\n  arrange(Budget)\n\n# A tibble: 9 × 2\n  Pref     Budget\n  <chr>     <dbl>\n1 千葉県    1124.\n2 埼玉県    1147.\n3 奈良県    1169.\n4 兵庫県    1197.\n5 大阪府    1203.\n6 神奈川県  1239.\n7 和歌山県  1252 \n8 東京都    1283.\n9 京都府    1399.\n\n\n　問題なくソートされましたね。注意点としてはfct_reorder()内に関数名を書く際、()は不要という点です。関数名の次の引数としてはその関数に別途必要な引数を指定します。引数が省略可能、あるいは不要な関数を使う場合は、省略しても構いませんし、数に制限はありません。\n　また、低い順ではなく、高い順にすることも可能です。次はScoreの中央値が高い順に水準を設定したPref_R2を作ってみましょう。\n\ndf <- df %>%\n  mutate(Pref_R2 = fct_reorder(Pref, Score, median, na.rm = TRUE, .desc = TRUE))\n\nlevels(df$Pref_R2)\n\n[1] \"和歌山県\" \"奈良県\"   \"千葉県\"   \"大阪府\"   \"東京都\"   \"埼玉県\"   \"京都府\"  \n[8] \"兵庫県\"   \"神奈川県\"\n\n\n　変わったのはmeanの代わりにmedianを使ったこと、そして.desc引数が追加された点です。fct_reorder()には.desc = FALSEがデフォルトとして指定されており、省略した場合は昇順でfactorの水準が決まります。ここで.desc = TRUEを指定すると、降順となります。実際、Scoreの中央値順になっているかを確認してみましょう。\n\ndf %>% \n  group_by(Pref) %>%\n  summarise(Score   = median(Score, na.rm = TRUE),\n            .groups = \"drop\") %>%\n  arrange(desc(Score))\n\n# A tibble: 9 × 2\n  Pref     Score\n  <chr>    <dbl>\n1 和歌山県  4   \n2 奈良県    3.88\n3 千葉県    3.75\n4 大阪府    3.75\n5 東京都    3.64\n6 埼玉県    3.61\n7 京都府    3.5 \n8 兵庫県    3.5 \n9 神奈川県  3.5 \n\n\nfct_reorder2(): 別の2変数の値を基準に水準の順番を変更する\n　この関数は別の変数を基準に水準が調整される点ではfct_reorder()と類似しています。ただし、よく誤解されるのは「変数Aの値が同じなら変数Bを基準に…」といったものではありません。たとえば、fct_reorder(x, y, mean)の場合、yの平均値 (mean())の順でxの水準を調整するという意味です。このmean()関数に必要なデータはベクトル1つです。しかし、関数によっては2つの変数が必要な場合があります。\n　これは頻繁に直面する問題ではありませんが、このfct_reorder2()関数が活躍するケースを紹介します。以下は6月27日から7月1日までの5日間、5地域におけるCOVID-19新規感染者数を表したデータです5。入力が面倒な方はここからダウンロードして読み込んでください。\n\n# 入力が面倒ならデータをダウンロードし、\n# Reorder2_df <- read_csv(\"Data/COVID19.csv\")\nReorder2_df <- tibble(\n  Country = rep(c(\"日本\", \"韓国\", \"中国 (本土)\", \"台湾\", \"香港\"),\n                each = 5),\n  Date    = rep(c(\"2020/06/27\", \"2020/06/28\", \"2020/06/29\",\n                  \"2020/06/30\", \"2020/07/01\"), 5),\n  NewPat  = c(100, 93, 86, 117, 130, \n               62, 42, 43,  50,  54,\n               17, 12, 19,   3,   5,\n                0,  0,  0,   0,   0,\n                1,  2,  4,   2,  28)\n)\n\nReorder2_df <- Reorder2_df %>%\n  mutate(Date = as.Date(Date))\n\nReorder2_df\n\n# A tibble: 25 × 3\n   Country Date       NewPat\n   <chr>   <date>      <dbl>\n 1 日本    2020-06-27    100\n 2 日本    2020-06-28     93\n 3 日本    2020-06-29     86\n 4 日本    2020-06-30    117\n 5 日本    2020-07-01    130\n 6 韓国    2020-06-27     62\n 7 韓国    2020-06-28     42\n 8 韓国    2020-06-29     43\n 9 韓国    2020-06-30     50\n10 韓国    2020-07-01     54\n# … with 15 more rows\n\n\n　可視化のコードはとりあえず無視し、グラフを出力してみましょう。\n\nReorder2_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = NewPat, color = Country),\n            size = 1) +\n  scale_x_date(date_labels = \"%Y年%m月%d日\") +\n  labs(x = \"年月日\", y = \"新規感染者数 (人)\", color = \"\") +\n  theme_gray(base_family = \"HiraKakuProN-W3\")\n\n\n\n\n\n\n\n\n　このグラフに違和感はあまりありませんが、「読みやすさ」の麺では改善の余地があります。たとえば、7月1日の時点で、新規感染者数が多いのは日本、韓国、香港、中国 (本土)、台湾の順です。しかし、右側の凡例の順番はそうではありません。この順番が一致すれば、更に図は読みやすくなるでしょう。\n\nfactor(Reorder2_df$Country)\n\n [1] 日本        日本        日本        日本        日本        韓国       \n [7] 韓国        韓国        韓国        韓国        中国 (本土) 中国 (本土)\n[13] 中国 (本土) 中国 (本土) 中国 (本土) 台湾        台湾        台湾       \n[19] 台湾        台湾        香港        香港        香港        香港       \n[25] 香港       \nLevels: 中国 (本土) 台湾 日本 韓国 香港\n\n\n　実際、何も指定せずにReorder2_dfのCountryをfactor化すると、韓国、香港、台湾、…の順であり、これは上のグラフと一致します。これをグラフにおける7月1日の新規感染者数の順で並べるためには、Dateを昇順にソートし、そして最後の要素 (\"2020/07/01\")内で新規感染者数 (NewPat)を降順に並べ替えた場合の順番にする必要があります。実際、Reorder2_dfをDateで昇順、NewPatで降順にソートし、最後の5行を抽出した結果が以下のコードです。\n\nReorder2_df %>%\n  arrange(Date, desc(NewPat)) %>%\n  slice_tail(n = 5)\n\n# A tibble: 5 × 3\n  Country     Date       NewPat\n  <chr>       <date>      <dbl>\n1 日本        2020-07-01    130\n2 韓国        2020-07-01     54\n3 香港        2020-07-01     28\n4 中国 (本土) 2020-07-01      5\n5 台湾        2020-07-01      0\n\n\n　このように、水準を調整する際に2つの変数 (DateとNewPat)が使用されます。fct_reorder2()はfct_reorder()と買い方がほぼ同じですが、基準となる変数がもう一つ加わります。\nデータ名 %>%\n  mutate(新しい変数名 = fct_reorder2(元の変数名, \n                                    基準となる変数1, 基準となる変数2,\n                                    関数名, 関数の引数))\n　重要なのはここの関数のところですが、fct_reorder2()はデフォルトでlast2()という関数が指定されており、まさに私たちに必要な関数です。したがって、ここでは関数名も省略できますが、ここでは一応明記しておきます。\n\nReorder2_df <- Reorder2_df %>%\n  mutate(Country2 = fct_reorder2(Country, Date, NewPat, last2)) \n\n　それでは新しく出来たCountry2の水準を確認してみましょう。\n\nlevels(Reorder2_df$Country2)\n\n[1] \"日本\"        \"韓国\"        \"香港\"        \"中国 (本土)\" \"台湾\"       \n\n\n　ちゃんと7月1日の新規感染者数基準で水準の順番が調整されましたので、これを使ってグラフをもう一回作ってみます。\n\nReorder2_df %>%\n  ggplot() +\n  geom_line(aes(x = Date, y = NewPat, color = Country2),\n            size = 1) +\n  scale_x_date(date_labels = \"%Y年%m月%d日\") +\n  labs(x = \"年月日\", y = \"新規感染者数 (人)\", color = \"\") +\n  theme_gray(base_family = \"HiraKakuProN-W3\")\n\n\n\n\n\n\n\n\n　これで図がさらに読みやすくなりました。ちなみに、{forcats}パッケージはlast2()以外にもfirst2()という関数も提供しております。これを使うと、7月1日でなく、6月27日の新規感染者数の降順で水準の順番が調整されます。他にも引数を2つ使用する自作関数も使えますが、fct_reorder2()の主な使いみちはlast2()で十分でしょう。\nfct_collapse(): 水準を統合する\n　水準数をより水準数に減らすためには、fct_recode()を使います。先ほど、fct_shift()で使ったdf4の例を考えてみましょう。df4のQ1の水準数は6つです。\n\nlevels(df4$Q1)\n\n[1] \"そう思う\"                     \"どちらかと言えばそう思う\"    \n[3] \"どちらとも言えない\"           \"どちらかと言えばそう思わない\"\n[5] \"そう思わない\"                 \"答えたくない\"                \n\n\n　これを4つに減らして見ましょう。具体的には「そう思う」と「どちらかと言えばそう思う」を「そう思う」に、「そう思わない」と「どちらかと言えばそう思わない」を「そう思わない」に統合します。これをfct_recode()で処理したのが以下のコードです。\n\n# fct_recode()を使った例\ndf4 <- df4 %>% \n    mutate(Q1_R2 = fct_recode(Q1,\n                              そう思う          = \"そう思う\",\n                              そう思う          = \"どちらかと言えばそう思う\",\n                              どちらとも言えない  = \"どちらとも言えない\",\n                              そう思わない       = \"どちらかと言えばそう思わない\",\n                              そう思わない       = \"そう思わない\",\n                              答えたくない       = \"答えたくない\"))\n\ndf4\n\n# A tibble: 10 × 4\n      ID Q1                           Q1_R                         Q1_R2        \n   <int> <fct>                        <fct>                        <fct>        \n 1     1 そう思う                     そう思う                     そう思う     \n 2     2 そう思わない                 そう思わない                 そう思わない \n 3     3 どちらとも言えない           どちらとも言えない           どちらとも言…\n 4     4 どちらかと言えばそう思う     どちらかと言えばそう思う     そう思う     \n 5     5 答えたくない                 答えたくない                 答えたくない \n 6     6 どちらかと言えばそう思う     どちらかと言えばそう思う     そう思う     \n 7     7 どちらかと言えばそう思わない どちらかと言えばそう思わない そう思わない \n 8     8 答えたくない                 答えたくない                 答えたくない \n 9     9 そう思わない                 そう思わない                 そう思わない \n10    10 そう思う                     そう思う                     そう思う     \n\nlevels(df4$Q1_R2)\n\n[1] \"そう思う\"           \"どちらとも言えない\" \"そう思わない\"      \n[4] \"答えたくない\"      \n\n\n　しかし、水準を統合するに特化したfct_collapse()を使えばより便利です。使い方は、fct_recode()に非常に似ているため省略しますが、=の右側をc()でまとめることが出来ます。\n\n# fct_collapse()を使った例\ndf4 <- df4 %>% \n    mutate(Q1_R3 = fct_collapse(Q1,\n                                そう思う = c(\"そう思う\", \"どちらかと言えばそう思う\"),\n                                どちらとも言えない = \"どちらとも言えない\",\n                                そう思わない = c( \"どちらかと言えばそう思わない\", \"そう思わない\"),\n                                答えたくない = \"答えたくない\"))\n\ndf4\n\n# A tibble: 10 × 5\n      ID Q1                           Q1_R                         Q1_R2   Q1_R3\n   <int> <fct>                        <fct>                        <fct>   <fct>\n 1     1 そう思う                     そう思う                     そう思… そう…\n 2     2 そう思わない                 そう思わない                 そう思… そう…\n 3     3 どちらとも言えない           どちらとも言えない           どちら… どち…\n 4     4 どちらかと言えばそう思う     どちらかと言えばそう思う     そう思… そう…\n 5     5 答えたくない                 答えたくない                 答えた… 答え…\n 6     6 どちらかと言えばそう思う     どちらかと言えばそう思う     そう思… そう…\n 7     7 どちらかと言えばそう思わない どちらかと言えばそう思わない そう思… そう…\n 8     8 答えたくない                 答えたくない                 答えた… 答え…\n 9     9 そう思わない                 そう思わない                 そう思… そう…\n10    10 そう思う                     そう思う                     そう思… そう…\n\nlevels(df4$Q1_R3)\n\n[1] \"そう思う\"           \"どちらとも言えない\" \"そう思わない\"      \n[4] \"答えたくない\"      \n\n\n　fct_recode()の結果と同じ結果が得られました。元の水準数や、減らされる水準数などによっては書く手間があまり変わらないので、好きな方を使っても良いでしょう。\nfct_drop(): 使われていない水準を除去する\n　水準としては存在するものの、データとしては存在しないケースもあります。これをここでは「空水準 (empty levels)」と呼びます。たとえば、以下のコードはPrefをfactor化してからPref == \"奈良県\"のケースを落としたものです。\n\nScore_df_f2 <- df %>%\n  mutate(Pref = fct_inorder(Pref)) %>%\n  filter(Pref != \"奈良県\") %>%\n  group_by(Pref) %>%\n  summarise(Score   = mean(Score, na.rm = TRUE),\n            .groups = \"drop\")\n\nScore_df_f2\n\n# A tibble: 8 × 2\n  Pref     Score\n  <fct>    <dbl>\n1 東京都    3.67\n2 神奈川県  3.53\n3 千葉県    3.72\n4 埼玉県    3.64\n5 大阪府    3.77\n6 京都府    3.68\n7 兵庫県    3.54\n8 和歌山県  3.97\n\n\n　このように結果としては、奈良県のデータを除外したため空水準である奈良県は表示されませんが、Pref変数はどうでしょうか。\n\nlevels(Score_df_f2$Pref)\n\n[1] \"東京都\"   \"神奈川県\" \"千葉県\"   \"埼玉県\"   \"大阪府\"   \"京都府\"   \"兵庫県\"  \n[8] \"奈良県\"   \"和歌山県\"\n\n\n　このように水準としては残っていることが分かります。使われていない水準が分析や可視化に影響を与えないケースもありますが、与えるケースもあります。これもこれまで勉強してきたfct_*()関数群で対応可能ですが、fct_drop()関数を使えば一発で終わります。実際にやってみましょう。\n\nScore_df_f2 <- Score_df_f2 %>%\n  mutate(Pref = fct_drop(Pref))\n\n\nlevels(Score_df_f2$Pref)\n\n[1] \"東京都\"   \"神奈川県\" \"千葉県\"   \"埼玉県\"   \"大阪府\"   \"京都府\"   \"兵庫県\"  \n[8] \"和歌山県\"\n\n\n　水準から奈良県が消えました。同じ機能をする関数としてはR内蔵関数であるdroplevels()関数があり、使い方はfct_drop()と同じです。\nfct_expand(): 水準を追加する\n　一方、空水準を追加することも可能です。fct_expand()関数には元の変数名に加え、追加する水準名を入れるだけです。たとえば、dfのPrefの水準は関東と関西の9都府県名となっていますが、ここに\"滋賀県\"という水準を追加してみます。。\n\ndf5 <- df %>%\n  mutate(Pref = fct_expand(Pref, \"滋賀県\"))\n\nlevels(df5$Pref)\n\n [1] \"京都府\"   \"兵庫県\"   \"千葉県\"   \"和歌山県\" \"埼玉県\"   \"大阪府\"  \n [7] \"奈良県\"   \"東京都\"   \"神奈川県\" \"滋賀県\"  \n\n\n　\"滋賀県\"という新しい水準が出来ましたね。ただし、新しく追加された水準は最後の順番になりますので、修正が必要な場合はfct_relevel()などを使って適宜修正してください。\n　新しく水準が追加されることによって、何かの変化はあるでしょうか。まずは都府県ごとにScoreの平均値とケース数を計算してみましょう。\n\ndf5 %>%\n  group_by(Pref) %>%\n  summarise(Score   = mean(Score, na.rm = TRUE),\n            N       = n(),\n            .groups = \"drop\")\n\n# A tibble: 9 × 3\n  Pref     Score     N\n  <fct>    <dbl> <int>\n1 京都府    3.68   414\n2 兵庫県    3.54   591\n3 千葉県    3.72  1000\n4 和歌山県  3.97   140\n5 埼玉県    3.64  1000\n6 大阪府    3.77  1000\n7 奈良県    3.85   147\n8 東京都    3.67  1000\n9 神奈川県  3.53  1000\n\n\n　見た目は全く変わらず、滋賀県の行が新しく出来たわけでもありません。{dplyr}のgroup_by()の場合、空水準はグループ化の対象になりません。一方、多くのR内蔵関数はケースとして存在しなくても計算の対象となります。たとえば、ベクトル内のある値が何個格納されているか確認するtable()関数の例を見てみましょう。\n\ntable(df5$Pref)\n\n\n  京都府   兵庫県   千葉県 和歌山県   埼玉県   大阪府   奈良県   東京都 \n     414      591     1000      140     1000     1000      147     1000 \n神奈川県   滋賀県 \n    1000        0 \n\n\n　\"滋賀県\"という列があり、合致するケースが0と表示されます。group_by()でも空の水準まで含めて出力する引数.dropがあります。デフォルトはTRUEですが、これをFALSEに指定してみます。\n\ndf5 %>%\n  group_by(Pref, .drop = FALSE) %>%\n  summarise(Score   = mean(Score, na.rm = TRUE),\n            N       = n(),\n            .groups = \"drop\")\n\n# A tibble: 10 × 3\n   Pref      Score     N\n   <fct>     <dbl> <int>\n 1 京都府     3.68   414\n 2 兵庫県     3.54   591\n 3 千葉県     3.72  1000\n 4 和歌山県   3.97   140\n 5 埼玉県     3.64  1000\n 6 大阪府     3.77  1000\n 7 奈良県     3.85   147\n 8 東京都     3.67  1000\n 9 神奈川県   3.53  1000\n10 滋賀県   NaN        0\n\n\n　空水準も出力され、Scoreの平均値は計算不可 (NaN)、ケース数は0という結果が得られました。\nfct_explicit_na(): 欠損値に水準を与える\n　まずは、実習用データdf6を作ってみまます。X1はnumeric型変数ですが、これをfactor化します。最初からtibble()内でfactor化しておいても問題ありませんが、練習だと思ってください。\n\ndf6 <- tibble(\n  ID = 1:10,\n  X1 = c(1, 3, 2, NA, 2, 2, 1, NA, 3, NA)\n)\n\ndf6 <- df6 %>%\n  mutate(X1 = factor(X1, \n                     levels = c(1, 2, 3),\n                     labels = c(\"ラーメン\", \"うどん\", \"そば\")))\n\ndf6\n\n# A tibble: 10 × 2\n      ID X1      \n   <int> <fct>   \n 1     1 ラーメン\n 2     2 そば    \n 3     3 うどん  \n 4     4 <NA>    \n 5     5 うどん  \n 6     6 うどん  \n 7     7 ラーメン\n 8     8 <NA>    \n 9     9 そば    \n10    10 <NA>    \n\n\n　それではX1をグループ化変数とし、ケース数を計算してみましょう。\n\ndf6 %>%\n  group_by(X1) %>%\n  summarise(N       = n(),\n            .groups = \"drop\")\n\n# A tibble: 4 × 2\n  X1           N\n  <fct>    <int>\n1 ラーメン     2\n2 うどん       3\n3 そば         2\n4 <NA>         3\n\n\n　NAもグループ化の対象となります。以下はこの欠損値も一つの水準として指定する方法について紹介します。欠損値を欠損値のままにするケースが多いですが、欠損値が何らかの意味を持つ場合、分析の対象になります。たとえば、多項ロジスティック回帰の応答変数として「分からない/答えたくない」を含めたり、「分からない/答えたくない」を選択する要因を分析したい場合は、欠損値に値を与える必要があります。なぜなら、一般的な分析において欠損値は分析対象から除外されるからです。\n　まずは、これまで紹介した関数を使ったやり方から紹介します。\n\ndf6 %>%\n         # まず、X1をcharacter型に変換し、X2という列に保存\n  mutate(X2 = as.character(X1),\n         # X2がNAなら\"欠損値\"、それ以外なら元のX2の値に置換\n         X2 = ifelse(is.na(X2), \"欠損値\", X2),\n         # X2を再度factor化する\n         X2 = factor(X2, \n                     levels = c(\"ラーメン\", \"うどん\", \"そば\", \"欠損値\")))\n\n# A tibble: 10 × 3\n      ID X1       X2      \n   <int> <fct>    <fct>   \n 1     1 ラーメン ラーメン\n 2     2 そば     そば    \n 3     3 うどん   うどん  \n 4     4 <NA>     欠損値  \n 5     5 うどん   うどん  \n 6     6 うどん   うどん  \n 7     7 ラーメン ラーメン\n 8     8 <NA>     欠損値  \n 9     9 そば     そば    \n10    10 <NA>     欠損値  \n\n\n　X1をcharacter型に戻す理由6は、水準にない値が入るとfactor化が解除されるからです。factor型をcharacter型に戻さずにdf6$X1のNAを\"欠損値\"に置換すると、以下のようになります。\n\n# df6のX1がNAなら\"欠損\"、それ以外なら元のX1の値を返す\nifelse(is.na(df6$X1), \"欠損値\", df6$X1)\n\n [1] \"1\"      \"3\"      \"2\"      \"欠損値\" \"2\"      \"2\"      \"1\"      \"欠損値\"\n [9] \"3\"      \"欠損値\"\n\n\n　\"ラーメン\"と\"うどん\"、\"そば\"がfactor化前の1, 2, 3に戻っただけでなく、NAが\"欠損値\"というcharacter型に置換されたため、全体がcharacter型に変換されました。このように欠損値に水準を与える作業は難しくはありませんが、面倒な作業です。そこで登場する関数がfct_exlpicit_na()関数です。使い方は、元の変数に加え、欠損値の水準名を指定するna_levelです。\n\ndf6 <- df6 %>%\n  # na_levelのデフォルト値は\"(Missing)\"\n  mutate(X2 = fct_explicit_na(X1, na_level = \"欠損値\"))\n\ndf6\n\n# A tibble: 10 × 3\n      ID X1       X2      \n   <int> <fct>    <fct>   \n 1     1 ラーメン ラーメン\n 2     2 そば     そば    \n 3     3 うどん   うどん  \n 4     4 <NA>     欠損値  \n 5     5 うどん   うどん  \n 6     6 うどん   うどん  \n 7     7 ラーメン ラーメン\n 8     8 <NA>     欠損値  \n 9     9 そば     そば    \n10    10 <NA>     欠損値  \n\n\n　欠損値が一つの水準になったことが分かります。\n\ndf6 %>%\n  group_by(X2) %>%\n  summarise(N       = n(),\n            .groups = \"drop\")\n\n# A tibble: 4 × 2\n  X2           N\n  <fct>    <int>\n1 ラーメン     2\n2 うどん       3\n3 そば         2\n4 欠損値       3\n\n\n　むろん、group_by()を使ってもちゃんと出力されます。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#行単位の操作",
    "href": "tutorial/R/dplyr_intro.html#行単位の操作",
    "title": "dplyr入門",
    "section": "行単位の操作",
    "text": "行単位の操作\n　ここでは行単位の操作について考えたいと思います。select()の説明で使ったmyDF1を見てみましょう。\n\nmyDF1\n\n# A tibble: 5 × 10\n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     2     3     4     5     3     8     3     1     9\n2     2     4     5     2     5     3     9     0     5     1\n3     3     6     1     1     6     2     5     3     9     3\n4     4     2     1     6     0     3     0     0     1     3\n5     5     7     0     9     2     1     1     2     3     8\n\n\n　ここでX1とX2とX3の平均値を計算し、X_Meanという名の変数にする場合、以下のような書き方が普通でしょう。\n\nmyDF1 %>%\n  mutate(X_Mean = mean(c(X1, X2, X3)))\n\n# A tibble: 5 × 11\n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D X_Mean\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1     1     2     3     4     5     3     8     3     1     9   3.13\n2     2     4     5     2     5     3     9     0     5     1   3.13\n3     3     6     1     1     6     2     5     3     9     3   3.13\n4     4     2     1     6     0     3     0     0     1     3   3.13\n5     5     7     0     9     2     1     1     2     3     8   3.13\n\n\n　あらら、なんかおかしくありませんか。1行目の場合、X1とX2、X3それぞれ2、5、3であり、平均値は3.333であるはずなのに3.133になりました。これは2行目以降も同じです。なぜでしょうか。\n　実は{dplyr}は行単位の計算が苦手です。実際、データフレーム (または、tibble)というのは既に説明したとおり、縦ベクトルを横に並べたものです。列をまたがる場合、データ型が異なる場合も多いため、そもそも使う場面も多くありません。したがって、以下のような書き方が必要でした。\n\nmyDF1 %>%\n  mutate(X_Mean = (X1 + X2 + X3) / 3)\n\n# A tibble: 5 × 11\n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D X_Mean\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1     1     2     3     4     5     3     8     3     1     9  3.33 \n2     2     4     5     2     5     3     9     0     5     1  3    \n3     3     6     1     1     6     2     5     3     9     3  5    \n4     4     2     1     6     0     3     0     0     1     3  0.667\n5     5     7     0     9     2     1     1     2     3     8  3.67 \n\n\n　先ほどのmean(c(X1, X2, X3))は(X1列とX2列、X3列)の平均値です。X1は長さ1のベクトルではなく、その列全体を指すものです。つまり、mean(c(X1, X2, X3))はmean(c(myD1F$X1, myDF1$X2, myDF1$X3))と同じことになります。だから全て3.133という結果が得られました。ただし、後者はベクトル同士の加減乗除になるため問題ありません。実際c(1, 2, 3) + c(3, 5, 0)は同じ位置の要素同士の計算になります。\n　ここでmean()関数を使う場合には全ての演算を、一行一行に分けて行う必要があります。ある一行のみに限定する場合、mean(c(X1, X2, X3))のX1などは長さ1のベクトルになるため、(X1 + X2 + X3) / 3と同じことになります。この「一行単位で処理を行う」ことを指定する関数がrowwise()関数です。これは行単位の作業を行う前に指定するだけです。\n\nmyDF1 %>%\n  rowwise() %>%\n  mutate(X_Mean = mean(c(X1, X2, X3)))\n\n# A tibble: 5 × 11\n# Rowwise: \n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D X_Mean\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1     1     2     3     4     5     3     8     3     1     9  3.33 \n2     2     4     5     2     5     3     9     0     5     1  3    \n3     3     6     1     1     6     2     5     3     9     3  5    \n4     4     2     1     6     0     3     0     0     1     3  0.667\n5     5     7     0     9     2     1     1     2     3     8  3.67 \n\n\n　これで問題なく行単位の処理ができるようになりました。今回は変数が3つのみだったので、これで問題ありませんが、変数が多くなると:やstarts_with()、num_range()などを使って変数を選択したくなります。この場合は計算する関数内にc_across()を入れます。ここではX1列からX3D列までの平均値を求めてみましょう。\n\nmyDF1 %>%\n  rowwise() %>%\n  mutate(X_Mean = mean(X1:X3D))\n\n# A tibble: 5 × 11\n# Rowwise: \n     ID    X1    Y1   X1D    X2    Y2   X2D    X3    Y3   X3D X_Mean\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1     1     2     3     4     5     3     8     3     1     9    5.5\n2     2     4     5     2     5     3     9     0     5     1    2.5\n3     3     6     1     1     6     2     5     3     9     3    4.5\n4     4     2     1     6     0     3     0     0     1     3    2.5\n5     5     7     0     9     2     1     1     2     3     8    7.5\n\n\n　実はrowwise()関数、2020年6月に公開された{dplyr} 1.0.0で注目されるようになった関数ですが、昔の{dplyr}にもrowwise()関数はありました。ただし、{purrr}パッケージや{tidyr}パッケージのnest()関数などにより使い道がなくなりましたが、なぜか華麗に復活しました。データ分析に使うデータは基本単位は列であるため、実際にrowwise()が使われる場面は今の段階では多くないでしょう。また、簡単な作業ならX1 + X2のような演算でも対応できます。それでも、覚えておけば便利な関数であることには間違いありません。"
  },
  {
    "objectID": "tutorial/R/dplyr_intro.html#データの結合",
    "href": "tutorial/R/dplyr_intro.html#データの結合",
    "title": "dplyr入門",
    "section": "データの結合",
    "text": "データの結合\n\n行の結合\n　まずは、複数のデータフレームまたはtibbleを縦に結合する方法について解説します。イメージとしては 図 9 のようなものです。\n\n\n\n\n\n図 9: 行の結合\n\n\n\n\n　行を結合する際には{dplyr}パッケージのbind_rows()関数を使います。この関数の使い方は以下の通りです。\n# 新しいデータ名ではなく、既にあるデータ名にすると上書きとなる\n新しいデータ名 <-  bind_rows(データ1, データ2, ...)\n　それでは早速実際に使ってみましょう。実習のために、4つのtibbleを作成します (tibbleでなくデータフレームでも問題ありません)。\n\n# tibble()の代わりにdata.frame()も可\nrbind_df1 <- tibble(X1 = 1:3,\n                    X2 = c(\"A\", \"B\", \"C\"),\n                    X3 = c(T, T, F)) # TRUEとFALSEはTはFと省略可能\n\nrbind_df2 <- tibble(X1 = 4:6,\n                    X2 = c(\"D\", \"E\", \"F\"),\n                    X3 = c(F, T, F))\n\nrbind_df3 <- tibble(X1 = 7:9,\n                    X3 = c(T, T, T),\n                    X2 = c(\"G\", \"H\", \"I\"))\n\nrbind_df4 <- tibble(X1 = 10:12,\n                    X2 = c(\"J\", \"K\", \"L\"),\n                    X5 = c(\"Song\", \"Yanai\", \"Hadley\"))\n\nrbind_df1 # rbind_df1を出力\n\n# A tibble: 3 × 3\n     X1 X2    X3   \n  <int> <chr> <lgl>\n1     1 A     TRUE \n2     2 B     TRUE \n3     3 C     FALSE\n\nrbind_df2 # rbind_df2を出力\n\n# A tibble: 3 × 3\n     X1 X2    X3   \n  <int> <chr> <lgl>\n1     4 D     FALSE\n2     5 E     TRUE \n3     6 F     FALSE\n\nrbind_df3 # rbind_df3を出力\n\n# A tibble: 3 × 3\n     X1 X3    X2   \n  <int> <lgl> <chr>\n1     7 TRUE  G    \n2     8 TRUE  H    \n3     9 TRUE  I    \n\nrbind_df4 # rbind_df4を出力\n\n# A tibble: 3 × 3\n     X1 X2    X5    \n  <int> <chr> <chr> \n1    10 J     Song  \n2    11 K     Yanai \n3    12 L     Hadley\n\n\n　まずは、rbind_df1とrbind_df2を結合してみます。この2つのデータは同じ変数が同じ順番で並んでいますね。\n\nBinded_df1 <- bind_rows(rbind_df1, rbind_df2)\nBinded_df1\n\n# A tibble: 6 × 3\n     X1 X2    X3   \n  <int> <chr> <lgl>\n1     1 A     TRUE \n2     2 B     TRUE \n3     3 C     FALSE\n4     4 D     FALSE\n5     5 E     TRUE \n6     6 F     FALSE\n\n\n　2つのデータが結合されたことが確認できます。それではrbind_df1とrbind_df2、rbind_df3はどうでしょうか。確かに3つのデータは同じ変数を持ちますが、rbind_df3は変数の順番がX1、X3、X2になっています。このまま結合するとエラーが出るでしょうか。とりあえず、やってみます。\n\nBinded_df2 <- bind_rows(rbind_df1, rbind_df2, rbind_df3)\nBinded_df2\n\n# A tibble: 9 × 3\n     X1 X2    X3   \n  <int> <chr> <lgl>\n1     1 A     TRUE \n2     2 B     TRUE \n3     3 C     FALSE\n4     4 D     FALSE\n5     5 E     TRUE \n6     6 F     FALSE\n7     7 G     TRUE \n8     8 H     TRUE \n9     9 I     TRUE \n\n\n　このように変数の順番が異なっても、先に指定したデータの変数順で問題なく結合できました。これまでの作業は{dplyr}パッケージのbind_rows()を使わずに、R内蔵関数のrbind()でも同じやり方でできます。bind_rows()の特徴は、変数名が一致しない場合、つまり今回の例だとrbind_df4が含まれる場合です。rbind_df1からrbind_df3までは順番が違ってもX1、X2、X3変数で構成されていました。一方、rbind_dr4にはX3がなく、新たにX4という変数があります。これをrbind()関数で結合するとエラーが出力されます。\n\n# rbind()を使う場合\nrbind(rbind_df1, rbind_df2, rbind_df3, rbind_df4)\n\nError in match.names(clabs, names(xi)): names do not match previous names\n\n\n　一方、bind_rows()はどうでしょうか。\n\nBinded_df3 <- bind_rows(rbind_df1, rbind_df2, rbind_df3, rbind_df4)\nBinded_df3\n\n# A tibble: 12 × 4\n      X1 X2    X3    X5    \n   <int> <chr> <lgl> <chr> \n 1     1 A     TRUE  <NA>  \n 2     2 B     TRUE  <NA>  \n 3     3 C     FALSE <NA>  \n 4     4 D     FALSE <NA>  \n 5     5 E     TRUE  <NA>  \n 6     6 F     FALSE <NA>  \n 7     7 G     TRUE  <NA>  \n 8     8 H     TRUE  <NA>  \n 9     9 I     TRUE  <NA>  \n10    10 J     NA    Song  \n11    11 K     NA    Yanai \n12    12 L     NA    Hadley\n\n\n　X1からX4まで全ての列が生成され、元のデータにはなかった列に関してはNAで埋められています。\n　ならば、bind_rows()の完全勝利かというと、そうとは限りません。自分で架空した複数のデータフレーム、またはtibbleを結合する際、「このデータは全て同じ変数を持っているはず」と事前に分かっているならrbind()の方が効果的です。なぜなら、変数名が異なる場合、エラーが出力されるからです。bind_rows()を使うと、コーディングミスなどにより、列名の相違がある場合でも結合してくれてしまうので、分析の結果を歪ませる可能性があります。\n\n\n列の結合\n　実はデータ分析においてデータの結合といえば、列の結合が一般的です。これは 図 10 のような操作を意味します。\n\n\n\n\n\n図 10: 列の結合\n\n\n\n\n　まずは、本章で作成したdf2をもう一回作ってみます。\n\ndf2 <- df %>%\n  group_by(Pref) %>%\n  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),\n            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),\n            Score_Mean  = mean(Score, na.rm = TRUE),\n            N           = n(),\n            .groups     = \"drop\")\n\ndf2\n\n# A tibble: 9 × 5\n  Pref     Budget_Mean ScoreN_Sum Score_Mean     N\n  <chr>          <dbl>      <dbl>      <dbl> <int>\n1 京都府         1399.        216       3.68   414\n2 兵庫県         1197.        230       3.54   591\n3 千葉県         1124.        259       3.72  1000\n4 和歌山県       1252          83       3.97   140\n5 埼玉県         1147.        278       3.64  1000\n6 大阪府         1203.        516       3.77  1000\n7 奈良県         1169.         45       3.85   147\n8 東京都         1283.       1165       3.67  1000\n9 神奈川県       1239.        587       3.53  1000\n\n\n　ラーメン屋の店舗ですが、たしかにデータには埼玉、東京、大阪などは1000店舗しか入っておりません。実はもっと多いですが、ぐるなびAPIの仕様上、最大1000店舗しか情報取得が出来ないからです。ここに実際の店舗数が入っている新しいデータセット、Ramen2.csvがあります。これを読み込み、df3という名で格納しましょう。\n\n# データのパスは適宜修正すること\ndf3 <- read_csv(\"Data/Ramen2.csv\")\n\ndf3\n\n# A tibble: 47 × 15\n   Pref      Pop   Area RamenN Turnout   LDP   CDP  DPFP Komei   JIP   JCP   SDP\n   <chr>   <dbl>  <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 北海道 5.38e6 83424.   1454    53.8  32.3  20.8  6.65 11.7   7.78 11.6   1.31\n 2 青森県 1.31e6  9646.    336    42.9  39.8  22.0  7.24 11.3   3.4   8.31  2.36\n 3 岩手県 1.28e6 15275.    285    56.5  35.5  17.8 12.5   8.22  4.36 10.4   3.83\n 4 宮城県 2.33e6  7282.    557    51.2  39.6  17.8  9.02 11.1   4.6   7.89  2.1 \n 5 秋田県 1.02e6 11638.    301    56.3  44.5  13.5  8.64 10.6   4.48  8.09  3.77\n 6 山形県 1.12e6  9323.    512    60.7  45.2  14.9  7.37  9.87  4.28  6.51  5.08\n 7 福島県 1.91e6 13784.    550    52.4  38.2  13.6 12.1  12.8   5.31  7.99  3.01\n 8 茨城県 2.92e6  6097.    663    45.0  39.3  15.2  7.15 15.1   6.73  7.73  1.46\n 9 栃木県 1.97e6  6408.    595    44.1  40.3  18.9  9.94 12.8   4.9   5.04  1.03\n10 群馬県 1.97e6  6362.    488    48.2  40.6  16.4  9.76 12.4   4.67  7.58  1.87\n# … with 37 more rows, and 3 more variables: Reiwa <dbl>, NHK <dbl>, HRP <dbl>\n\n\n\n\n\n\n\n\n  \n  \n    \n      変数名\n      説明\n    \n  \n  \n    Pref\n都道府県名\n    Pop\n日本人人口 (2015年国勢調査)\n    Area\n面積 (2015年国勢調査)\n    RamenN\nぐるなびに登録されたラーメン屋の店舗数\n    Turnout\n2019年参院選: 投票率 (比例)\n    LDP\n2019年参院選: 自民党の得票率 (比例)\n    CDP\n2019年参院選: 立憲民主党の得票率 (比例)\n    DPFP\n2019年参院選: 国民民主党の得票率 (比例)\n    Komei\n2019年参院選: 公明党の得票率 (比例)\n    JIP\n2019年参院選: 日本維新の会の得票率 (比例)\n    JCP\n2019年参院選: 日本共産党の得票率 (比例)\n    SDP\n2019年参院選: 社会民主党の得票率 (比例)\n    Reiwa\n2019年参院選: れいわ新選組の得票率 (比例)\n    NHK\n2019年参院選: NHKから国民を守る党の得票率 (比例)\n    HRP\n2019年参院選: 幸福実現党の得票率 (比例)\n  \n  \n  \n\n\n\n\n　本データは都道府県ごとの人口、面積、ぐるなびに登録されたラーメン屋の店舗数、2019年参議院議員通常選挙の結果が格納されています。人口と面積は2015年国勢調査、ぐるなびの情報は2020年6月時点での情報です。\n　df2にデータ上の店舗数ではなく、実際の店舗数を新しい列として追加したい場合はどうすれば良いでしょうか。簡単な方法としてはdf3から情報を取得し、それを自分で入れる方法です。\n\ndf3 %>%\n  # df2のPrefベクトルの要素と一致するものに絞る\n  filter(Pref %in% df2$Pref) %>%\n  # 都道府県名とラーメン屋の店舗数のみ抽出\n  select(Pref, RamenN)\n\n# A tibble: 9 × 2\n  Pref     RamenN\n  <chr>     <dbl>\n1 埼玉県     1106\n2 千葉県     1098\n3 東京都     3220\n4 神奈川県   1254\n5 京都府      415\n6 大阪府     1325\n7 兵庫県      591\n8 奈良県      147\n9 和歌山県    140\n\n\n　そして、この情報をdf2$RamenN <- c(415, 1106, 1254, ...)のように追加すればいいですね。\n　しかし、このような方法は非効率的です。そもそもdf3から得られた結果の順番とdf2の順番も一致しないので、一々対照しながらベクトルを作ることになります。ここで登場する関数が{dplyr}の*_join()関数群です。この関数群には4つの関数が含まれており、以下のような使い方になります。\n# 新しいデータ名ではなく、データ1またはデータ2の名前に格納すると上書きとなる\n\n# 1. データ1を基準に結合\n新しいデータ名 <-  left_join(データ1, データ2, by = \"共通変数名\")\n\n# 2. データ2を基準に結合\n新しいデータ名 <- right_join(データ1, データ2, by = \"共通変数名\")\n\n# 3. データ1とデータ2両方に共通するケースのみ結合\n新しいデータ名 <- inner_join(データ1, データ2, by = \"共通変数名\")\n\n# 4. データ1とデータ2、どれかに存在するケースを結合\n新しいデータ名 <-  full_join(データ1, データ2, by = \"共通変数名\")\n　4つの関数の違いについて説明する前に、by引数について話したいと思います。これは主にキー (key)変数と呼ばれる変数で、それぞれのデータに同じ名前の変数がある必要があります。df2とdf3だとそれがPref変数です。どの*_join()関数でも、Prefの値が同じもの同士を結合することになります。\n　データのキー変数名が異なる場合もあります。たとえば、データ1の都道府県名はPrefという列に、データ2の都道府県名はPrefectureという列になっている場合、by = \"Pref\"でなく、by = c(\"データ1のキー変数名\" = \"データ2のキー変数名\")、つまり、by = c(\"Pref\" = \"Prefecture\")と指定します。\n　それでは、df3から都道府県名とラーメン屋の店舗数だけ抽出し、df4として格納しておきます。\n\ndf4 <- df3 %>%\n  select(Pref, RamenN)\n\ndf4\n\n# A tibble: 47 × 2\n   Pref   RamenN\n   <chr>   <dbl>\n 1 北海道   1454\n 2 青森県    336\n 3 岩手県    285\n 4 宮城県    557\n 5 秋田県    301\n 6 山形県    512\n 7 福島県    550\n 8 茨城県    663\n 9 栃木県    595\n10 群馬県    488\n# … with 37 more rows\n\n\n　これから共通変数名の値をキー (key)と呼びます。今回の例だとPrefがdf2とdf4のキー変数であり、その値である\"東京都\"、\"北海道\"などがキーです。\n　まずは、inner_join()の仕組みについて考えます。これはdf2とdf4に共通するキーを持つケースのみ結合する関数です。df4には\"北海道\"というキーがありますが、df2にはありません。したがって、キーが\"北海道\"のケースは結合から除外されます。これをイメージにしたものが@fig-inner-joinです7。それぞれ3 \\(\\times\\) 2 (3行2列)のデータですが、キーが一致するケースは2つしかないため、結合後のデータは3 \\(\\times\\) 2となります。\n\n\n\n\n\n図 11: inner_join()の仕組み\n\n\n\n\n　実際にやってみましょう。\n\ninner_join(df2, df4, by = \"Pref\")\n\n# A tibble: 9 × 6\n  Pref     Budget_Mean ScoreN_Sum Score_Mean     N RamenN\n  <chr>          <dbl>      <dbl>      <dbl> <int>  <dbl>\n1 京都府         1399.        216       3.68   414    415\n2 兵庫県         1197.        230       3.54   591    591\n3 千葉県         1124.        259       3.72  1000   1098\n4 和歌山県       1252          83       3.97   140    140\n5 埼玉県         1147.        278       3.64  1000   1106\n6 大阪府         1203.        516       3.77  1000   1325\n7 奈良県         1169.         45       3.85   147    147\n8 東京都         1283.       1165       3.67  1000   3220\n9 神奈川県       1239.        587       3.53  1000   1254\n\n\n　共通するキーは9つのみであり、結果として返されたデータの大きさも9 \\(\\times\\) 6です。df2に足されたdf4は2列のデータですが、キー変数であるPrefは共通するため、1列のみ足されました。キー変数を両方残す場合はkeep = TRUE引数を追加してください。\n　一方、full_join()は、すべてのキーに対して結合を行います (図 12)。たとえば、df2には\"北海道\"というキーがありません。それでも新しく出来上がるデータには北海道の列が追加されます。ただし、道内店舗の平均予算、口コミ数などの情報はないため、欠損値が代入されます。\n\n\n\n\n\n図 12: full_join()の仕組み\n\n\n\n\n　それでは実際、結果を確認してみましょう。今回は結合後、RamenNが大きい順で出力します。\n\nfull_join(df2, df4, by = \"Pref\") %>%\n  arrange(desc(RamenN)) # ぐるなびに登録された店舗の多い都道府県から出力\n\n# A tibble: 47 × 6\n   Pref     Budget_Mean ScoreN_Sum Score_Mean     N RamenN\n   <chr>          <dbl>      <dbl>      <dbl> <int>  <dbl>\n 1 東京都         1283.       1165       3.67  1000   3220\n 2 北海道           NA          NA      NA       NA   1454\n 3 大阪府         1203.        516       3.77  1000   1325\n 4 愛知県           NA          NA      NA       NA   1255\n 5 神奈川県       1239.        587       3.53  1000   1254\n 6 埼玉県         1147.        278       3.64  1000   1106\n 7 千葉県         1124.        259       3.72  1000   1098\n 8 福岡県           NA          NA      NA       NA    985\n 9 新潟県           NA          NA      NA       NA    705\n10 静岡県           NA          NA      NA       NA    679\n# … with 37 more rows\n\n\n　df2にはなかった北海道や愛知県などの行ができました。そして、df2にはない情報はすべて欠損値 (NA)となりました。\n　続いて、left_join()ですが、これは先に指定したデータに存在するキーのみで結合を行います (図 13)。今回はdf2が先に指定されていますが、df2のキーはdf4のキーの部分集合であるため、inner_join()と同じ結果が得られます。\n\n\n\n\n\n図 13: left_join()の仕組み\n\n\n\n\n　一方、right_join()はleft_join()と逆の関数であり、後に指定したデータに存在するキーを基準に結合を行います (図 14)。後に指定されたdf4のキーはdf2のキーを完全に含むので、full_join()と同じ結果が得られます。\n\n\n\n\n\n図 14: right_join()の仕組み\n\n\n\n\n　これからはdf2とdf4を結合することになりますが、この2つのtibbleの大きさが異なります。df2は9つの都府県のみであるに対し、df4は47都道府県全てのデータが入っているからです。\n　ここまではキー変数が一つである場合についてのみ考えましたが、複数のキー変数が必要な場合もあります。たとえば、市区町村の人口・面積データと市区町村の投票率データを結合するとします。各自治体に与えられている「全国地方公共団体コード」が両データに含まれている場合は、このコードをキー変数として使えば問題ありませんが、市区町村名をキー変数として使わざる得ないケースもあるでしょう。しかし、キー変数が複数ある場合もあります。たとえば、府中市は東京都と広島県にありますし、太子町は大阪府と兵庫県にあります。この場合、市区町村名のみでケースをマッチングすると、重複されてマッチングされる恐れがあります。この場合はキー変数を増やすことで対処できます。たとえば、同じ都道府県なら同じ市区町村は存在しないでしょう8。キー変数を複数指定する方法は簡単です。たとえば、市区町村名変数がMunip、都道府県名変数がPrefならby = c(\"Munip\", \"Pref\")と指定するだけです。\n　最後に、キー変数以外の変数名が重複する場合について考えましょう。これはパネルデータを結合する時によく直面する問題です。同じ回答者に2回の調査を行った場合、回答者のIDでデータを結合することになります。ただし、それぞれのデータにおいて回答者の性別に関する変数がF1という名前の場合、どうなるでしょうか。同じデータの同じ名前の変数が複数あると、非常に扱いにくくなります。実際の結果を見てみましょう。\n\nWave1_df <- tibble(ID = c(1, 2, 3, 4, 5),\n                   F1 = c(1, 1, 0, 0, 1),\n                   F2 = c(18, 77, 37, 50, 41),\n                   Q1 = c(1, 5, 2, 2, 3))\n\nWave2_df <- tibble(ID = c(1, 3, 4, 6, 7),\n                   F1 = c(1, 0, 0, 0, 1),\n                   F2 = c(18, 37, 50, 20, 62),\n                   Q1 = c(1, 2, 2, 5, 4))\n\nfull_join(Wave1_df, Wave2_df, by = \"ID\")\n\n# A tibble: 7 × 7\n     ID  F1.x  F2.x  Q1.x  F1.y  F2.y  Q1.y\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     1    18     1     1    18     1\n2     2     1    77     5    NA    NA    NA\n3     3     0    37     2     0    37     2\n4     4     0    50     2     0    50     2\n5     5     1    41     3    NA    NA    NA\n6     6    NA    NA    NA     0    20     5\n7     7    NA    NA    NA     1    62     4\n\n\n　それぞれの変数名の後に.xと.yが付きます。この接尾辞 (suffix)はsuffix引数を指定することで、分析側からカスタマイズ可能です。たとえば、接尾辞を_W1、_W2にしたい場合は\n\nfull_join(Wave1_df, Wave2_df, by = \"ID\", suffix = c(\"_W1\", \"_W2\"))\n\n# A tibble: 7 × 7\n     ID F1_W1 F2_W1 Q1_W1 F1_W2 F2_W2 Q1_W2\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     1    18     1     1    18     1\n2     2     1    77     5    NA    NA    NA\n3     3     0    37     2     0    37     2\n4     4     0    50     2     0    50     2\n5     5     1    41     3    NA    NA    NA\n6     6    NA    NA    NA     0    20     5\n7     7    NA    NA    NA     1    62     4\n\n\nのように、データ1とデータ2それぞれの接尾辞を指定するだけです。"
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html",
    "href": "tutorial/R/rmarkdown_intro.html",
    "title": "R Markdown入門",
    "section": "",
    "text": "R Markdownとは名前通り、RとMarkdownが結合されたものです。本書を通じてRを紹介してきましたが、Markdownは初めて出てくるものです。John GruberとAaron Swartzが2004年提案した軽量Markup言語ですが、そもそもMarkup言語とはなんでしょうか。\n　Markup言語を知るためにはプレーンテキスト（plain text）とリッチテキスト（rich text、またはマルチスタイルテキスト）の違いについて知る必要があります。プレーンテキストとは書式情報などが含まれていない純粋なテキストのみで構成されている文書です。書式情報とは文書の余白、文字の大きさ、文字の色などがあります。これまでRStudio上で書いてきたRコードもプレーンテキストです。コードに色が自動的に付けられますが、これはRStudioが色付けをしてくれただけで、ファイル自体はテキストのみで構成されています。macOSのTextEdit、Windowsのメモ帳、Linux GUI環境のgeditやKATE、CLI環境下のvim、Emacs、nanoなどで作成したファイルは全てプレーンテキストです。これらのテキストエディターには書式設定や図表の挿入などの機能は付いておりません。一方、リッチテキストとは書式情報だけでなく、図表なども含まれる文書です。Microsoft Wordとかで作成したファイルがその代表的な例です。他にもPagesやLibreOffice Writerから作成したファイルなどがあります。これらのワードプロセッサーソフトウェアは書式の設定や図表・リンクの挿入などができます。そして、Markup言語とはプレーンテキストのみでリッチテキストを作成するための言語です。\n　Markup言語の代表的な存在がHTMLです。そもそもHTMLのMLはMarkup Languageの略です。読者の皆さんがウェブブラウザから見る画面のほとんどはHTMLで書かれています。この『私たちのR』もHTMLです。この文書には図表があり、太字、見出し、水平線など、テキスト以外の情報が含んでいます。しかし、HTMLは純粋なテキストのみで書かれており、ウェブブラウザ（Firefox、Chrome、Edgeなど）がテキストファイルを読み込み、解釈して書式が付いている画面を出力してくれます。例えば、リンク（hyperlink）について考えてみましょう。「SONGのHP」をクリックすると宋のホームページに移動します。ある単語をクリックすることで、他のウェブサイトへ飛ばす機能を付けるためにはHTMLファイルの中に、以下のように入力します。\n<a href=\"https://www.jaysong.net\">SONGのHP</a>\n　これをウェブブラウザが自動的に「SONGのHP」と変換し、画面に出力してくれます。これは、書式情報などが必要な箇所にコードを埋め込むことによって実現されます。そして、このMarkup言語をより単純な文法で再構成したものがMarkdownです。例えば、以上のHTMLはMarkdownでは以下のように書きます。\n[SONGのHP](https://www.jaysong.net)\n　同じ意味のコードですが、Markdownの方がより簡潔に書けます。このMarkdownは最終的にHTMLやMicrosoft Word、PDF形式で変換されます。一般的にはHTML出力を使いますが、自分のPCにLaTeX環境が用意されている場合はPDF形式で出力することも可能であり、個人的には推奨しておりませんが、Microsoft Work文書ファイルへ変換することも可能です。また、HTML（+ JavaScript）へ出力可能であることを利用し、スライドショー、e-Book、ホームページの作成にもMarkdownが使えます。\n　R MarkdownはMarkdownにRコードとその結果を同時に載せることができるMarkdownです。それでもピンと来ない方も多いでしょう。それでは、とりあえず、R Markdownをサンプルコードから体験してみましょう。"
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html#とりあえずknit",
    "href": "tutorial/R/rmarkdown_intro.html#とりあえずknit",
    "title": "R Markdown入門",
    "section": "とりあえずKnit",
    "text": "とりあえずKnit\n　R Markdownの文法について説明する前に、とりあえずR Markdownというのがどういうものかを味見してみます。既に述べたようにR MarkdownにはRコードも含まれるため、事前にプロジェクトを生成してから進めることをおすすめします。\n　R Markdownファイルを生成するにはFile -> New File -> R Markdown…を選択します。Title:とAuthor:には文書のタイトルと作成者を入力します。Default Output FormatはデフォルトはHTMLとなっていますが、このままにしておきましょう。ここでの設定はいつでも変更可能ですので、何も触らずにOKを押しても大丈夫です。\n　OKを押したら自動的にR Markdownのサンプルファイルが生成されます。そしたらSourceペインの上段にあるボタン1をクリックしてみましょう。最初はファイルの保存ダイアログが表示されますが、適切な場所（プロジェクトのフォルダなど）に保存すれば、自動的にMarkdown文書を解釈し始めます。処理が終わればViewerペインに何かの文章が生成されます。これからの内容を進める前にSourceペインとViewerペインの中身をそれぞれ対応しながら、どのような関係があるのかを考えてみましょう。\n　R Markdownファイルは大きく3つの領域に分けることができます（ 図 1）。まず、最初に---と---で囲まれた領域はヘッダー（Header）と呼ばれる領域です。ここでは題目、作成者情報の入力以外にも、文書全体に通じる設定を行います。これは第@ref(rmarkdown-header)節で解説します。次はR Markdownの核心部であるチャンク（Chunk）です。チャンクは```{r}と```で囲まれた領域であり、Rコードが入る箇所です。チャンクに関しましては第@ref(rmarkdown-grammar)節の後半と第@ref(rmarkdown-chunk)節で解説します。その他の領域がマークダウン（Markdown）であり、文書に該当します。\n\n\n\n\n\n図 1: R Markdownのサンプルページ\n\n\n\n\n　まずは、文章の書き方から説明します。非常に簡単な文法で綺麗、かつ構造化された文書が作成可能であり、これに慣れるとMarkdown基盤のノートアプリなどを使って素早くノート作成、メモが出来ます。"
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html#rmarkdown-grammar",
    "href": "tutorial/R/rmarkdown_intro.html#rmarkdown-grammar",
    "title": "R Markdown入門",
    "section": "Markdown文法の基本",
    "text": "Markdown文法の基本\n　まずは、Markdownの文法について解説します。ここではMarkdown文書内に以下のようなことを作成する方法を実際の書き方と、出力画面を対比しながら解説していきます。\n\n改行\n強調: 太字、イタリック、アンダーライン、取り消し線\n箇条書き\n見出し\n区切り線\n表\n画像\nリンク\n脚注\n数式\n引用\nコメント\nRコード\n\n\n改行\n　Markdownにおける改行はやや特殊です。特殊といっても難しいことはありません。普段よりもう一行改行するだけです。Markdownの場合、1回の改行は改行として判定されず、同じ行の連続と認識します。たとえば、Inputのように入力するとOutputのように文章1と文章2が繋がります。\nInput:\n文章1\n文章2\nOutput:\n文章1 文章2\n　文章1と文章2を改行するためにはもう一行、改行する必要があります。以下の例を見てください。\nInput:\n文章1\n\n文章2\nOutput:\n文章1\n文章2\n　こうすることで段落間の間隔を強制的に入れることとなり、作成者側にも読みやすい文書構造になります2。\n\n\n強調\n文章の一部を強調する方法として太字、イタリック3、アンダーラインがあり、強調ではありませんが、ついでに取り消し線についても紹介します。いずれも強調したい箇所を記号で囲むだけです。\nInput:\n文章の一部を**太字**にしてみましょう。\n\n*イタリック*もいいですね。\n\n~~取り消し線~~はあまり使わないかも。\n\n<u>アンダーライン</u>はHTMLタグを使います。\nOutput:\n文章の一部を太字にしてみましょう。\nイタリックもいいですね。\n取り消し線はあまり使わないかも。\nアンダーラインはHTMLタグを使います。\n\n\n箇条書き\n箇条書きには順序なしと順序付きがあります。順序なしの場合*または-の後に半角スペースを1つ入れるだけです。また、2文字以上の字下げで下位項目を追加することもできます。\nInput:\n- 項目1\n  - 項目1-1\n  - 項目1-2\n    - 項目1-2-1\n      - 項目1-2-1-1\n    - 項目1-2-2\n- 項目2\n- 項目3\nOutput:\n\n項目1\n\n項目1-1\n項目1-2\n\n項目1-2-1\n\n項目1-2-1-1\n\n項目1-2-2\n\n\n項目2\n項目3\n\n　続きまして順序付き箇条書きですが、これは-（または*）を数字.に換えるだけです。順序なしの場合と違って数字の後にピリオド（.）が付くことに注意してください。また、下位項目を作成する際、順序なしはスペース2つ以上が必要でしたが、順序付きの場合、少なくとも3つが必要です。\nInput:\n1. 項目1\n   1. 項目1-1\n   2. 項目1-2\n2. 項目2\n   * 項目2-1\n   * 項目2-2\n3. 項目3\nOutput:\n\n項目1\n\n項目1-1\n項目1-2\n\n項目2\n\n項目2-1\n項目2-2\n\n項目3\n\n\n\n見出し\n　章、節、段落のタイトルを付ける際は#を使います。#の数が多いほど文字が小さくなります。章の見出しを##にするなら節は###、小節または段落は####が適切でしょう。見出しは####まで使えます。\nInput:\n# 見出し1\n## 見出し2\n### 見出し3\n#### 見出し4\nOutput:\n見出し1\n見出し2\n見出し3\n見出し4\n\n\n区切り線\n区切り線は---または***を使います。\nInput:\n---\nOutput:\n\n\n\n表\n　Markdownの表は非常にシンプルな書き方をしています。行は改行で、列は|で区切られます。ただ、表の第1行はヘッダー（変数名や列名が表示される行）扱いとなり、ヘッダーと内容の区分は|---|で行います。以下はMarkdownを利用した簡単な表の書き方です。ここでは可読性のためにスペースを適宜入れましたが、スペースの有無は結果に影響を与えません。\nInput:\n|ID   |Name     |Math    |English |Favorite food|\n|:---:|---------|-------:|-------:|-------------|\n|1    |SONG     |15      |10      |Ramen        |\n|2    |Yanai    |100     |100     |Cat food     |\n|3    |Shigemura|80      |50      |Raw chicken  |\n|4    |Wickham  |80      |90      |Lamb         |\nOutput:\n\n\n\nID\nName\nMath\nEnglish\nFavorite food\n\n\n\n\n1\nSONG\n15\n10\nRamen\n\n\n2\nYanai\n100\n100\nCat food\n\n\n3\nShigemura\n80\n50\nRaw chicken\n\n\n4\nWickham\n80\n90\nLamb\n\n\n\n　1行目はヘッダーであり、太字かつ中央揃えになります。2行目以降はデフォルトでは左揃えになりますが。ただし。|---|をいじることによって当該列の揃えを調整できます。|:---|は左 (デフォルト)、|---:|は右、|:---:|は中央となります。また-の個数は1個以上なら問題ありません。つまり、|-|も|---|も同じです。\n\n\n画像\n　R Markdownに画像を入れるには![代替テキスト](ファイル名)と入力します。当たり前ですが、画像ファイルがワーキングディレクトリにない場合はパスを指定する必要があります。[代替テキスト]は画像を読み込めなかった場合のテキストを意味します。これは画像が読み込めなかった場合の代替テキストでもありますが、視覚障害者用のウェブブラウザーのためにも使われます。これらのウェブブラウザーはテキストのみ出力されるものが多く、画像の代わりには代替テキストが読み込まれます。\n　例えば、Figsフォルダー内のfavicon.pngというファイルを読み込むとしたら以下のように書きます。\nInput:\n![『私たちのR』ロゴ](Figs/favicon.png)\nOutput:\n\n\n\n『私たちのR』ロゴ\n\n\n\n\nリンク\n　ハイパーリンクは[テキスト](URL)のような形式で書きます。[]内は実際に表示されるテキストであり、()は飛ばすURLになります。\nInput:\n毎日1回は[SONGのホームページ](https://www.jaysong.net)へアクセスしましょう。\nOutput:\n毎日1回はSONGのホームページへアクセスしましょう。\n\n\n脚注\n　脚注は[^固有識別子]と[^固有識別子]: 脚注内容の2つの要素が必要です。まず、文末脚注を入れる箇所に[^xxxx]を挿入します。xxxxは任意の文字列れ構いません。しかし、同じR Markdown内においてこの識別子は被らないように注意してください。実際の脚注の内容は[^xxxx]: 内容のように入力します。これはどこに位置しても構いません。文書の途中でも、最後に入れても、脚注の内容は文末に位置します。ただし、脚注を入れる段落のすぐ後の方が作成する側としては読みやすいでしょう。\nInput:\nこれは普通の文章です[^foot1]。\n\n[^foot1]: これは普通の脚注です。\nOutput:\nこれは普通の文章です4。\n\n\n数式\n　インライン数式は$数式$で埋め込むことができます。数式はLaTeXの書き方とほぼ同じです。ちなみに、R Markdownの数式はMathJaxによってレンダリングされます。このMathJaxライブラリはHTMLに埋め込まれているのではないため、インターネットに接続せずにHTMLファイルを開くと数式が正しく出力されません。\nInput:\nアインシュタインと言えば、$e = mc^2$でしょう。\nOutput:\nアインシュタインと言えば、\\(e = mc^2\\)でしょう。\n　数式を独立した行として出力する場合は、$の代わりに$$を使用します。\nInput:\n独立した数式の書き方\n\n$$\ny_i \\sim \\text{Normal}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma).\n$$\n\nOutput:\n独立した数式の書き方\n\\[\ny_i \\sim \\text{Normal}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma).\n\\]\n　もし数式が複数の行で構成されている場合は$$内にaligned環境（\\begin{aligned}〜\\end{aligned}）を使用します。むろん、LaTeXと使い方は同じです。\nInput:\n複数の行にわたる数式の書き方\n\n$$\n\\begin{aligned}\n  Y_i      & \\sim \\text{Bernoulli}(\\theta_i), \\\\\n  \\theta_i & = \\text{logit}^{-1}(y_i^*), \\\\\n  y_i^*    & = \\beta_0 + \\beta_1 x_1 + \\beta_2 z_1.\n\\end{aligned}\n$$\n\nOutput:\n複数の行にわたる数式の書き方\n\\[\n\\begin{aligned}\n  Y_i      & \\sim \\text{Bernoulli}(\\theta_i), \\\\\n  \\theta_i & = \\text{logit}^{-1}(y_i^*), \\\\\n  y_i^*    & = \\beta_0 + \\beta_1 x_1 + \\beta_2 z_1.\n\\end{aligned}\n\\]\n　ここまで見ればお分かりかと思いますが、$$の中にはLaTeXコマンドが使用可能です。たとえば、行列を作成する際は以下のように\\begin{bmatrix}環境を使います。\nInput:\n行列の書き方\n\n$$\nX = \\begin{bmatrix}\n  x_{11} & x_{12} \\\\\n  x_{21} & x_{22} \\\\\n  x_{31} & x_{32}\n\\end{bmatrix}.\n$$\n\nOutput:\n行列の書き方\n\\[\nX = \\begin{bmatrix}\n  x_{11} & x_{12} \\\\\n  x_{21} & x_{22} \\\\\n  x_{31} & x_{32}\n\\end{bmatrix}.\n\\]\n\n\n引用\n　引用の際は文章の最初に>を入れるだけです。>の後に半角のスペースが1つ入ります。\nInput:\n「政治とは何か」についてイーストンは以下のように定義しました。\n\n> [A] political system can be designated as those interactions through which values are authoritatively allocated for a society.\nOutput:\n「政治とは何か」についてイーストンは以下のように定義しました。\n\n[A] political system can be designated as those interactions through which values are authoritatively allocated for a society.\n\n\n\nコメント\n　R Markdownにもコメントを付けることができます。とりあえず書いたが要らなくなった段落や文章があって、消すことがもったいない場合はコメントアウトするのも1つの方法です。ただし、コメントアウトの方法はRは#でしたが、これはR Markdownでは見出しの記号です。R Markdownのコメントは<!--と-->で囲みます。\nInput:\n文章1\n\n<!--\nここはコメントです。\n-->\n\n文章2\nOutput:\n文章1\n\n文章2\n\n\nコード\n　以上の内容まで抑えると、R Markdownを使って、簡単な文法のみで構造化された文書が作成できます。しかし、R Markdownの意義は文章とコード、結果が統合されることです。それでは文書にRコードを入れる方法について紹介します。\n　コードは```{r}と```の間に入力します。これだけです。これでコードと結果が同時に出力されます。たとえば、print(\"Hello World!\")を走らすコードを入れてみます。\nInput:\n\"Hello World!\"を出力するコード\n\n```{r, error=TRUE}\nprint(\"Hello World!\")\n```\nOutput:\n“Hello World!”を出力するコード\n\nprint(\"Hello World!\")\n\n[1] \"Hello World!\"\n\n\n　```{r}と```で囲まれた範囲をR Markdownではチャンク（Chunk）と呼びます。このチャンク内ではRと全く同じことが出来ます。パッケージやデータの読み込み、オブジェクトの生成、データハンドリング、可視化など、全てです。\nInput: 　\n```{r}\n# パッケージの読み込み\npacman::p_load(tidyverse)\n# R内蔵データセットのirisを使った可視化\niris %>%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %>%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal(base_size = 12)\n```\nOutput:\n\n# パッケージの読み込み\npacman::p_load(tidyverse)\n# R内蔵データセットのirisを使った可視化\niris %>%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %>%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n　他にも文中にRコードを埋め込むことも可能です。例えば、ベクトルX <- c(2, 3, 5, 7, 12)があり、この平均値を文中で示したいとします。むろん、文中に「5.8」と書いても問題はありません。しかし、実はXの入力ミスが見つかり、実はc(2, 3, 5, 7, 11)になったらどうなるでしょうか。この「5.8」と書いた箇所を見つけて5.6と修正したいといけません。これは非常に面倒な作業であり、ミスも起こりやすいです。文中でRコードを入れるためには`rコード`のように入力します。\n```{r}\nX <- c(2, 3, 5, 7, 11)\n```\n\n変数`X`の平均値は`r mean(X)`です。\nOutput:\n\nX <- c(2, 3, 5, 7, 11)\n\n変数Xの平均値は5.6です。\nここで`X`ですが、単に`で囲まれただけではコードと認識されません。これは主に文中に短いコードを入れる際に使う機能です。"
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html#rmarkdown-chunk",
    "href": "tutorial/R/rmarkdown_intro.html#rmarkdown-chunk",
    "title": "R Markdown入門",
    "section": "チャンクのオプション",
    "text": "チャンクのオプション\n　既に説明しましたとおり、R MakrdownはR + Markdownです。Rはチャンク、Markdownはチャンク外の部分に相当し、それぞれのカスタマイズが可能です。分析のコードと結果はチャンクにオプションを付けることで修正可能であり、文章の部分は次節で紹介するヘッダーで調整できます。ここではチャンクのオプションについて説明します。\n　チャンクは```{r}で始まりますが、実は{r}の箇所にオプションを追加することができます。具体的には{r チャンク名, オプション1, オプション2, ...}といった形です。まずはチャンク名について解説します。\n\nチャンク名とチャンク間依存関係\n　チャンク名は{r チャンク名}で指定し、rとチャンク名の間には,が入りません。これはチャンクに名前をしていするオプションですが、多くの場合分析に影響を与えることはありません。このチャンク名が重要となるのはcacheオプションを付ける場合です。\n　cacheオプションは処理結果を保存しておくことを意味します。チャンク内のコードはKnitする度に計算されます。もし、演算にかなりの時間を費やすコードが含まれている場合、Knitの時間も長くなります。この場合、cache = TRUEオプションを付けておくと、最初のKnit時に結果をファイルとして保存し、次回からはその結果を読み込むだけとなります。時間が非常に節約できるため、よく使われるオプションの1つです。ただし、チャンク内のコードが修正された場合、Knit時にもう一回処理を行います。コードの実質的な内容が変わらなくても、つまり、スペースを1つ入れただけでも再計算となります。\n　ここで1つ問題が生じます。たとえば、以下のようなコードを考えてみてください。\n```{r}\nX <- c(2, 3, 5, 7, 10)\n```\n\n```{r, cache = TRUE}\nmean(X)\n```\n　この構造に問題はありません。しかし、ここでXの5番目の要素を11に修正したとします。そしてもう一回Knitを行ったらどうなるでしょうか。正解は「何も変わらない」です。新しいmean(X)の結果は5.6のはずですが、5.4のままです。なぜなら、2番目のチャンクの結果は既に保存されており、コードも修正していないからです。もう一回強調しておきますが、cahce = TRUEの状態で当該チャンクが修正されない場合、結果は変わりません。\n　このようにあるチャンクの内容が他のチャンク内容に依存しているケースがあります。この場合、dependsonオプションを使います。使い方はdependson = \"依存するチャンク名\"です。もし、1番目のチャンク名をdefine_Xとしたら、dependson = \"define_X\"とオプションを加えます。\n```{r define_X}\nX <- c(2, 3, 5, 7, 10)\n```\n\n```{r, cache = TRUE, dependson = \"define_X\"}\nmean(X)\n```\n　このようにチャンク名とcache、dependsonオプションを組み合わせると、依存するチャンクの中身が変わったら、cache = TRUEでも再計算を行います。\n\n\nコードまたは結果の表示/非常時\n　次は「コードだけ見せたい」、「結果だけ見せたい」場合使うオプションを紹介します。これはあまり使わないかも知れませんが、本書のような技術書にはよく使う機能です。コードのみ出力し、計算を行わない場合はeval = FALSEオプションを、コードを見せず、結果のみ出力する場合はecho = FALSEを指定するだけです。\n他にもコードと結果を両方隠すことも可能です。つまり、チャンク内のコードは実行されるが、そのコードと結果を隠すことです。この場合に使うオプションがincludeであり、既定値はTRUEです。チャンクオプションにinclude = FALSEを追加すると、当該チャンクのコードは実行されますが、コードと結果は表示されません。\n\n\nプロット\n　既に見てきた通り、R Markdownは作図の結果も出力してくれます。そこで、図のサイズや解像度を変えることもできます。ここではプロットに関するいくつかのオプションを紹介します。\n\nfig.height: 図の高さ。単位はインチ。デフォルト値は7\nfig.width: 図の幅。単位はインチ。デフォルト値は7\nfig.align: 図の位置。デフォルトは\"left\"。\"center\"の場合、中央揃え、\"right\"の場合は右揃えになる。\nfig.cap: 図のキャプション\ndpi: 図の解像度。デフォルトは72。出版用の図は一般的に300以上を使う\n\n　実際に高さ5インチ、幅7インチ、中央揃え、解像度72dpiの図を作成し、キャプションとして「irisデータセットの可視化」を付けてみましょう。\nInput: 　\n```{r, fig.height = 5, fig.width = 7, fig.align = \"center\", fig.cap = \"`iris`データセットの可視化\", dpi = 72}\niris %>%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %>%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal(base_size = 12)\n```\nOutput:\n\niris %>%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %>%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nirisデータセットの可視化\n\n\n\n\n\n\nコードの見栄\n　第@ref(programming)章ではスクリプトの書き方を紹介しましたが、常にその書き方に則った書き方をしているとは限りません。自分だけが見るコードなら別に推奨されない書き方でも問題ないかも知れませんが、R Markdownの結果は他人と共有するケースが多いため、読みやすいコードを書くのも大事です。ここで便利なオプションがtidyオプションです。tidy = TRUEを加えると、自動的にコードを読みやすい形に調整してくれます。たとえば、以下のコードは字下げもなく、スペースもほとんど入れていないコードですが、tidy = TRUEを付けた場合と付けなかった場合の出力結果の違いを見てみましょう。\nInput:\n```{r, eval = FALSE}\nfor(i in 1:10){\nprint(i*2)\n}\n```\nOutput:　\n\nfor(i in 1:10){\nprint(i*2)\n}\n\nInput:\n```{r, eval = FALSE, tidy = TRUE}\nfor(i in 1:10){\nprint(i*2)\n}\n```\nOutput:\n\nfor (i in 1:10) {\n    print(i * 2)\n}\n\n　tidy = TRUEを付けただけで、読みやすいコードになりました。ちなみにtidyオプションを使うためには事前に{formatR}パッケージをインストールしておく必要があります。ただし、{formatR}パッケージはR Markdwon内において読み込んでおく必要はありません。また、{formatR}パッケージは万能ではないため、普段から読みやすいコードを書くようにしましょう。\n\n\nチャンクオプションのもう一つの書き方\n　これまでチャンクオプションは{r}の内部に指定すると述べましたが、チャンクオプションが多くなる場合、チャンクの第1行目が長くなり、コードの可読性が低下する可能性があります。ここで便利な機能が#|によるチャンクオプションの指定です。これはチャンク内部に#|を書いておけば、#|以降の内容がチャンクオプションとして認識される機能です。\n　図@ref(fig:rmarkdown-chunk-1)の作図チャンクは以下のように書くことも可能です。\n```{r}\n#| fig.height: 5\n#| fig.width: 7\n#| fig.align: \"center\"\n#| fig.cap: \"`iris`データセットの可視化\"\n#| dpi: 72\n\niris %>%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %>%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal(base_size = 12)\n```\n　もう一つの方法は、全て (あるいは、ほとんど)のチャンクに渡って共通するチャンクオプションの場合、最初に宣言しておくことです。もし、全ての図を中央に揃え (fig.align = \"center\")、解像度を300dpi (dpi = 300)にするなら、後述するYAMLヘッダーに続く最初のチャンクを以下のように書きます。\n```{r, include = FALSE}\nknitr::opts_chunk$set(fig.align = \"center\", dpi = 300)\n```\n　こう書いておくと、そのRMarkdownファイルの全てのチャンクにfig.align = \"center\"とdpi = 300オプションが付くようになります。一部のチャンクにfig.alignやdpiのオプションを付ける場合、当該チャンクのオプションが優先されます。\n　このようにチャンクオプションが見やすくなるメリットがありますが、現在 (2022年07月07日)のRStudioは、チャンク内部のオプション指定の入力補助に対応していないことに注意してください。通常の書き方だとif()文のような条件分岐がチャンクのオプションとして使えますが、#|の書き方だと正しく作動しません。ただし、#| eval: ...を#| eval = ...にすると作動します。"
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html#rmarkdown-header",
    "href": "tutorial/R/rmarkdown_intro.html#rmarkdown-header",
    "title": "R Markdown入門",
    "section": "ヘッダーのオプション",
    "text": "ヘッダーのオプション\n　R Markdownファイルを生成すると、ファイルの最上段には以下のようなヘッダー（header）というものが生成されます。\n---\ntitle: \"Untitled\"\nauthor: \"Jaehyun Song\"\ndate: \"8/6/2020\"\noutput: html_document\n---\n　基本的には4つの項目が指定されており、それぞれ文書のタイトル（title:）、作成者名（author:）、作成日（date:）、出力形式（output:）があります。タイトルと作成者名はファイル生成時に指定した内容が、作成日は今日の日付が、出力形式はHTMLで設定した場合html_documentになっています。R MarkdownヘッダーはYAML（やむる）形式で書かれています。こちらはいつでも修正可能であり、インラインRコードを埋め込むことも可能です。たとえば、作成日をファイル生成日でなく、Knitした日付にしたい場合は日付を出力するSys.Date()関数を使って、date: \"最終修正: `r Sys.Date()`\"のように書くことも可能です。他にもデフォルトでは指定されていませんが、subtitle:を使ってサブタイトルを指定したり、abstract:で要約を入れることも可能です。 　 　また、R Markdownのコメントは<!--と-->を使いますが、ヘッダー内のコメントはRと同様、#を使います。\n　ヘッダーで設定できる項目は数十個以上ですが、ここでは頻繁に使われる項目について紹介します。\n\n目次の追加\n　R Markdownの文章が長くなったり、コードが多く含まれる場合、目次を入れたら文章の構造が一目で把握できるでしょう。目次は見出しに沿って自動的に生成されます。#は章、##は節といった形式です。\n---\ntitle: \"タイトル\"\nsubtitle: \"サブタイトル\"\nauthor: \"作成者名\"\ndate: \"最終修正: `r Sys.Date()`\"\noutput:\n  html_document:\n    toc: FALSE\n    toc_depth: 3\n    toc_float: FALSE\n    number_sections: FALSE\n---\n　字下げには常に注意してください。html_documentはoutput:の下位項目ですから、字下げを行います。また、tocはhtml_documentの下位項目ですので、更に字下げをします。ここでは目次と関連する項目として以下の4つを紹介します。\n\ntoc: 目次の出力有無\n\nデフォルトはFALSE、目次を出力する際はTRUEにします。\n\ntoc_depth: 目次の深さを指定\n\nデフォルトは3であり、これはどのレベルの見出しまで目次に含むかをしてします。toc_depth: 2なら##見出しまで目次に出力されます\n\ntoc_float: 目次のフローティング\n\nデフォルトはFALSEです。これをTRUEにすると、目次は文書の左側に位置するようになり、文書をスクロールしても目次が付いてきます。\n\nnumber_sections: 見出しに通し番号を付けるか\n\nデフォルトはFALSEです。これをTRUEにすると、見出しに通し番号が付きます。むろん、付いた通し番号は目次でも出力されます。\n\n\n\n\nコードのハイライト\n　R Markdownの場合、コードチャンク内のコードの一部に対して自動的に色付けを行います。たとえば、本書の場合、関数名は緑、引数は赤、文字列は青といった形です。これはコードの可読性を向上させる効果もあります。この色付けの詳細はhtml_document:のhighligh:から調整できます。\n---\ntitle: \"タイトル\"\nsubtitle: \"サブタイトル\"\nauthor: \"作成者名\"\ndate: \"最終修正: `r Sys.Date()`\"\noutput:\n  html_document:\n    highlight: \"tango\"\n---\nR Markdown 2.3の場合、使用可能なテーマは以下の10種類です。自分の好みでハイライトテーマを替えてみましょう。\n\n\n\nhighlight\n出力結果\n\n\n\n\n\"default\"\n\n\n\n\"tango\"\n\n\n\n\"pygments\"\n\n\n\n\"kate\"\n\n\n\n\"monochrome\"\n\n\n\n\"espresso\"\n\n\n\n\"zenburn\"\n\n\n\n\"haddock\"\n\n\n\n\"breezedark\"\n\n\n\n\"textmate\""
  },
  {
    "objectID": "tutorial/R/rmarkdown_intro.html#rmarkdown-japanese",
    "href": "tutorial/R/rmarkdown_intro.html#rmarkdown-japanese",
    "title": "R Markdown入門",
    "section": "日本語が含まれているPDFの出力",
    "text": "日本語が含まれているPDFの出力\n　日本語が含まれているPDF出力には片桐智志さんの{rmdja}パッケージが提供するテンプレが便利です。{rmdja}パッケージの詳細は公式レポジトリに譲りますが、ここでは簡単な例をお見せします。以下の内容は自分のPCにLaTeX環境が導入されている場合を想定しています5。\n　まずは{rmdja}のインストールからです。CRANに登録されていたいため、GitHub経由でインストールします。\n# remotes::の代わりにdevtools::も可\n# pacman::p_install_gh()も可\nremotes::install_github('Gedevan-Aleksizde/rmdja')\n次はR Markdown文書を作成しますが、RStudioのFile > New File > R Markdown …を選択します。左側のFrom Templateを選択し、pdf article in Japaneseを選択します。OKをクリックするとサンプル文書が表示されるので、YAMLヘッダー以下の内容を修正するだけです。図表番号などの相互参照（cross reference）が初回Knitでは反映されない場合がありますが、2回目からは表示されます。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html",
    "href": "tutorial/R/montecarlo_intro.html",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggforce)\n\n　モンテカルロ法 (Monte Carlo method)とは無作為に抽出された乱数を用い、数値計算やシミュレーションを行う手法を意味します。モンテカルロはヨーロッパのモナコ公国内の一つの地区であり、カジノで有名なところです。カジノではサイコロやルーレット、無作為に配られたカードなど、乱数が頻繁に使われることからこのように名付けられました。\n　モンテカルロ法を用いたシミュレーションがモンテカルロ・シミュレーションです。計算があまりにも複雑だったり、実質的に代数で解が得られない解析学上の問題などに強みを持つ手法です。一部、明快な例（共役事前分布が存在するなど）を除き、事後分布の計算が非常に複雑（実質、不可能）だと知られていたベイズ統計学もモンテカルロ法（マルコフ連鎖モンテカルロ法; MCMC）によって、ようやく使えるものになったなど、今になってモンテカルロ法は非常に広く用いられています。\n　モンテカルロ法から得られた結果には常に誤差が存在します。とりわけ、生成された乱数が少ない（= 試行回数が少ない）場合、この誤差は大きくなります。しかし、近年はパソコンの性能が飛躍的に発達しているため、かなり小さな誤差で、つまりより正確な結果が得られるようになりました。\n　以下ではまず、モンテカルロ法を理解するために必須知識である乱数生成について解説します。具体的には乱数生成のアルゴリズムでなく、Rで乱数を生成する方法について紹介します。続いて、モンテカルロ法を用いたシミュレーションの例として誕生日問題、モンティ・ホール問題、円周率の計算、ブートストラップ法を紹介します。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html#monte-rng",
    "href": "tutorial/R/montecarlo_intro.html#monte-rng",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "乱数生成",
    "text": "乱数生成\n\nsample()によるサンプリング\n　無作為に値を抽出する方法には2つが考えられます。一つは値の集合から無作為に値を抽出する方法、もう一つは正規分布などの確率分布から値を抽出する方法です。ここではまずsample()関数を用い、値の集合から無作為に値を抽出する方法について説明します。\n\nsample(x = 値の集合ベクトル, size = 抽出の回数, \n       replace = 復元抽出の有無, prob = 各要素が抽出される確率)\n\n　replaceは復元抽出の有無を指定する引数であり、既定値はFALSE、つまり非復元抽出がデフォルトとなっています。これは一度抽出された要素は、二度と抽出されないことを意味します。値の集合が{0, 1}で、5個の値を抽出する（=sizeがxの長さより大きい）ならば、replaceは必ずTRUEに設定する必要があります。抽選などは非復元抽出であるため、replace引数は省略可能です。しかし、対数の法則やブートストラップなどは復元抽出を仮定している場合が多く、意識的にreplace関数は指定することを推奨します。probは各要素が抽出される確率を意味し、xの実引数と同じ長さのnumeric型ベクトルを指定します。probの実引数の総和は1であることが望ましいですが、総和が1でない場合、自動的に総和が1になるよう正則化を行います。つまり、c(1, 3)はc(0.25, 0.75)と同じことを意味します。\n　サイコロを3回振るコードを書くなら、値の集合（x）はc(1, 2, 3, 4, 5, 6)、または1:6で、抽出の回数（size）は3となります。また、一回出た目も抽出される可能性があるため、復元抽出を行う必要があります（replace = TRUE）。そして各目が出る確率は1/6ですが、各値が抽出される確率が等しい場合、省略可能です。\n\n# この場合、prob引数は省略可能\nsample(1:6, 3, replace = TRUE, prob = rep(1/6, 6))\n\n[1] 1 5 2\n\n\n　今回はサイコロを1万回振り、それぞれの目が出た回数を棒グラフとして示してみます。無作為に抽出された値であれば、各目が出る回数は等しいはずです。ベクトルに対してtable()関数を使うと、各要素が出現した回数が出力され、このオブジェクトをbarplot()関数に渡すと棒グラフを作成することができます。\n\nDice_vec <- sample(1:6, 10^4, replace = TRUE)\ntable(Dice_vec) %>% barplot()\n\n\n\n\n\n\n\n\n　1から6までの目が出た回数がほぼ同じであることが確認できます。この6つの棒の高さがすべて同じになることはありえませんが（そもそも1万を6で割ったら余りが出ますね）、ほぼ同じ割合であることから、疑似乱数とは言え、シミュレーション用としては十分でしょう。\n\n\n確率分布からの乱数制制\n\n\n\n関数名\n確率分布\nパラメーター\n\n\n\n\nrbeta()\nベータ分布\nn, shape1, shape2\n\n\nrbinom()\n二項分布\nn, size, prob\n\n\nrcauchy()\nコーシー分布\nn, location, scale\n\n\nrchisq()\n\\(\\chi^2\\)分布\nn, df\n\n\nrexp()\n指数分布\nn, rate\n\n\nrf()\n\\(F\\)分布\nn, df1, df2\n\n\nrgamma()\nガンマ分布\nn, shape, scale\n\n\nrgeom()\n幾何分布\nn, prob\n\n\nrhyper()\n超幾何分布\nnn, m, n, k\n\n\nrlnorm()\n対数正規分布\nn, meanlog, sdlog\n\n\nrmultinom()\n多項分布\nn, size, prob\n\n\nrnbinom()\n負の二項分布\nn, size, prob\n\n\nrnorm()\n正規分布\nn, mean, sd\n\n\nrpois()\nポアソン分布\nn, lambda\n\n\nrt()\nt分布\nn, df\n\n\nrunif()\n一様分布\nn, min, max\n\n\nrweibull()\nワイブル分布\nn, shape, scale\n\n\nmvtnorm::rmvnorm()\n多変量正規分布\nn, mean, sigma\n\n\n\n　以上の表に掲載されているパラメーター以外にも指定可能なパラメーターがあるため、詳細は各関数のヘルプを参照してください。たとえば、ガンマ分布の場合、rateで、負の二項分布の場合、muで分布の形状を指定することができます。また、多変量正規分布の乱数を抽出するには{mvtnorm}パッケージのrmvnorm()を使いますが、ここでのmeanは数値型ベクトル、sigmaは行列構造の分散共分散行列を使います1。\n\n\nシードについて\n　特定の分布から乱数を抽出する場合、当たり前ですが、抽出の度に値が変わります。たとえば、平均0、標準偏差1の正規分布（標準正規分布）から5つの値を抽出し、小数点3桁に丸める作業を3回繰り返しみましょう。\n\nrnorm(5) %>% round(3)\n\n[1] 0.066 1.094 1.627 0.247 1.526\n\nrnorm(5) %>% round(3)\n\n[1]  0.159 -0.965  1.615  1.159  1.368\n\nrnorm(5) %>% round(3)\n\n[1]  0.214 -0.511 -1.480  0.854 -1.340\n\n\n　このように、抽出の度に結果が変わります。1回きりのシミュレーションではこれで問題ないでしょうが、同じシミュレーションから同じ結果を得るためには、乱数を固定する必要があります。そこで使うのがシード（seed）です。シードが同じなら抽出される乱数は同じ値を取ります。シードの指定はset.seed(numeric型スカラー)です。たとえば、シードを19861008にし、同じ作業をやってみましょう。\n\nset.seed(19861008)\nrnorm(5) %>% round(3)\n\n[1] -0.086  0.396 -1.330  0.574  0.152\n\nrnorm(5) %>% round(3)\n\n[1]  0.555  0.620 -1.133  0.572  0.900\n\n\n　シードを指定しても2つのベクトルは異なる値を取りますが、もう一度シードを指定してから乱数抽出をしてみましょう。\n\nset.seed(19861008)\nrnorm(5) %>% round(3)\n\n[1] -0.086  0.396 -1.330  0.574  0.152\n\n\n　先ほどのコードでシードを指定した直後に抽出した乱数と同じ乱数が得られました。モンテカルロ・シミュレーションにおいて乱数は非常に重要ですが、これはシミュレーションの度に異なる結果が得られることを意味します。つまり、自分が書いたコードから100%同じ結果が得られないだけでなく、自分も同じ結果を再現できないことを意味します。この場合、シードを指定すると乱数が固定され、シミュレーション結果の再現ができるようになります。\n　一つ注意すべき点は乱数を固定した後、複数回抽出を繰り返す場合、その順番も固定されるという点です。たとえば、シードを固定せずにもう一回5つの値を抽出してみましょう。\n\nrnorm(5) %>% round(3)\n\n[1]  0.555  0.620 -1.133  0.572  0.900\n\n\n　この結果は先ほどシード指定後、2回目の抽出結果と同じ結果となります。結果を再現するという点では大きな問題はないはずですが、仕様を理解しておくことは重要でしょう。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html#monte-birthday",
    "href": "tutorial/R/montecarlo_intro.html#monte-birthday",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "例1: 誕生日問題",
    "text": "例1: 誕生日問題\n　まず簡単な例として誕生日問題 (birthday problem)をシミュレーションで確認してみましょう。誕生日問題とは「何人いれば、その中に誕生日が同じ2人以上がいる確率が50%を超えるか。」といった問題です。1年を365日で考えると（2月29日生まれの皆さん、すみません…）、366人がいれば確実に (= 100%)同じ誕生日の人が2人以上いることになりますね。100%でなく、50%まで基準を下げるならその半分である183人は必要じゃないかと思うかも知れません。しかし、実はたった23人が集まれば、その中で同じ誕生日の人が2人以上いる確率が50%になります。誕生日のパラドックスとも呼ばれるものですが、これをシミュレーションで確認してたいと思います。\n　学生の数をn_studentとし、1から365までの公差1の等差数列から、n_student個の値を復元抽出します。\n\nn_student <- 30 # 学生数\n# 「1から365までの公差1の等差数列」からn_student個の値を復元抽出\nBirth_vec <- sample(1:365, n_student, replace = TRUE)\n\nBirth_vec\n\n [1] 340 122 230 148  12 136  87  46  16 310 104 351 137  53 361  65 106 361 108\n[20] 281 299  67 342   4 338  88 156 254 192  37\n\n\n　このベクトルの中で重複する要素があるかどうかを確認するにはduplicated()関数を使います。ある要素が他の要素と重複するならTRUEが、なければFALSEが表示されます。\n\nduplicated(Birth_vec)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n　ただし、一つでもTRUEが含まれていれば、同じ誕生日の人が二人以上はいるということとなるので、更にany()関数を使います。any()内の条件文において、一つでもTRUEがあれば返り値はTRUEとなり、全てFALSEならFALSEを返す関数です。\n\nany(duplicated(Birth_vec))\n\n[1] TRUE\n\n\n　以上の作業を100回繰り返す場合、試行回数が100回となります。この場合、TRUEの結果が出る試行は何回でしょうか。\n\nn_student <- 10   # 学生数\nn_trials  <- 100  # 試行回数\n\n# 結果を格納する空ベクトルを用意する\nResult_vec <- rep(NA, n_trials)\n\n# 反復処理\nfor (i in 1:n_trials) {\n    Birth_vec     <- sample(1:365, n_student, replace = TRUE)\n    Result_vec[i] <- any(duplicated(Birth_vec))\n}\n\nResult_vec\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [49] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE\n\n\n　100回の試行の中でTRUEが出たのは10回ですね。割合で考えると10%です。この試行回数を無限にすると確率として解釈できますが、無限回繰り返しは世の中が終わるまでやっても終わりませんね。ただし、十分に多い試行回数、たとえば1万回程度繰り返すと確率に近似できるでしょう。\n\nn_student <- 10    # 学生数\nn_trials  <- 10000 # 試行回数\n\n# 結果を格納する空ベクトルを用意する\nResult_vec <- rep(NA, n_trials)\n\n# 反復処理\nfor (i in 1:n_trials) {\n    Birth_vec     <- sample(1:365, n_student, replace = TRUE)\n    Result_vec[i] <- any(duplicated(Birth_vec))\n}\n\nsum(Result_vec)\n\n[1] 1147\n\n\n　1万回の試行からTRUEが出た回数は1147回であり、11.5%ですね。つまり、人が10人集まれば誕生日が同じ人が2人以上いる確率は約11.5%ということになります。実はこの確率は厳密に計算可能であり、理論的な確率は約11.7%です。シミュレーションから得られた結果が理論値にかなり近似していることが分かります。\n　今回は試行回数は100に固定し、学生数を2から100まで調整しながら同じ誕生日の人が2人以上いる割合を計算してみましょう。以下では割合を計算する関数Birthday_Func()を作成し、{purrr}のmap_dbl()関数を使用して反復処理を行います。関数の作成は第@ref(functions)章を、{purrr}の使い方については第@ref(iteration)章を参照してください。\n\n# 割合を計算する関数を作成する\nBirthday_Func <- function (n, n_trials) {\n    \n    # 各試行の結果を格納する空ベクトルを用意する。\n    Result_vec <- rep(NA, n_trials)\n    \n    # n_trials回だけ{}内コードを繰り返す。\n    for (i in 1:n_trials) {\n        # 1:365からn個の値を復元抽出\n        Birth_vec     <- sample(1:365, n, replace = TRUE)\n        # 重複する要素があるかをチェックし、結果ベクトルのi番目に格納\n        Result_vec[i] <- any(duplicated(Birth_vec))\n    }\n    \n    # 各試行結果が格納されたベクトルからTRUEの割合を返す\n    mean(Result_vec)\n}\n\n# 学生数をStudents列に格納したデータフレーム (tibble)を作成\nProb_df <- tibble(Students = 2:100)\n\n# Students列の値に応じてBirthday_Func()を実行\nProb_df <- Prob_df %>%\n    mutate(Probs = map_dbl(Students, ~Birthday_Func(.x, 100)))\n\n　{purrr}関数を使わずに、for()文を使用した例は以下のようになります。\n\n# {purrr}を使わない方法\nProb_df <- tibble(Students = 2:100,\n                  Probs    = NA)\n\nfor (i in 1:nrow(Prob_df)) {\n    \n    Result_vec <- rep(NA, n_trials)\n    \n    for (j in 1:n_trials) {\n        Birth_vec     <- sample(1:365, Prob_df$Students[i], replace = TRUE)\n        Result_vec[j] <- any(duplicated(Birth_vec))\n    }\n    \n    Prob_df$Probs[i] <- mean(Result_vec)\n}\n\n　結果を確認してみましょう。\n\nProb_df\n\n# A tibble: 99 × 2\n   Students Probs\n      <int> <dbl>\n 1        2  0.01\n 2        3  0.02\n 3        4  0.06\n 4        5  0.01\n 5        6  0.02\n 6        7  0.08\n 7        8  0.06\n 8        9  0.09\n 9       10  0.12\n10       11  0.18\n# … with 89 more rows\n\n\n　学生数が何人いれば、同じ誕生日の人が2人以上いる割合が50%になるのでしょうか。割合が0.4以上、0.6以下の行を抽出してみましょう。\n\nProb_df %>%\n    filter(Probs >= 0.4 & Probs <= 0.6)\n\n# A tibble: 7 × 2\n  Students Probs\n     <int> <dbl>\n1       18  0.41\n2       21  0.46\n3       22  0.41\n4       23  0.45\n5       24  0.52\n6       25  0.58\n7       27  0.56\n\n\n　大体23人前後ですかね。試行回数を増やせばもう少し厳密に検証出来るかも知れませんが、とりあえず以上の結果を可視化してみましょう。\n\nProb_df %>%\n    ggplot() +\n    geom_line(aes(x = Students, y = Probs * 100), size = 1) +\n    geom_hline(yintercept = 50, color = \"red\", linetype = 2) +\n    labs(x = \"学生数\",\n         y = \"同じ誕生日の人が2人以上いる割合 (%)\\n(試行回数 = 100)\") +\n    theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　非常に直感に反する結果かも知れませんが、50人程度いれば、ほぼ確実に同じ誕生日の人が2人以上いることが分かります。この誕生日問題は以下のように解くことができます。人が\\(n\\)人いる場合、同じ誕生日の人が2人以上いる確率\\(p(n)\\)は、\n\\[\np(n) = 1 - \\frac{365!}{365^n (365-n)!}\n\\]\n　!は階乗を意味し、5!は\\(5 \\times 4 \\times 3 \\times 2 \\times 1\\)を意味します。Rではfactorial()関数を使います。それでは\\(p(50)\\)はいくらでしょうか。\n\n1 - factorial(365) / (365^50 * factorial(365 - 50))\n\n[1] NaN\n\n\n　あらら、NaNがでましたね。つまり、計算不可です。実際、factorial(365)だけでも計算結果はInfが出ます。むろん、実際に無限ではありませんが、非常に大きい数値ということです。以上の式を計算可能な式に変形すると以下のようになります。\n\\[\np(n) = 1 - \\frac{n! \\times _{365}C_n}{365^n}\n\\]\n　\\(C\\)は二項係数を意味し\\(_nC_k\\)は\\(\\frac{n!}{k!(n-k)!}\\)です。Rではchoose(n, k)で計算可能です。\n\n1 - ((factorial(50) * choose(365, 50)) / 365^50)\n\n[1] 0.9703736\n\n\n　結果は0.9703736です。つまり、人が50人いれば同じ誕生日の人が2人以上いる確率は約97%ということです。それでは、以上の式を関数化し、p(22)とp(23)を計算してみましょう。\n\nBirth_Expect <- function (n) {\n    1 - ((factorial(n) * choose(365, n)) / 365^n)\n}\n\nBirth_Expect(22)\n\n[1] 0.4756953\n\nBirth_Expect(23)\n\n[1] 0.5072972\n\n\n　確率が50%を超える人数は23人であることが分かります。先ほどのデータフレーム (Prob_df)にこの理論値をExpectという名の列として追加してみましょう。\n\nProb_df <- Prob_df %>%\n    mutate(Expect = map_dbl(Students, ~Birth_Expect(.x)))\n\nProb_df\n\n# A tibble: 99 × 3\n   Students Probs  Expect\n      <int> <dbl>   <dbl>\n 1        2  0.01 0.00274\n 2        3  0.02 0.00820\n 3        4  0.06 0.0164 \n 4        5  0.01 0.0271 \n 5        6  0.02 0.0405 \n 6        7  0.08 0.0562 \n 7        8  0.06 0.0743 \n 8        9  0.09 0.0946 \n 9       10  0.12 0.117  \n10       11  0.18 0.141  \n# … with 89 more rows\n\n\n　これは人間にとっては読みやすい表ですが、可視化まで考えると、tidyなデータといは言えません。したがって、pivot_longer()関数を使用してtidyなデータに整形します。{tidyr}パッケージの使い方は第@ref(tidydata)章を参照してください。\n\nProb_df2 <- Prob_df %>%\n    pivot_longer(cols      = Probs:Expect,\n                 names_to  = \"Type\",\n                 values_to = \"Prob\") \n\nProb_df2\n\n# A tibble: 198 × 3\n   Students Type      Prob\n      <int> <chr>    <dbl>\n 1        2 Probs  0.01   \n 2        2 Expect 0.00274\n 3        3 Probs  0.02   \n 4        3 Expect 0.00820\n 5        4 Probs  0.06   \n 6        4 Expect 0.0164 \n 7        5 Probs  0.01   \n 8        5 Expect 0.0271 \n 9        6 Probs  0.02   \n10        6 Expect 0.0405 \n# … with 188 more rows\n\n\n　こちらのデータを使用し、シミュレーションから得られた結果と理論値を折れ線グラフで出力してみましょう。\n\nProb_df2 %>%\n    mutate(Type = ifelse(Type == \"Probs\", \"シミュレーション\", \"理論値\")) %>%\n    ggplot() +\n    geom_line(aes(x = Students, y = Prob * 100, color = Type), size = 1) +\n    labs(x     = \"学生数\",\n         y     = \"同じ誕生日の人が2人以上いる割合 (%)\\n(試行回数 = 100)\",\n         color = \"\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　最後に以上の作業を試行回数10000としてもう一回やってみましょう。\n\n# 学生数をStudents列に格納したデータフレーム (tibble)を作成\nProb_df <- tibble(Students = 2:100)\n\n# Students列の値に応じてBirthday_Func()を実行\nProb_df <- Prob_df %>%\n    mutate(Simul  = map_dbl(Students, ~Birthday_Func(.x, 10000)),\n           Expect = map_dbl(Students, ~Birth_Expect(.x)))\n\nProb_df %>%\n    pivot_longer(cols      = Simul:Expect,\n                 names_to  = \"Type\",\n                 values_to = \"Prob\") %>%\n    mutate(Type = ifelse(Type == \"Simul\", \"シミュレーション\", \"理論値\")) %>%\n    ggplot() +\n    geom_line(aes(x = Students, y = Prob * 100, color = Type), size = 1) +\n    labs(x     = \"学生数\",\n         y     = \"同じ誕生日の人が2人以上いる割合 (%)\\n(試行回数 = 100)\",\n         color = \"\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n　モンテカルロ・シミュレーションから得られた割合と理論上の確率が非常に近似していることが分かります。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html#monte-montyhall",
    "href": "tutorial/R/montecarlo_intro.html#monte-montyhall",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "例2: モンティ・ホール問題",
    "text": "例2: モンティ・ホール問題\n　抽出される乱数が必ずしも数値である必要はありません。たとえば、コイン投げの表と裏、ポーカーで配られたカードなど、数値以外の乱数もあり得ます。ここでは「AとB、C」から一つを選ぶ例として、モンティ・ホール問題をモンテカルロ法で解いてみましょう。\n　モンティ・ホール問題はアメリカのテレビ番組「Let’s make a deal」の中のゲームであり、この番組の司会者の名前がモンティ・ホール (Monty Hall)さんです。このゲームのルールは非常にシンプルです。\n\n3つのドアがあり、1つのドアの裏に商品 (車)がある。残りの2つは外れ（ヤギ）である。\n参加者はドアを選択する。\n司会者が残りのドア2つの中で商品がないドアを開けて中身を見せる。\nここで参加者はドアの選択を変える機会が与えられる。\n\n　直観的に考えて、司会者が外れのドアを1つ教えてくれたなら、自分が選んだドアを含め、残りの2つのドアの1つに絶対に商品があります。直感的に考えてみると、当たる確率は半々であって、変えても、変えなくても当たる確率は同じだと考えられます。詳細はWikipediaなどを参照してください。この問題を巡る論争とかも紹介されていてなかなか面白いです。\n　結論から申しますと選択を変えた方が、変えなかった場合より当たる確率が2倍になります。これは条件付き確率とベイズの定理を用いることで数学的に説明できますが、ここではあえてモンテカルロ法で調べてみたいと思います。シミュレーションの具体的な手順は以下の通りです。\n\n結果を格納する長さ1万の空ベクトルを2つ用意する。 (Switch_YesとSwitch_No)。\niの初期値を1とする。\n当たり (車)の位置をA, B, Cの中から無作為に1つ決め、Car_Positionに格納する。\n最初の選択肢をA, B, Cの中から無作為に1つ決め、Choiceに格納する。\n選択肢を変更した場合の結果をSwitch_Yesのi番目の要素としてに格納する。\n\n当たりの位置と最初の選択肢が同じなら (Switch_Yes == Choice)、結果は外れ (ヤギ)\n当たりの位置と最初の選択肢が同じでないなら (Switch_Yes != Choice)、結果は当たり (車)\n\n選択肢を変更しなかった場合の結果をSwitch_Noのi番目の要素として格納する。\n\n当たりの位置と最初の選択肢が同じなら (Switch_Yes == Choice)、結果は当たり (車)\n当たりの位置と最初の選択肢が同じでないなら (Switch_Yes != Choice)、結果は外れ (ヤギ)\n\niの値を1増やし、3に戻る。\n3〜7の手順を1万回繰り返す。\n\n　以上の手順をコードで書くと以下のようになります。\n\nSwitch_Yes <- rep(NA, 10000)\nSwitch_No  <- rep(NA, 10000)\n\nset.seed(19861009)\nfor (i in 1:10000) {\n    Car_Position <- sample(c(\"A\", \"B\", \"C\"), 1)\n    Choice       <- sample(c(\"A\", \"B\", \"C\"), 1)\n    \n    Switch_Yes[i] <- ifelse(Car_Position == Choice, \"Goat\", \"Car\")\n    Switch_No[i]  <- ifelse(Car_Position == Choice, \"Car\", \"Goat\")\n}\n\ntable(Switch_Yes)\n\nSwitch_Yes\n Car Goat \n6737 3263 \n\ntable(Switch_No)\n\nSwitch_No\n Car Goat \n3263 6737 \n\n\n　選択肢を変更し、車を獲得した回数は10000回中、6737であり、約67%です。つまり、選択肢を変えた方が、変えなかった場合に比べ、車が当たる確率が約2倍高いことを意味します。むろん、車よりもヤギが重宝される地域に住んでいるなら、あえて選択肢を変えず、ヤギを狙った方が良いかも知れません。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html#monte-pi",
    "href": "tutorial/R/montecarlo_intro.html#monte-pi",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "例3: 円周率の計算",
    "text": "例3: 円周率の計算\n　今回はもう一つの例として、円周率 (\\(\\pi\\))の計算を紹介したいと思います。\\(\\pi\\)は無理数であるため、厳密な計算は出来ませんが、モンテカルロ・シミュレーションである程度近似できます。たとえば、半径1 (\\(r = 1\\))の円を考えてみましょう。\n\n\n\n\n\n\n\n\n\n　円の面積は\\(r^2\\pi\\)であるため、この円の面積は\\(\\pi\\)です。また、四角形は辺の長さが2の正四角形ですから面積は4です。続いて、四角形の範囲内の点を付けます。無作為に20個を付けてみます。\n\n\n\n\n\n\n\n\n\n　20個点のうち、円の外側にあるのは5個、円の内側は15個です。つまり、75%の点が円内にあることを意味します。点の位置は無作為ですので、もし円の大きさが\\(\\pi\\)であれば、点が円内に入る確率は\\(\\frac{\\pi}{4}\\)です。今回の例だと\\(\\frac{\\pi}{4} = 0.75\\)であるため、\\(\\pi = 0.75 \\times 4 = 3\\)となります。実際の円周率は3.141593…なので、そこそこ近似できていますね。\n　それではこれを実際にやってみましょう。今回は20個の点ではなく、100個にしてみましょう。まず、100個の点を無作為に抽出します。\n\nset.seed(19861009)\npi_df <- tibble(x = runif(100, -1, 1),\n                y = runif(100, -1, 1))\n\npi_df\n\n# A tibble: 100 × 2\n         x      y\n     <dbl>  <dbl>\n 1 -0.950  -0.344\n 2  0.0724  0.150\n 3  0.874   0.971\n 4  0.938   0.267\n 5  0.205   0.469\n 6 -0.343  -0.590\n 7 -0.0245 -0.635\n 8 -0.273  -0.886\n 9 -0.405   0.701\n10  0.528  -0.547\n# … with 90 more rows\n\n\n　まず、各辺の長さが2の正四角形とpi_dfで生成した100個の点をプロットします。四角形を描くときにはgeom_rect()幾何オブジェクトを使用します。マッピングは四角形の左下の座標 (xminとymin)、右上の座標 (xmaxとymax)に行います。今回は原点が (0, 0)の半径1の円に接する四角形ですから、左下の座標は (-1, -1)、右上の座標は (1, 1)となります。\n\npi_df %>%\n    ggplot() +\n    geom_rect(aes(xmin = -1, ymin = -1, xmax = 1, ymax = 1),\n              fill = \"white\", color = \"black\") +\n    geom_point(aes(x = x, y = y)) +\n    coord_fixed(ratio = 1) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n　ここに円を追加してみましょう。円を描くときには{ggforce}パッケージのgeom_circle()幾何オブジェクトを使用します。マッピングは円の原点 (x0とy0)、円の半径 (r)です。原点は (0, 0)で半径は1の円を重ねます。\n\npi_df %>%\n    ggplot() +\n    geom_rect(aes(xmin = -1, ymin = -1, xmax = 1, ymax = 1),\n              fill = \"white\", color = \"black\") +\n    geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +\n    geom_point(aes(x = x, y = y)) +\n    coord_fixed(ratio = 1) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n　これだけだと読みづらいので、円の中か外かで点の色分けをしてみましょう。そのためには各点が円内に入っているかどうかを判定した変数in_circleを追加します。点(\\(x\\), \\(y\\))が、原点が(\\(x^\\prime\\), \\(y^\\prime\\))、かつ半径\\(r\\)の円内に入っている場合、\\((x - x^\\prime)^2 + (y - y^\\prime)^2 < r^2\\)が成立します。今回は原点が (0, 0)で、半径が1であるため、\\(x^2 + y^2 < 1\\)か否かを判定します。この条件を満たしているかどうかを示すin_circleという変数を追加します。\n\npi_df <- pi_df %>%\n    mutate(in_circle = if_else(x^2 + y^2 < 1^2, \"円内\", \"円外\"))\n\n　散布図レイヤー (geom_point())内にcolorをin_circle変数でマッピングします。\n\npi_df %>%\n    ggplot() +\n    geom_rect(aes(xmin = -1, ymin = -1, xmax = 1, ymax = 1),\n              fill = \"white\", color = \"black\") +\n    # 円の内側か外側かで色分け\n    geom_point(aes(x = x, y = y, color = in_circle), size = 2) +\n    geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +\n    labs(x = \"X\", y = \"Y\", color = \"\") +\n    coord_fixed(ratio = 1) +\n    theme_void(base_size = 12)\n\n\n\n\n\n\n\n\n　実際に円内の点と円外の点の個数を数えてみましょう。\n\npi_df %>%\n    group_by(in_circle) %>%\n    summarise(N = n())\n\n# A tibble: 2 × 2\n  in_circle     N\n  <chr>     <int>\n1 円内         82\n2 円外         18\n\n\n　円内の点は82個、円外の点は18ですね。つまり、\\(\\frac{\\pi}{4} = 0.82\\)であり、\\(\\pi = 0.82 \\times 4 = 3.28\\)です。\n\n82 / 100 * 4\n\n[1] 3.28\n\n\n　今回は100個の点で円周率の近似値を計算しましたが、点の数を増やすとより正確な近似値が得られます。以下の例は10000個の点から得られた円周率の例です。\n\nset.seed(19861009)\npi_df2 <- tibble(x = runif(5000, -1, 1),\n                 y = runif(5000, -1, 1))\n\npi_df2 <- pi_df2 %>%\n    mutate(in_circle = if_else(x^2 + y^2 < 1, \"円内\", \"円外\"))\n\npi_df2 %>%\n    ggplot() +\n    geom_rect(aes(xmin = -1, ymin = -1, xmax = 1, ymax = 1),\n              fill = \"white\", color = \"black\") +\n    geom_point(aes(x = x, y = y, color = in_circle), size = 2) +\n    geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +\n    labs(x = \"X\", y = \"Y\", color = \"\") +\n    coord_fixed(ratio = 1) +\n    theme_void(base_size = 12)\n\n\n\n\n\n\n\npi_df2 %>%\n    group_by(in_circle) %>%\n    summarise(N = n())\n\n# A tibble: 2 × 2\n  in_circle     N\n  <chr>     <int>\n1 円内       3919\n2 円外       1081\n\n\n　円内の点は3919個、円外の点は1081ですね。この結果から円周率を計算してみましょう。\n\n3919 / 5000 * 4\n\n[1] 3.1352\n\n\n　より実際の円周率に近い値が得られました。"
  },
  {
    "objectID": "tutorial/R/montecarlo_intro.html#monte-bootstrap",
    "href": "tutorial/R/montecarlo_intro.html#monte-bootstrap",
    "title": "モンテカルロ・シミュレーション入門",
    "section": "例4: ブートストラップ法",
    "text": "例4: ブートストラップ法\n　最後に、様々な分析から得られた統計量の不確実性を計算する方法の一つであるブートストラップ法 (bootstrapping)について説明します。たとえば、平均値の差分の検定 (t検定)差、回帰分析における標準誤差などはRのt.test()、lm()関数を使用すれば瞬時に計算できます。こちらの標準誤差はデータを与えられれば、常に同じ値が得られるもので、何らかの計算式があります。しかし、世の中にはモデルが複雑すぎて、統計量の標準誤差がうまく計算できないケースもあります。そこで登場するのがブートストラップ法を用いると、不確実性の近似値が得られます。ブートストラップ法は @Efron:1979 が提案した以来、データ分析において広く使われています。ブートストラップ法については優れた教科書が多くあるので詳細な説明は割愛し、以下ではブートストラップ法の簡単な例を紹介します。\n　ブートストラップにおいて重要なのは元のデータセットのサンプルサイズを\\(n\\)とした場合、復元抽出を用いてサンプルサイズ\\(n\\)のデータセットをもう一度構築することです。そしてその平均値を計算します。これらの手順を5000回繰り返せば、5000個の平均値が得られます。この5000個の平均値の平均値は元のデータセットの平均値に近似し、5000個の平均値の標準偏差は元のデータセットの平均値の標準誤差 (標準誤差)に近似できます。これは本当でしょうか。実際にやってみましょう。\n　まずは、長さ100のベクトルを2つ作成します。この2つのベクトルは平均値が0.7、0.9、標準偏差1の正規分布に従うとします。\n\\[\n\\begin{aligned}\n\\mbox{Data1} & \\sim \\mbox{Normal}(\\mu = 0.7, \\sigma = 1) \\\\\n\\mbox{Data2} & \\sim \\mbox{Normal}(\\mu = 0.9, \\sigma = 1)\n\\end{aligned}\n\\]\n　2つの標本の平均値の差分は約0.2です。この差分の不確実性はいくらでしょうか。ここでは主に使われる平均値の差の検定 (\\(t\\)検定)をやってみましょう。回は標準誤差が同じ2つの分布から得られた標本であるため、等分散を仮定する\\(t\\)検定を行います (var.equal = TRUEを追加)。\n\nset.seed(19861009)\nData1 <- rnorm(100, 0.7, 1)\nData2 <- rnorm(100, 0.9, 1)\n\nttest_result <- t.test(Data1, Data2, var.equal = TRUE)\n\nttest_result\n\n\n    Two Sample t-test\n\ndata:  Data1 and Data2\nt = -2.1707, df = 198, p-value = 0.03114\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.58521800 -0.02807095\nsample estimates:\nmean of x mean of y \n0.6912143 0.9978588 \n\n\n平均値の差分の不確実性 (=標準誤差)はttest_result$stderrで抽出可能であり、今回は約0.141です。標準誤差の値さえ分かれば、検定統計量も、信頼区間も、\\(p\\)値も計算できるため、重要なのはやはり標準誤差でしょう。以下では試行回数は1万のブートストラップ法で標準誤差の近似値を計算してみます。\n\n結果を格納する長さ1万の空ベクトルResult_vecを作成する。\niの初期値を1と設定する。\nData1から100個 (= Data1の大きさ)の値を無作為抽出 (復元抽出)し、Sample1に格納する。\nData2から100個 (= Data2の大きさ)の値を無作為抽出 (復元抽出)し、Sample2に格納する。\nResult_vecのi番目の位置にSample1の平均値とSample2の平均値の差分を格納する。\niを1増加させる。\n3~6の手順を1万回繰り返す。\n\n\nn_trials   <- 10000\nResult_vec <- rep(NA, n_trials)\n\nfor (i in 1:n_trials) {\n    Sample1 <- sample(Data1, 100, replace = TRUE)\n    Sample2 <- sample(Data2, 100, replace = TRUE)\n    \n    Result_vec[i] <- mean(Sample1) - mean(Sample2)\n}\n\n　Result_vecには平均値の差分が10000個格納されており、これらの値の平均値をブートストラップ推定量 (bootstrap estimate)と呼ぶとします。\n\n# 元のデータの平均値の差分\ndelta <- mean(Data1) - mean(Data2)\ndelta\n\n[1] -0.3066445\n\n# ブートストラップ推定量 (平均値の差分の平均値)\nboot_delta <- mean(Result_vec)\nboot_delta\n\n[1] -0.3045563\n\n# ブートストラップ推定量のバイアス\nboot_b <- delta - boot_delta\n\n　実際の平均値の差分 (\\(\\delta_0\\)) は-0.3066445、ブートストラップ推定量 (\\(\\delta^*\\)) は -0.3045563であるため、その差は-0.0020881です。これをブートストラップ推定量のバイアス (\\(b\\)) と呼びます。\n　ただし、我々に興味があるのは平均値の差分ではありません。平均値の差分はブートストラップ法を用いなくても普通に計算できるからです。ここで重要なのは平均値の差分の不確実性、つまり標準誤差でしょう。ブートストラップ推定量の標準誤差はResult_vecの標準偏差を計算するだけで十分です。\n\nboot_se <- sd(Result_vec) # ブートストラップ推定量の標準偏差\n\n　ブートストラップ推定量の標準偏差 (\\(\\mbox{se}^*\\))は約0.14であり、t検定の結果から得られた標準誤差0.141と非常に近い値が得られました。\n　95%信頼区間を計算してみます。百分位数信頼区間 (Percentile CI)はResult_Vecの左側の領域が2.5%、右側の領域が2.5%となる区間ですので、quantile()関数で計算することができます。\n\n# 95%信頼区間\nquantile(Result_vec, c(0.025, 0.975))\n\n       2.5%       97.5% \n-0.58084484 -0.02999534 \n\n\n　バイアスを補正しないWald信頼区間は\\(\\delta_0 \\pm 1.96 \\times \\mbox{se}^*\\)のように計算します (1.96は標準正規分布の累積密度分布において下側領域が0.975となる点です。)。\n\ndelta + qnorm(c(0.025, 0.975)) * boot_se\n\n[1] -0.58201710 -0.03127185\n\n\n　バイアス修正Wald信頼区間は\\((\\delta_0 - b) \\pm 1.96 \\times \\mbox{se}^*\\)です。\n\n(delta - boot_b) + qnorm(c(0.025, 0.975)) * boot_se\n\n[1] -0.57992897 -0.02918372\n\n\n　これまでの結果を比較してみましょう。\n\n\n\n\n \n  \n     \n    t検定 \n    ブートストラップ \n  \n \n\n  \n    (1): 平均値の差分 \n    -0.307 \n    -0.305 \n  \n  \n    (2): 標準誤差 \n    0.141 \n    0.14 \n  \n  \n    (3): 95%信頼区間 \n    [-0.585, -0.028] \n     \n  \n  \n    (4): 95%信頼区間 (百分位数) \n     \n    [-0.581, -0.03] \n  \n  \n    (5): 95%信頼区間 (Wald) \n     \n    [-0.582, -0.031] \n  \n  \n    (6): 95%信頼区間 (バイアス修正Wald) \n     \n    [-0.58, -0.029] \n  \n\n\n\n\n\n　今回の例はt.test()関数を使えば一発で終わる問題ですが、モデルが複雑になれば推定量の不確実性の計算が難しくなるケースがあります。その時に力を発揮するのがブートストラップ方であり、{boot}、{bootstrap}、{simpleboot}など様々なパッケージが利用可能です。"
  },
  {
    "objectID": "tutorial/R/tidyr_intro.html",
    "href": "tutorial/R/tidyr_intro.html",
    "title": "Jaehyun Song, Ph.D.",
    "section": "",
    "text": "本章ではグラフの作成に適した形へデータを整形することについて学習します。ただし、これはグラフに限られた話ではありません。作図に適したデータは分析にも適します。"
  },
  {
    "objectID": "tutorial/R/tidyr_intro.html#tidydata-intro",
    "href": "tutorial/R/tidyr_intro.html#tidydata-intro",
    "title": "Jaehyun Song, Ph.D.",
    "section": "整然データ (tidy data)とは",
    "text": "整然データ (tidy data)とは\n分析や作図に適したデータの形は整然データ、または簡潔データ (tidy data)と呼ばれます。整然データの概念はtidyverse世界の産みの親であるHadely Wickham先生が提唱した概念であり、詳細は Wickham (2014) を参照してください。\n整然データは目指す到達点は非常に単純です。それは「データの構造 (structure)と意味 (semantic)を一致させる」ことです。そして、この「意味」を出来る限り小さい単位で分解します。\n例えば、3人で構成されたあるクラス内の被験者に対し、投薬前後に測定した数学成績があるとします。投薬前の成績は\"Control\"、投薬後の状況を\"Treatment\"とします。これをまとめたのが 表 1 です。\n\n\n\n\n\n\n表 1:  Messy Dataの例 (1) \n  \n  \n    \n      Name\n      Control\n      Treatment\n    \n  \n  \n    Hadley\n90\n90\n    Song\n80\n25\n    Yanai\n100\n95\n  \n  \n  \n\n\n\n\n\nまた、以上の表は転置も可能であり、以下のように表現することが可能です ( 表 2 )。\n\n\n\n\n\n\n表 2:  Messy Dataの例 (2) \n  \n  \n    \n      Treat\n      Hadely\n      Song\n      Yanai\n    \n  \n  \n    Control\n90\n80\n100\n    Treatment\n90\n25\n95\n  \n  \n  \n\n\n\n\n\n2つのデータが持つ情報は全く同じです。これは「同じ意味を持つが、異なる構造を持つ」とも言えます。このような多様性が生じる理由は行と列のあり方が各値を説明するに十分ではないからです。異なるデータ構造として表現される余地があるということです。\nたとえば、 表 1 の場合、各列は以下のような3つの情報があります。\n\nName: 被験者名\nControl: 投薬前の数学成績\nTreatment: 投薬後の数学成績\n\nこのデータの問題は「投薬有無」と「数学成績」が2回登場したという点です。1は問題ありませんが、2と3の値は「投薬有無 \\(\\times\\) 数学成績」の組み合わせです。一つの変数に2つの情報が含まれていますね。これによって、投薬有無を行にしても列にしてもいいわけです。「ならばこっちの方が柔軟だしいいのでは?」と思う方もいるかも知れません。しかし、パソコンはこの曖昧さが嫌いです。なぜなら、人間のような思考ができないからです。データフレームは縦ベクトルの集合であるから、各列には一つの情報のみ格納する必要があります。たとえば、以下のように列を変更するとしましょう。\n\nName: 被験者名\nTreat: 投薬有無\nMath_Score: 数学成績\n\nTreatは投薬前なら\"Control\"の値を、投薬後なら\"Treatment\"の値が入ります。Math_Socreには数学成績が入ります。これに則って表に直したのが 表 3 です。\n\n\n\n\n\n\n表 3:  整然データの例 \n  \n  \n    \n      Name\n      Treat\n      Math_Score\n    \n  \n  \n    Hadley\nControl\n90\n    Hadley\nTreatment\n90\n    Song\nControl\n80\n    Song\nTreatment\n25\n    Yanai\nControl\n100\n    Yanai\nTreatment\n95\n  \n  \n  \n\n\n\n\n\n表が長くなりましたが、これなら一つの列に2つ以上の情報が含まれることはありません。この場合、 表 1 と 表 2 のように、行と列を転置することができるでしょうか。\n\n\n\n\n\n表 4:  転置された表の例 \n \n  \n    Name \n    Hadley \n    Hadley \n    Song \n    Song \n    Yanai \n    Yanai \n  \n \n\n  \n    Treat \n    Control \n    Treatment \n    Control \n    Treatment \n    Control \n    Treatment \n  \n  \n    Math_Score \n    90 \n    90 \n    80 \n    25 \n    100 \n    95 \n  \n\n\n\n\n\n\nその結果が 表 4 ですが、いかがでしょうか。まず、列名が重複している時点でアウトですし、人間が見ても非常に分かりにくい表になりました。また、一つの列に異なるデータ (この場合、character型とnumeirc型)が混在しています。パソコンから見てはわけのわからないデータになったわけです。\nここまで来たら整然データのイメージはある程度掴めたかも知れません。具体的に整然データとは次の4つの条件を満たすデータです(Wickham 2014)。\n\n1つの列は、1つの変数を表す。\n1つの行は、1つの観測を表す。\n1つのセル（特定の列の特定の行）は、1つの値を表す。\n1つの表は、1つの観測単位 (unit of observation)をもつ（異なる観測単位が混ざっていない）。\n\n以下でも、 表 1 と@tbl-tidydata-intro-3 を対比しながら、以上の4条件をより詳しく説明します。\n\n1つの列は、1つの変数を表す\n表 1 と 表 3 に含まれる情報は以下の3つで共通しています。\n\n被験者名\n投薬有無\n数学成績\n\nこれらの情報がそれぞれデータの変数になるわけですが、整然データは一つの列が一つの変数を表します。それではまず、 表 1 ( 図 1 の左)から考えてみましょう。この図には3つの情報が全て含まれています。しかし、数学成績は2列に渡って格納されており、「1列1変数」の条件を満たしておりません。一方、 表 3 ( 図 1 の右)は投薬前後を表すTreat変数を作成し、その値に応じた数学成績が格納されており、「1列1変数」の条件を満たしています。\n\n\n\n\n\n図 1: 1つの列は、1つの変数を表す\n\n\n\n\n「1列1変数」は整然データの最も基本となる条件であり、整然データ作成の出発点とも言えます。\n\n\n1つの行は、1つの観測を表す\n図 2 の左は一行当たり、いくつの観察が含まれているでしょうか。そのためにはこのデータが何を観察しているかを考える必要があります。このデータは投薬前後の数学成績を観察し、量的に測定したものです。つまり、同じ人に対して2回観察を行ったことになります。したがって、投薬前の数学成績と投薬後の数学成績は別の観察であり、 図 2 の左は3行の表ですが、実は6回分の観察が含まれていることになります。1行に2つの観察が載っていることですね。\n\n\n\n\n\n図 2: 1つの行は、1つの観測を表す\n\n\n\n\n一方、 図 2 の右は6行のデータであり、観察回数とデータの行数が一致しています。つまり、1行に1観察となります。\n今回は数学成績しか測っていたいので、簡単な例ですが、実際のデータには曖昧な部分があります。たとえば、投薬によって血圧が変化する可能性があるため、最高血圧もまた投薬前後に測定したとします。それが 表 5 の左です。\n\n\n表 5: 1行1観察の例\n\n\n\n\n\n\n(a) 1行1観察? \n  \n  \n    \n      Name\n      Treat\n      Math\n      Blood\n    \n  \n  \n    Hadley\nControl\n90\n110\n    Hadley\nTreatment\n90\n115\n    Song\nControl\n80\n95\n    Song\nTreatment\n25\n110\n    Yanai\nControl\n100\n100\n    Yanai\nTreatment\n95\n95\n  \n  \n  \n\n\n\n\n\n\n\n(b) これは? \n  \n  \n    \n      Name\n      Treat\n      Type\n      Value\n    \n  \n  \n    Hadley\nControl\nMath\n90\n    Hadley\nControl\nBlood\n110\n    Hadley\nTreatment\nMath\n90\n    Hadley\nTreatment\nBlood\n115\n    Song\nControl\nMath\n80\n    Song\nControl\nBlood\n95\n    Song\nTreatment\nMath\n25\n    Song\nTreatment\nBlood\n110\n    Yanai\nControl\nMath\n100\n    Yanai\nControl\nBlood\n100\n    Yanai\nTreatment\nMath\n95\n    Yanai\nTreatment\nBlood\n95\n  \n  \n  \n\n\n\n\n\n3人に投薬前後に数学成績と最高血圧を測定した場合の観察回数は何回でしょう。3人 \\(\\times\\) 2時点 \\(\\times\\) 2指標の測定だから12回の測定でしょうか。ならば、 表 5 の右が整然データでしょう。しかし、この場合、1列1変数という条件が満たされなくなります。Value列には数学成績と血圧が混在しており、2つの変数になります。ならば、どれも整然データではないということでしょうか。実は整然データは 表 5 の左です。なぜなら、「1観察=1値」ではないからです。データにおける観察とは観察単位ごとに測定された値の集合です。観察対象とは人や自治体、企業、国などだけでなく、時間も含まれます。たとえば、人の特徴 (性別、身長、所得、政治関心など)を測定しもの、ある日の特徴 (気温、株価など)を測定したもの全てが観察です。むろん、人 \\(\\times\\) 時間のような組み合わせが観察単位ともなり得ます。この一つ一つの観察単位から得られた値の集合が観察です。 表 5 の分析単位は「人 \\(\\times\\) 時間」です。成績や最高血圧は分析単位が持つ特徴や性質であって、分析単位ではありません。\n\n\n1つのセルは、1つの値を表す\nこの条件に反するケースはあまりないかも知れません。たとえば、「Hadleyは処置前後の数学成績が同じだし、一行にまとめよう」という意味で 図 3 のような表を作る方もいるかも知れませんが、あまりいないでしょう。\n\n\n\n\n\n図 3: 1つのセルは、1つの値を表す\n\n\n\n\n図 3 の例は「1セル1値」の条件に明らかに反します。しかし、基準が曖昧な変数もあり、その一つが日付です。\n\n\n表 6: 日付の扱い方\n\n\n\n\n\n\n(a) 例1 \n  \n  \n    \n      Date\n      Stock\n    \n  \n  \n    2020/06/29\n100\n    2020/06/30\n105\n    2020/07/01\n110\n    2020/07/02\n85\n    2020/07/03\n90\n  \n  \n  \n\n\n\n\n\n\n\n(b) 例2 \n  \n  \n    \n      Year\n      Month\n      Date\n      Stock\n    \n  \n  \n    2020\n6\n29\n100\n    2020\n6\n30\n105\n    2020\n7\n1\n110\n    2020\n7\n2\n85\n    2020\n7\n3\n90\n  \n  \n  \n\n\n\n\n\n表 6 の左側の表はどうでしょうか。5日間の株価を記録した架空のデータですが、たしかにDate列には日付が1つずつ、Stockには株価の値が1つずつ格納されています。しかし、解釈によっては「Dateに年、月、日といった3つの値が含まれているぞ」と見ることもできます。この解釈に基づく場合、 表 6 の右側の表が整然データとなり、左側は雑然データとなります。このケースは第一条件であった「一列一変数」とも関係します。なぜなら、Dateという列が年・月・日といった3変数で構成されているとも解釈できるからです。\n分析によっては左側のような表でも全く問題ないケースもあります。時系列分析でトレンド変数のみ必要ならこれでも十分に整然データと呼べます。しかし、季節変動などの要素も考慮するならば、左側は雑然データになります。データとしての使い勝手は右側の方が優れているのは確かです。\nデータを出来る限り細かく分解するほど情報量が豊かになりますが、それにも限度はあるでしょう。たとえば、「Yearは実は世紀の情報も含まれているのでは…?」という解釈もできますが、これを反映してデータ整形を行うか否かは分析の目的と分析モデルによって異なります。この意味で、明らかな雑然データはあり得ますが、明らかな整然データは存在しないでしょう。どちらかといえば、整然さの度合いがあり、「これなら十分に整然データと言えないだろうか」と判断できれば十分ではないかと筆者 (Song)は考えます。\n\n\n1つの表は、1つの観測単位をもつ\ne-statなどから国勢調査データをダウンロードした経験はあるでしょうか。以下の 図 4 は2015年度国勢調査データの一部です。\n\n\n\n\n\n図 4: 国勢調査データ\n\n\n\n\nこのデータの観察単位はなんでしょうか。データのの1行目は全国の人口を表しています。つまり、単位は国となります。しかし、2行目は北海道の人口です。この場合の観測単位は都道府県となります。つづいて、3行目は札幌市なので単位は市区町村になります。4行目は札幌市中央区、つまり観測単位が行政区になっています。そして14行目は函館市でまた単位は市区町村に戻っています。実際、会社や政府が作成するデータには 図 4 や 図 5 のようなものが多いです。とりわけ、 図 5 のように、最後の行に「合計」などが表記されている場合が多いです。\n\n\n\n\n\n図 5: 1つの表は、1つの観測単位をもつ\n\n\n\n\nこのような表・データを作成することが悪いことではありません。むしろ、「読む」ための表ならこのような書き方が一般的でしょう。しかし、「分析」のためのデータは観察の単位を統一する必要があります。"
  },
  {
    "objectID": "tutorial/R/tidyr_intro.html#tidydata-gather",
    "href": "tutorial/R/tidyr_intro.html#tidydata-gather",
    "title": "Jaehyun Song, Ph.D.",
    "section": "Wide型からLong型へ",
    "text": "Wide型からLong型へ\n以下では「1列1変数」の条件を満たすデータの作成に便利なpivot_longer()とpivot_wider()関数について解説します。この関数群はおなじみの{dplyr}でなく、{tidyr}パッケージが提供している関数ですが、どれも{tidyverse}パッケージ群に含まれているため、{tidyverse}パッケージを読み込むだけで十分です。本節ではpivot_longer()を、次節ではpivot_wider()を取り上げます。\nまず、pivot_longer()ですが、この関数は比較的に新しい関数であり、これまでは{tidyr}のgather()関数が使われてきました。しかし、gahter()関数は将来、なくなる予定の関数であり、今から{tidyr}を学習する方はpivot_*()関数群に慣れておきましょう。\nまずは{tidyverse}パッケージを読み込みます。\n\npacman::p_load(tidyverse)\n\n今回は様々な形のデータを変形する作業をするので、あるデータセットを使うよりも、架空の簡単なデータを使います。\n\ndf1 <- tibble(\n  Name      = c(\"Hadley\", \"Song\", \"Yanai\"),\n  Control   = c(90, 80, 100),\n  Treatment = c(90, 25, 95),\n  Gender    = c(\"Male\", \"Female\", \"Female\")\n)\n\ndf1\n\n# A tibble: 3 × 4\n  Name   Control Treatment Gender\n  <chr>    <dbl>     <dbl> <chr> \n1 Hadley      90        90 Male  \n2 Song        80        25 Female\n3 Yanai      100        95 Female\n\n\nこのデータは既に指摘した通り「1列1変数」の条件を満たしております。この条件を満たすデータは以下のような形となります。\n\n\n# A tibble: 6 × 4\n  Name   Gender Treat     Math_Score\n  <chr>  <chr>  <chr>          <dbl>\n1 Hadley Male   Control           90\n2 Hadley Male   Treatment         90\n3 Song   Female Control           80\n4 Song   Female Treatment         25\n5 Yanai  Female Control          100\n6 Yanai  Female Treatment         95\n\n\nTreat変数が作成され、元々は変数名であった\"Control\"と\"Treatment\"が値として格納されます。この変数をキー変数と呼びます。そして、キー変数の値に応じた数学成績がMath_Scoreという変数でまとめられました。この変数を値変数と呼びます。\n「1列1変数」を満たさなかった最初のデータは「Wide型データ」、これを満たすようなデータは「Long型データ」と呼ばれます。これは相対的に最初のデータが横に広いから名付けた名前であって、「Wide型=雑然データ」もしくは「Long型=雑然データ」ではないことに注意してください1。\nWide型データをLong型へ変換する関数がpivot_longer()であり、基本的な使い方は以下の通りです。\n\n# pivot_longer()の使い方\nデータ名 %>%\n  pivot_longer(cols      = c(まとめる変数1, まとめる変数2, ...),\n               names_to  = \"キー変数名\",\n               values_to = \"値変数名\")\n\nここでは同じ変数がControlとTreatment変数で分けられているため、まとめる変数はこの2つであり、cols = c(Control, Treatment)と指定します。ControlとTreatmentは\"で囲んでも、囲まなくても同じです。また、{dplyr}のselect()関数で使える変数選択の関数 (starts_with()、where()など)や:演算子も使用可能です。また、cols引数はpivot_longer()の第2引数であるため、cols =は省略可能です（第一引数はパイプにより既に渡されています）。\nnames_toとvalues_to引数はそれぞれキー変数名と値変数名を指定する引数で、ここは必ず\"で囲んでください。このdf1をLong型へ変換し、df1_Lと名付けるコードが以下のコードです。\n\ndf1_L <- df1 %>%\n  pivot_longer(Control:Treatment,\n               names_to  = \"Treat\",\n               values_to = \"Math_Score\")\n\ndf1_L\n\n# A tibble: 6 × 4\n  Name   Gender Treat     Math_Score\n  <chr>  <chr>  <chr>          <dbl>\n1 Hadley Male   Control           90\n2 Hadley Male   Treatment         90\n3 Song   Female Control           80\n4 Song   Female Treatment         25\n5 Yanai  Female Control          100\n6 Yanai  Female Treatment         95\n\n\nこれだけでもpivot_longer()関数を使ってWide型からLong型への変換は問題なくできますが、以下ではもうちょっと踏み込んだ使い方について解説します。「ここまでで十分だよ」という方は、ここを飛ばしても構いません。\n今回の実習データdf3は3人の体重を3日間に渡って計測したものです。ただし、ドジっ子のSongは2日目にうっかり測るのを忘れており、欠損値となっています。\n\ndf2 <- tibble(\n  Name = c(\"Hadley\", \"Song\", \"Yanai\"),\n  Day1 = c(75, 120, 70),\n  Day2 = c(73,  NA, 69),\n  Day3 = c(71, 140, 71)\n)\n\ndf2\n\n# A tibble: 3 × 4\n  Name    Day1  Day2  Day3\n  <chr>  <dbl> <dbl> <dbl>\n1 Hadley    75    73    71\n2 Song     120    NA   140\n3 Yanai     70    69    71\n\n\nまず、これをこれまでのやり方でLong型へ変形し、df2_Lと名付けます。\n\ndf2_L <- df2 %>%\n  pivot_longer(starts_with(\"Day\"),\n               names_to  = \"Days\",\n               values_to = \"Weight\")\n\ndf2_L\n\n# A tibble: 9 × 3\n  Name   Days  Weight\n  <chr>  <chr>  <dbl>\n1 Hadley Day1      75\n2 Hadley Day2      73\n3 Hadley Day3      71\n4 Song   Day1     120\n5 Song   Day2      NA\n6 Song   Day3     140\n7 Yanai  Day1      70\n8 Yanai  Day2      69\n9 Yanai  Day3      71\n\n\nこれでも問題ないかも知れませんが、以下のような操作を追加に行うとします。\n\nWeightが欠損している行を除去する\nDays列の値から\"Day\"を除去し、numeric型にする\n\n以上の作業を行うには、dplyrが便利でしょう。ちなみにstr_remove()関数が初めて登場しましたが、簡単に説明しますと、str_remove(\"X123\", \"X\")は\"X123\"から\"X\"を除去し、\"123\"のみ残す関すです。残された値が数字のみであってもデータ型はcharacter型なので、もう一回、numeric型に変換する必要があります2。dplyrを使ったコードは以下の通りです。\n\n# 1. WeightがNAのケースを除去\n# 2. Days変数の値から\"Day\"を除去\n# 3. Days変数をnumeric型へ変換\ndf2_L %>%\n  filter(!is.na(Weight)) %>%             # 1\n  mutate(Days = str_remove(Days, \"Day\"), # 2\n         Days = as.numeric(Days))        # 3\n\n# A tibble: 8 × 3\n  Name    Days Weight\n  <chr>  <dbl>  <dbl>\n1 Hadley     1     75\n2 Hadley     2     73\n3 Hadley     3     71\n4 Song       1    120\n5 Song       3    140\n6 Yanai      1     70\n7 Yanai      2     69\n8 Yanai      3     71\n\n\n実はこの作業、pivot_longer()内で行うことも可能です。たとえば、values_toで指定した変数の値が欠損しているケースを除去するにはvalues_drop_na引数をTRUEに指定するだけです。\n\n# Weight変数がNAのケースを除去する\ndf2 %>%\n  pivot_longer(starts_with(\"Day\"),\n               names_to       = \"Days\",\n               values_to      = \"Weight\",\n               values_drop_na = TRUE)\n\n# A tibble: 8 × 3\n  Name   Days  Weight\n  <chr>  <chr>  <dbl>\n1 Hadley Day1      75\n2 Hadley Day2      73\n3 Hadley Day3      71\n4 Song   Day1     120\n5 Song   Day3     140\n6 Yanai  Day1      70\n7 Yanai  Day2      69\n8 Yanai  Day3      71\n\n\nそれでは、キー変数から共通する文字列を除去するにはどうすれば良いでしょうか。この場合、names_prefix引数を使います。これはnames_toで指定した新しく出来る変数の値における接頭詞を指定し、それを除去する引数です。今回は\"Day1\"、\"Day2\"、\"Day3\"から\"Day\"を除去するので、names_prefix = \"Day\"と指定します。こうすることで、Days列から\"Day\"が除去されます。ただし、数字だけ残っても、そのデータ型はcharacter型ですので、このデータ型を変換する必要があります。ここで使うのがnames_transform引数であり、これはlist型のオブジェクトを渡す必要があります。Days列をnumeric型にする場合はlist(Days = as.numeric)です。複数の列のデータ型を変える場合、list()の中に追加していきます。それでは実際に走らせてみましょう。\n\n# 1. Day変数の値から\"Day\"を除去する\n# 2. Day変数をinteger型に変換\n# 3. Weight変数がNAのケースを除去する\ndf2 %>%\n  pivot_longer(starts_with(\"Day\"),\n               names_to        = \"Days\",\n               names_prefix    = \"Day\",                   # 1\n               names_transform = list(Days = as.numeric), # 2\n               values_to       = \"Weight\",\n               values_drop_na  = TRUE)                    # 3\n\n# A tibble: 8 × 3\n  Name    Days Weight\n  <chr>  <dbl>  <dbl>\n1 Hadley     1     75\n2 Hadley     2     73\n3 Hadley     3     71\n4 Song       1    120\n5 Song       3    140\n6 Yanai      1     70\n7 Yanai      2     69\n8 Yanai      3     71\n\n\nこれでWide型をLong型が変換され、整然でありながら、より見栄の良いデータが出来上がりました。他にもpivot_longer()は様々な引数に対応しており、詳細は?pivot_longerやレファレンスページを参照してください。"
  },
  {
    "objectID": "tutorial/R/tidyr_intro.html#tidydata-spread",
    "href": "tutorial/R/tidyr_intro.html#tidydata-spread",
    "title": "Jaehyun Song, Ph.D.",
    "section": "Long型からWide型へ",
    "text": "Long型からWide型へ\nご存知の通り、「Long型データ=整然データ」ではありません。実際、 表 5 の右はLong型データですが、1列に2つの変数が含まれており、整然データとは言えません。このようなデータはいくらでもあります。とりわけ、「分析」のためじゃなく、「読む」ための表の場合において多く発見されます。\n\n\n表 7: Long型データの例\n\n\n\n\n\n\n(a) 非整然データ \n  \n  \n    \n      都道府県\n      区分\n      人口\n      面積\n    \n  \n  \n    北海道\n総人口\n5381733\n83424.31\n    \n外国人\n21676\n83424.31\n    青森県\n総人口\n1308265\n9645.59\n    \n外国人\n3447\n9645.59\n    岩手県\n総人口\n1279594\n15275.01\n    \n外国人\n5017\n15275.01\n    宮城県\n総人口\n2333899\n7282.22\n    \n外国人\n13989\n7282.22\n  \n  \n  \n\n\n\n\n\n\n\n(b) 整然データ \n  \n  \n    \n      都道府県\n      総人口\n      外国人\n      面積\n    \n  \n  \n    北海道\n5381733\n21676\n83424.31\n    青森県\n1308265\n3447\n9645.59\n    岩手県\n1279594\n5017\n15275.01\n    宮城県\n2333899\n13989\n7282.22\n  \n  \n  \n\n\n\n\n\n変数名が日本語になっていますが、これは「読むための表」を読み込むことを仮定しています。このように変数名として日本語は使えますが、自分でデータセットを作成する際、変数名はローマ字にすることを強く推奨します。\n表 7 の左の場合、人口列に総人口と外国人人口といった2つの変数の値が格納されているため、整然データではありません。これを整然データにしたものが右の表です。本節ではLong型データをWide型データへ変換するpivot_wider()関数を紹介します。この関数は同じく{tidyr}が提供しているspread()関数とほぼ同じ関数ですが、今はpivot_wider()の使用が推奨されており、spread()はいずれか{tidyr}から外される予定です。\nまずは、実習用データを読み込みます。\n\ndf3 <- read_csv(\"Data/Population2015.csv\")\n\ndf3\n\n# A tibble: 94 × 4\n   都道府県 区分      人口   面積\n   <chr>    <chr>    <dbl>  <dbl>\n 1 北海道   総人口 5381733 83424.\n 2 <NA>     外国人   21676 83424.\n 3 青森県   総人口 1308265  9646.\n 4 <NA>     外国人    3447  9646.\n 5 岩手県   総人口 1279594 15275.\n 6 <NA>     外国人    5017 15275.\n 7 宮城県   総人口 2333899  7282.\n 8 <NA>     外国人   13989  7282.\n 9 秋田県   総人口 1023119 11638.\n10 <NA>     外国人    2914 11638.\n# … with 84 more rows\n\n\nこのデータは2015年国勢調査から抜粋したデータであり、各変数の詳細は以下の通りです。\n\n\n\n\n\n\n表 8:  データの概要 \n  \n  \n    \n      変数名\n      説明\n    \n  \n  \n    都道府県\n都道府県名\n    区分\n総人口/外国人人口の区分\n    人口\n人口 (人)\n    面積\n面積 (km$^2$)\n  \n  \n  \n\n\n\n\n\nまずは変数名が日本語になっているので、rename()関数を使ってそれぞれPref、Type、Population、Areaに変更します。\n\ndf3 <- df3 %>%\n  rename(\"Pref\" = 都道府県, \"Type\" = 区分, \"Population\" = 人口, \"Area\" = 面積)\n\ndf3\n\n# A tibble: 94 × 4\n   Pref   Type   Population   Area\n   <chr>  <chr>       <dbl>  <dbl>\n 1 北海道 総人口    5381733 83424.\n 2 <NA>   外国人      21676 83424.\n 3 青森県 総人口    1308265  9646.\n 4 <NA>   外国人       3447  9646.\n 5 岩手県 総人口    1279594 15275.\n 6 <NA>   外国人       5017 15275.\n 7 宮城県 総人口    2333899  7282.\n 8 <NA>   外国人      13989  7282.\n 9 秋田県 総人口    1023119 11638.\n10 <NA>   外国人       2914 11638.\n# … with 84 more rows\n\n\n次は、Pref列の欠損値を埋めましょう。ここの欠損値は、当該セルの一つ上のセルの値で埋まりますが、これはfill()関数で簡単に処理できます。欠損値を埋めたい変数名をfill()の引数として渡すだけです。\n\ndf3 <- df3 %>%\n  fill(Pref)\n\ndf3\n\n# A tibble: 94 × 4\n   Pref   Type   Population   Area\n   <chr>  <chr>       <dbl>  <dbl>\n 1 北海道 総人口    5381733 83424.\n 2 北海道 外国人      21676 83424.\n 3 青森県 総人口    1308265  9646.\n 4 青森県 外国人       3447  9646.\n 5 岩手県 総人口    1279594 15275.\n 6 岩手県 外国人       5017 15275.\n 7 宮城県 総人口    2333899  7282.\n 8 宮城県 外国人      13989  7282.\n 9 秋田県 総人口    1023119 11638.\n10 秋田県 外国人       2914 11638.\n# … with 84 more rows\n\n\nそして、いよいよpivot_wider()関数の出番ですが、基本的に使い方は以下の通りです。\n\n# pivot_wider()の使い方\nデータ名 %>%\n  pivot_wider(names_from  = キー変数名,\n              values_from = 値変数名)\n\nまず、キー変数名は列として展開する変数名であり、ここではTypeになります。そして、値変数名は展開される値の変数であり、ここではPopulationになります。つまり、「PopulationをTypeごとに分けて別の列にする」ことになります。また、values_from引数は長さ2以上のベクトルを指定することで、複数の値変数を指定することも可能です。たとえば、df3にIncomeという平均所得を表す列があり、これらも総人口と外国人それぞれ異なる値を持っているとしたら、values_from = c(Population, Income)のように複数の値変数を指定することが出来ます。今回は値変数が1つのみですが、早速やってみましょう。\n\ndf3_W <- df3 %>%\n  pivot_wider(names_from  = Type,\n              values_from = Population)\n\ndf3_W\n\n# A tibble: 47 × 4\n   Pref     Area  総人口 外国人\n   <chr>   <dbl>   <dbl>  <dbl>\n 1 北海道 83424. 5381733  21676\n 2 青森県  9646. 1308265   3447\n 3 岩手県 15275. 1279594   5017\n 4 宮城県  7282. 2333899  13989\n 5 秋田県 11638. 1023119   2914\n 6 山形県  9323. 1123891   5503\n 7 福島県 13784. 1914039   8725\n 8 茨城県  6097. 2916976  41310\n 9 栃木県  6408. 1974255  26494\n10 群馬県  6362. 1973115  37126\n# … with 37 more rows\n\n\nまた、日本語の変数名が出来てしまったので、それぞれTotalとForeignerに変更し、relocate()関数を使ってAreaを最後の列に移動します。\n\ndf3_W <- df3_W %>%\n  rename(\"Total\"     = 総人口,\n         \"Foreigner\" = 外国人) %>%\n  relocate(Area, .after = last_col())\n\ndf3_W\n\n# A tibble: 47 × 4\n   Pref     Total Foreigner   Area\n   <chr>    <dbl>     <dbl>  <dbl>\n 1 北海道 5381733     21676 83424.\n 2 青森県 1308265      3447  9646.\n 3 岩手県 1279594      5017 15275.\n 4 宮城県 2333899     13989  7282.\n 5 秋田県 1023119      2914 11638.\n 6 山形県 1123891      5503  9323.\n 7 福島県 1914039      8725 13784.\n 8 茨城県 2916976     41310  6097.\n 9 栃木県 1974255     26494  6408.\n10 群馬県 1973115     37126  6362.\n# … with 37 more rows\n\n\nこれで整然データの出来上がりです。\nこのpivot_wider()関数はpivot_longer()関数同様、様々な引数を提供しておりますが、主に使う機能は以上です。他にはpivot_wider()によって出来た欠損値を埋める引数であるvalues_fillがあり、デフォルト値はNULLです。ここに0や\"Missing\"などの長さ1のベクトルを指定すれば、指定した値で欠損値が埋まります。\npivot_wider()関数の詳細は?pivot_widerもしくは、レファレンスページを参照してください。"
  },
  {
    "objectID": "tutorial/R/tidyr_intro.html#tidydata-separate",
    "href": "tutorial/R/tidyr_intro.html#tidydata-separate",
    "title": "Jaehyun Song, Ph.D.",
    "section": "列の操作",
    "text": "列の操作\n他にも「1列1変数」の条件を満たさないケースを考えましょう。pivot_longer()は1つの変数が複数の列に渡って格納されている際に使いましたが、今回は1つの列に複数の変数があるケースを考えてみましょう。たとえば、年月日が1つの列に入っている場合、これを年、月、日の3列で分割する作業です。また、これと関連して、列から文字列を除去し、数値のみ残す方法についても紹介します。\n実習用データを読み込んでみましょう。\n\ndf4 <- read_csv(\"Data/COVID19_JK.csv\")\n\ndf4\n\n# A tibble: 172 × 5\n      ID Date      Week  Confirmed_Japan Confirmed_Korea\n   <dbl> <chr>     <chr> <chr>           <chr>          \n 1     1 2020/1/16 木    1人             <NA>           \n 2     2 2020/1/17 金    0人             <NA>           \n 3     3 2020/1/18 土    0人             <NA>           \n 4     4 2020/1/19 日    0人             <NA>           \n 5     5 2020/1/20 月    0人             1人            \n 6     6 2020/1/21 火    0人             0人            \n 7     7 2020/1/22 水    0人             0人            \n 8     8 2020/1/23 木    0人             0人            \n 9     9 2020/1/24 金    2人             1人            \n10    10 2020/1/25 土    0人             0人            \n# … with 162 more rows\n\n\nこのデータは2020年1月16日から2020年7月5日まで、COVID-19 (新型コロナ)の新規感染者数を日本と韓国を対象に収集したものです。データはWikipedia (日本 / 韓国)から収集しました。韓国の新規感染者数は最初の4日分が欠損値のように見えますが、最初の感染者が確認されたのが1月20日のため、1月19日までは欠損となっています。\n\n\n\n\n\n\n表 9:  データの概要 \n  \n  \n    \n      変数名\n      説明\n    \n  \n  \n    ID\nケースID\n    Date\n年月日\n    Week\n曜日\n    Confirmed_Japan\n新規感染者数 (日本)\n    Confirmed_Korea\n新規感染者数 (韓国)\n  \n  \n  \n\n\n\n\n\nこのデータの場合、観察単位は「国 \\(\\times\\) 日」です。しかし、df4は1行に日本と韓国の情報が格納されており「1行1観察」の条件を満たしておりません。したがって、pivot_longer()を使ってLong型へ変換し、新しいデータの名前をdf4_Lと名付けます。\n\ndf4_L <- df4 %>%\n  pivot_longer(cols         = starts_with(\"Confirmed\"), \n               names_to     = \"Country\",\n               names_prefix = \"Confirmed_\",\n               values_to    = \"Confirmed\")\n\ndf4_L\n\n# A tibble: 344 × 5\n      ID Date      Week  Country Confirmed\n   <dbl> <chr>     <chr> <chr>   <chr>    \n 1     1 2020/1/16 木    Japan   1人      \n 2     1 2020/1/16 木    Korea   <NA>     \n 3     2 2020/1/17 金    Japan   0人      \n 4     2 2020/1/17 金    Korea   <NA>     \n 5     3 2020/1/18 土    Japan   0人      \n 6     3 2020/1/18 土    Korea   <NA>     \n 7     4 2020/1/19 日    Japan   0人      \n 8     4 2020/1/19 日    Korea   <NA>     \n 9     5 2020/1/20 月    Japan   0人      \n10     5 2020/1/20 月    Korea   1人      \n# … with 334 more rows\n\n\n続いて、新規感染者数を表すConfirmed列から「人」を除去しましょう。人間にとってはなんの問題もありませんが、パソコンにとって1人や5人は文字列に過ぎず、分析ができる状態ではありません。ここで使う関数がparse_number()です。引数として指定した列から数値のみ抽出します。\"$1000\"や\"1, 324, 392\"のような数値でありながら、character型として保存されている列から数値のみを取り出す際に使う関数です。使い方は以下の通りです。\n\nデータ名 %>%\n  mutate(新しい変数名 = parse_number(数値のみ抽出する変数名))\n\n似たようなものとしてparse_character()があり、これは逆に文字列のみ抽出する関数です。\nここではConfimedから数値のみ取り出し、Confrimed列に上書きし、それをdf4_Sと名付けます。\n\ndf4_S <- df4_L %>%\n  mutate(Confirmed = parse_number(Confirmed))\n\ndf4_S\n\n# A tibble: 344 × 5\n      ID Date      Week  Country Confirmed\n   <dbl> <chr>     <chr> <chr>       <dbl>\n 1     1 2020/1/16 木    Japan           1\n 2     1 2020/1/16 木    Korea          NA\n 3     2 2020/1/17 金    Japan           0\n 4     2 2020/1/17 金    Korea          NA\n 5     3 2020/1/18 土    Japan           0\n 6     3 2020/1/18 土    Korea          NA\n 7     4 2020/1/19 日    Japan           0\n 8     4 2020/1/19 日    Korea          NA\n 9     5 2020/1/20 月    Japan           0\n10     5 2020/1/20 月    Korea           1\n# … with 334 more rows\n\n\nそれでは国、曜日ごとの新規感染者数を調べてみます。求める統計量は曜日ごとの新規感染者数の合計、平均、標準偏差です。まず、曜日は月から日の順になるよう、factor型に変換します。そして、国と曜日ごとに記述統計量を計算し、df4_S_Summary1という名で保存します。\n\ndf4_S <- df4_S %>%\n  mutate(Week = factor(Week, \n                       levels = c(\"月\", \"火\", \"水\", \"木\", \"金\", \"土\", \"日\")))\n\ndf4_S_Summary1 <- df4_S %>%\n  group_by(Country, Week) %>%\n  summarise(Sum     = sum(Confirmed,  na.rm = TRUE),\n            Mean    = mean(Confirmed, na.rm = TRUE),\n            SD      = sd(Confirmed,   na.rm = TRUE),\n            .groups = \"drop\")\n\ndf4_S_Summary1\n\n# A tibble: 14 × 5\n   Country Week    Sum  Mean    SD\n   <chr>   <fct> <dbl> <dbl> <dbl>\n 1 Japan   月     2540 106.   156.\n 2 Japan   火     2093  87.2  111.\n 3 Japan   水     2531 105.   133.\n 4 Japan   木     2704 108.   151.\n 5 Japan   金     3083 123.   172.\n 6 Japan   土     3327 133.   189.\n 7 Japan   日     3244 130.   179.\n 8 Korea   月     1609  67.0  126.\n 9 Korea   火     1641  68.4  111.\n10 Korea   水     1626  67.8  102.\n11 Korea   木     1883  78.5  137.\n12 Korea   金     2099  87.5  143.\n13 Korea   土     2194  91.4  174.\n14 Korea   日     2088  87    215.\n\n\ndf4_S_Summary1はこの状態で整然データですが、もし人間が読むための表を作るなら、韓国と日本を別の列に分けた方が良いかも知れません。pivot_wider()を使って、日本と韓国のの新規感染者数を2列に展開します。\n\ndf4_S_Summary1 %>%\n  pivot_wider(names_from  = Country,\n              values_from = Sum:SD)\n\n# A tibble: 7 × 7\n  Week  Sum_Japan Sum_Korea Mean_Japan Mean_Korea SD_Japan SD_Korea\n  <fct>     <dbl>     <dbl>      <dbl>      <dbl>    <dbl>    <dbl>\n1 月         2540      1609      106.        67.0     156.     126.\n2 火         2093      1641       87.2       68.4     111.     111.\n3 水         2531      1626      105.        67.8     133.     102.\n4 木         2704      1883      108.        78.5     151.     137.\n5 金         3083      2099      123.        87.5     172.     143.\n6 土         3327      2194      133.        91.4     189.     174.\n7 日         3244      2088      130.        87       179.     215.\n\n\nこれで人間にとって読みやすい表が出来ました。今は「日本の合計」、「韓国の合計」、「日本の平均」、…の順番ですが、これを日本と韓国それぞれまとめる場合は、relocate()を使います。\n\ndf4_S_Summary1 %>%\n  pivot_wider(names_from  = Country,\n              values_from = Sum:SD) %>%\n  relocate(Week, ends_with(\"Japan\"), ends_with(\"Korea\"))\n\n# A tibble: 7 × 7\n  Week  Sum_Japan Mean_Japan SD_Japan Sum_Korea Mean_Korea SD_Korea\n  <fct>     <dbl>      <dbl>    <dbl>     <dbl>      <dbl>    <dbl>\n1 月         2540      106.      156.      1609       67.0     126.\n2 火         2093       87.2     111.      1641       68.4     111.\n3 水         2531      105.      133.      1626       67.8     102.\n4 木         2704      108.      151.      1883       78.5     137.\n5 金         3083      123.      172.      2099       87.5     143.\n6 土         3327      133.      189.      2194       91.4     174.\n7 日         3244      130.      179.      2088       87       215.\n\n\n新規感染者が確認されるのは金〜日曜日が多いことが分かります。\n曜日ではなく、月ごとに記述統計料を計算する場合は、まずDate列を年、月、日に分割する必要があります。具体的にはDateを\"/\"を基準に別ければいいです。そこで登場するのはseparate()関数であり、使い方は以下の通りです。\n\n# separate()の使い方\nデータ名 %>%\n  separate(cols = 分割する変数名\n           into = 分割後の変数名,\n           sep  = \"分割する基準\")\n\ncolsにはDateを指定し、intoは新しく出来る列名を指定します。今回はDateが3列に分割されるので、長さ3のcharacter型ベクトルを指定します。ここではYear、Month、Dayとしましょう。最後のsep引数は分割する基準となる文字を指定します。df4のDateは\"2020/06/29\"のように年月日が\"/\"で分けられているため、\"/\"を指定します。実際にやってみましょう。\n\ndf4_S <- df4_S %>%\n  separate(col = Date, into = c(\"Year\", \"Month\", \"Day\"), sep = \"/\")\n\ndf4_S\n\n# A tibble: 344 × 7\n      ID Year  Month Day   Week  Country Confirmed\n   <dbl> <chr> <chr> <chr> <fct> <chr>       <dbl>\n 1     1 2020  1     16    木    Japan           1\n 2     1 2020  1     16    木    Korea          NA\n 3     2 2020  1     17    金    Japan           0\n 4     2 2020  1     17    金    Korea          NA\n 5     3 2020  1     18    土    Japan           0\n 6     3 2020  1     18    土    Korea          NA\n 7     4 2020  1     19    日    Japan           0\n 8     4 2020  1     19    日    Korea          NA\n 9     5 2020  1     20    月    Japan           0\n10     5 2020  1     20    月    Korea           1\n# … with 334 more rows\n\n\n新しく出来た変数は元の変数があった場所になります。ここまで来たら月ごとに新規感染者の記述統計量は計算できます。曜日ごとに行ったコードのWeekをMonthに変えるだけです。また、Monthは数字のみで構成されたcharacter型であるため、このままでも問題なくソートされます。したがって、別途factor化の必要もありません（むろん、してもいいですし、むしろ推奨されます）。\n\ndf4_S_Summary2 <- df4_S %>%\n  group_by(Country, Month) %>%\n  summarise(Sum     = sum(Confirmed,  na.rm = TRUE),\n            Mean    = mean(Confirmed, na.rm = TRUE),\n            SD      = sd(Confirmed,   na.rm = TRUE),\n            .groups = \"drop\")\n\ndf4_S_Summary2\n\n# A tibble: 14 × 5\n   Country Month   Sum    Mean     SD\n   <chr>   <chr> <dbl>   <dbl>  <dbl>\n 1 Japan   1        17   1.06    1.34\n 2 Japan   2       213   7.34    7.81\n 3 Japan   3      1723  55.6    42.4 \n 4 Japan   4     12135 404.    146.  \n 5 Japan   5      2763  89.1    73.0 \n 6 Japan   6      1742  58.1    23.0 \n 7 Japan   7       929 186.     45.1 \n 8 Korea   1        11   0.917   1.44\n 9 Korea   2      3139 108.    203.  \n10 Korea   3      6737 217.    222.  \n11 Korea   4       887  29.6    26.6 \n12 Korea   5       729  23.5    16.2 \n13 Korea   6      1348  44.9    10.4 \n14 Korea   7       289  57.8     6.61\n\n\n\ndf4_S_Summary2 %>%\n  pivot_wider(names_from  = Country,\n              values_from = Sum:SD) %>%\n  relocate(Month, ends_with(\"Japan\"), ends_with(\"Korea\"))\n\n# A tibble: 7 × 7\n  Month Sum_Japan Mean_Japan SD_Japan Sum_Korea Mean_Korea SD_Korea\n  <chr>     <dbl>      <dbl>    <dbl>     <dbl>      <dbl>    <dbl>\n1 1            17       1.06     1.34        11      0.917     1.44\n2 2           213       7.34     7.81      3139    108.      203.  \n3 3          1723      55.6     42.4       6737    217.      222.  \n4 4         12135     404.     146.         887     29.6      26.6 \n5 5          2763      89.1     73.0        729     23.5      16.2 \n6 6          1742      58.1     23.0       1348     44.9      10.4 \n7 7           929     186.      45.1        289     57.8       6.61\n\n\n平均値から見ると、日本は7都道府県を対象に緊急事態宣言が行われた4月がピークで緩やかに減少していますが、7月になって上がり気味です。韓国はカルト宗教団体におけるクラスターが発生した3月がピークで、6月からまた上がり気味ですね。傾向としては韓国が日本に1ヶ月先行しているように見えます。\nそれではseparate()関数の他の引数についても簡単に紹介します。まず、sep引数はnumeric型でも可能です。この場合、文字列内の位置を基準に分割されます。年月日が20200629のように保存されている場合は、何らかの基準となる文字がありません。この場合、sep = c(4, 6)にすると、「\"20200629\"の4文字目と5文字目の間で分割、6文字目と7文字目の間で分割」となります。また、sep = c(-4, -2)のように負の値も指定可能であり、この場合は右からの位置順で分割します。\nまた、separate()後は元の変数がなくなりますが、remove = FALSEの場合、元の変数 (ここではDate)が残ります。他にもconvert引数もあります。convert = TRUEの場合、適切なデータ型へ変換してくれます。デフォルト値はFALSEであり、この場合、character型として分割されます。先ほどの例だとYearもMonthもDayも現在はcharacter型です。separate()内でconvert = TRUEを追加すると、分割後のYear、Month、Dayはnumeric型として保存されます。\nseparate()の詳細は?separateまたは、レファレンスページを参照してください。"
  },
  {
    "objectID": "tutorial/R/purrr_intro.html",
    "href": "tutorial/R/purrr_intro.html",
    "title": "purrr入門",
    "section": "",
    "text": "修正履歴\n\n2021/01/17: 公開\n2021/01/18: リンク修正\n誤字・脱字は随時修正しております。\n\n以下の内容は現在執筆中の内容の一部となります。\n\nSong Jaehyun・矢内勇生『私たちのR: ベストプラクティスの探求』(E-book)\n\n「反復処理」章を抜粋したものであり、今後のアップデートは『私たちのRで行います。\nここをお読みになる前に、まず、dplyr入門とggplot2入門 [理論編]、ggplot2入門 [基礎編]、ggplot2入門 [応用編]を一読して下さい。\n\nしたがって、いきなりオブジェクト、関数、引数といった馴染みのない概念が出てきます。これらの概念に馴染みのない方は、予め「Rプログラミング入門の入門」の前半をご一読ください。\n\n本記事の中には横長の表が含まれています。表右上、左上の三角形アイコンをクリックすると中身を確認できます。"
  },
  {
    "objectID": "tutorial/R/purrr_intro.html#iteration-intro",
    "href": "tutorial/R/purrr_intro.html#iteration-intro",
    "title": "purrr入門",
    "section": "*apply()関数群とmap_*()関数群",
    "text": "*apply()関数群とmap_*()関数群\nまず、我らの盟友、{tidyverse}を読み込んでおきましょう。\n\nlibrary(tidyverse)\n\nそれでは、*apply()関数群とmap_*()関数群の動きとその仕組について調べてみましょう。まず、実習用データとして長さ5のnumericベクトルnum_vecを用意します。\n\nnum_vec <- c(3, 2, 5, 4, 7)\n\nこのnum_vecの個々の要素に2を足す場合はどうすれば良いでしょうか。Rはベクトル単位での演算が行われるため、num_vec + 2だけで十分です。+の右側にある2は長さ1のベクトルですが、num_vecの長さに合わせてリサイクルされます（『私たちのR』の「データの構造」）。\n\nnum_vec + 2\n\n[1] 5 4 7 6 9\n\n\n賢明なRユーザーなら上のコードが正解でしょう。しかし、これからの練習のために+を使わずに、for()文を使用してみましょう（『私たちのR』の「Rプログラミングの基礎」）。\n\nfor (i in num_vec) {\n    print(i + 2)\n}\n\n[1] 5\n[1] 4\n[1] 7\n[1] 6\n[1] 9\n\n\n実はこれと同じ役割をする関数がRには内蔵されており、それがlapply()関数です。\nlapply(オブジェクト名, 関数名, 関数の引数)\n以下のコードからも確認出来ますが、足し算を意味する+も関数です。ただし、演算子を関数として使う場合は演算子を`で囲む必要があり、+だと`+`と表記します。\n\n`+`(num_vec, 2)\n\n[1] 5 4 7 6 9\n\n\nしたがって、lapply()関数を使用してnum_vecの全要素に2を足す場合、以下のようなコードとなります。\n\nlapply(num_vec, `+`, 2)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 7\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 9\n\n\nこれと同じ動きをする関数が{purrr}パッケージのmap()です。{purrr}は{tidyverse}を読み込むと自動的に読み込まれます。\n\nmap(num_vec, `+`, 2)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 7\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 9\n\n\nただし、lapply()とmap()の場合、戻り値はリスト型となります。もし、ベクトル型の戻り値が必要な場合は更にunlist()関数を使うか、sapply()を使います。\n\n# unlist()を利用し、リストを解除する\nlapply(num_vec, `+`, 2) %>% unlist()\n\n[1] 5 4 7 6 9\n\n# sapply()を利用すると戻り値はベクトルとなる\nsapply(num_vec, `+`, 2)\n\n[1] 5 4 7 6 9\n\n\nmap()関数ならunlist()でも良いですが、sapply()と同じ動きをするmap_dbl()があります。これは戻り値がdoubleのnumericベクトルになるmap()関数です。\n\n# map_dbl()を利用するとnumeric (double)のベクトルが返される\nmap_dbl(num_vec, `+`, 2)\n\n[1] 5 4 7 6 9\n\n\nもし、2を足すだけでなく、更に3で割るためにはどうすれば良いでしょうか。まず考えられるのは更にsapply()やmap_dblを使うことです。\n\nmap_dbl(num_vec, `+`, 2) %>% \n    map_dbl(`/`, 3)\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\n\nもう一つの方法はsapply()やmap_dbl()の第二引数に直接関数を指定する方法です。\n\nsapply(num_vec, function(x){(x + 2) / 3})\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\nmap_dbl(num_vec, function(x){(x + 2) / 3})\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\n\n上のコードだと、num_vecの要素が第二引数で指定した関数の引数（x）として用いられます。関数が長くなる場合は、sapply()やmap()の外側に関数を予め指定して置くことも可能です。\n\nadd_two_divide_three <- function(x){\n    (x + 2) / 3 \n}\nsapply(num_vec, add_two_divide_three)\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\nmap_dbl(num_vec, add_two_divide_three)\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\n\nここまでの例だとsapply()とmap_dbl()はほぼ同じ関数です。なぜわざわざ{purrr}パッケージを読み込んでまでmap_dbl()関数を使う必要があるでしょうか。それはmap_dbl()の内部にはラムダ（lambda）式、あるいは無名関数（anonymous function）と呼ばれるものが使用可能だからです。ラムダ式は「dplyr入門」でも説明しましたが、もう一回解説します。ラムダ式は使い捨ての関数で、map_dbl()内部での処理が終わるとメモリ上から削除される関数です。使い捨てですので、関数の名前（オブジェクト名）も与えられておりません。\nこのラムダ式の作り方ですが、~で始まり、引数の部分には.xが入ります1。したがって、~(.x + 2) / 3はfunction(x){(x + 2) / 3}の簡略したものとなります。ただし、後者だと無名関数の引数に該当するxをyやjなどに書き換えても問題ありませんが、ラムダ式では必ず.xと表記する必要があります。\n\nmap_dbl(num_vec, ~(.x + 2) / 3)\n\n[1] 1.666667 1.333333 2.333333 2.000000 3.000000\n\n\nかなり短めなコードで「num_vecの全要素に2を足して3で割る」処理ができました。一方、sapply()やlapply()のような*apply()関数群だとラムダ式を使うことはできません。現在のメモリ上にある関数のみしか使うことができません。\n\nsapply(num_vec, ~(.x + 2) / 3)\n\nError in match.fun(FUN): '~(.x + 2)/3' is not a function, character or symbol\n\n\nこれまでの例は正しいコードではありますが、良いコードとは言えないでしょう。なぜならnum_vec + 2という最適解が存在するからです。*apply()とmap_*()はより複雑な処理に特化しています。たとえば、リスト型データの処理です。以下の例を考えてみましょう。\n\nnum_list <- list(List1 = c(1, 3, 5, 7, 9),\n                 List2 = c(2, 4, 6, 8, 10, 13, 3),\n                 List3 = c(3, 2, NA, 5, 8, 9, 1))\n\nnum_listは3つのnumeric型ベクトルで構成されたリスト型オブジェクトです。それぞれのベクトルの平均値を求めてみましょう。これを普通にmean()関数のみで済まそうとすると、リストからベクトルを一つずつ抽出し、3回のmean()関数を使用する必要があります、\n\nmean(num_list[[\"List1\"]], na.rm = TRUE)\n\n[1] 5\n\nmean(num_list[[\"List2\"]], na.rm = TRUE)\n\n[1] 6.571429\n\nmean(num_list[[\"List3\"]], na.rm = TRUE)\n\n[1] 4.666667\n\n\nもしリストの長さが大きくなると、以下のようにfor()文の方が効率的でしょう。\n\nfor (i in names(num_list)) {\n    print(mean(num_list[[i]], na.rm = TRUE))\n}\n\n[1] 5\n[1] 6.571429\n[1] 4.666667\n\n\n計算結果をベクトルとして出力/保存する場合は予めベクトルを用意しておく必要があります。\n\n# num_listの長さと同じ長さの空ベクトルを生成\nReturn_vec <- rep(NA, length(num_list))\n\nfor (i in 1:length(num_list)) {\n    Return_vec[i] <- mean(num_list[[i]], na.rm = TRUE)\n}\n\nReturn_vec\n\n[1] 5.000000 6.571429 4.666667\n\n\n以上の例はsapply()、またはmap_dbl関数を使うとより短くすることができます。\n\nsapply(num_list, mean, na.rm = TRUE)\n\n   List1    List2    List3 \n5.000000 6.571429 4.666667 \n\nmap_dbl(num_list, mean, na.rm = TRUE)\n\n   List1    List2    List3 \n5.000000 6.571429 4.666667 \n\n\n*apply()もmap_*()も、それぞれの要素に対して同じ処理を行うことを得意とする関数です。他の応用としては、各ベクトルからn番目の要素を抽出することもできます。ベクトルからn番目の要素を抽出するにはベクトル名[n]と入力しますが、実はこの[も関数です。関数として使う場合は+と同様、`[`と表記します。num_list内の3つのベクトルから3番目の要素を抽出してみましょう2。\n\nmap_dbl(num_list, `[`, 3)\n\nList1 List2 List3 \n    5     6    NA \n\n\nここまで来たらmap_*()関数群の仕組みについてイメージが出来たかと思います。map_*()関数群の動きは@fig-map-insideのように表すことができます。第一引数はデータであり、そのデータの各要素に対して第二引数で指定された関数を適用します。この関数に必要な（データを除く）引数は第三引数以降に指定します。この関数部（第二引数）はRやパッケージなどで予め提供されている関数でも、内部に直接無名関数を作成することもできます。この無名関数はfunction(x){}のような従来の書き方も可能ですが、map_*()関数群の場合~で始まるラムダ式を使うことも可能です。\n\n\n\n\n\n図 1: map_*()関数群のイメージ\n\n\n\n\nmap()の場合、返り値はリストとなり、map_dbl()の返り値はnumeric (double)型のベクトルとなります。他にもmap_*()関数群にはmap_int()、map_lgl()、map_chr()、map_df()などがあり、それぞれ返り値のデータ型/データ構造を表しています。例えば、返り値がcharacter型のベクトルであれば、map_chr()を使います。c(1, 2, 3, 4, 5)のベクトルの各要素の前に\"ID: \"を付ける例だと以下のように書きます。\n\n# 以下のコードでもOK\n# map_chr(c(1, 2, 3, 4, 5), ~paste0(\"ID: \", .x))\nc(1, 2, 3, 4, 5) %>%\n  map_chr(~paste0(\"ID: \", .x))\n\n[1] \"ID: 1\" \"ID: 2\" \"ID: 3\" \"ID: 4\" \"ID: 5\""
  },
  {
    "objectID": "tutorial/R/purrr_intro.html#iteration-two-arguments",
    "href": "tutorial/R/purrr_intro.html#iteration-two-arguments",
    "title": "purrr入門",
    "section": "引数が2つ以上の場合",
    "text": "引数が2つ以上の場合\n{purrr}を使った本格的な例を紹介する前に、引数が2つ以上の場合を考えたいと思います。まずは引数が2つの場合です。この場合、map2_*()関数群を使用します。例えば、num_vecに長さ5のnumeric型ベクトルnum_vec2をかける例を考えてみましょう。\nこの場合、データとしてnum_vecとnum_vec2が必要となり、ラムダ式にも2つの引数が必要です。まず、num_vec2を用意します。\n\nnum_vec2 <- c(1, 0, 1, 0, 1)\n\n続いてmap2_dbl()関数を使用しnum_vecとnum_vec2の掛け算を行います。map2_*()の使い方はmap_*()とほぼ同様です。\nmap2_dbl(データ1, データ2, 関数 or ラムダ式, 追加の引数)\nmap_*()との違いとしては、(1) データが2つである、(2) 関数、またはラムダ式に2つの引数が必要である点です。この2点目の引数ですが、データ2は.yと表記します。したがって、データ1とデータ2の掛け算を意味するラムダ式は~.x * .yです。\n\nmap2_dbl(num_vec, num_vec2, ~.x * .y)\n\n[1] 3 0 5 0 7\n\n\nnum_vecとnum_vec2が必ずしも同じデータ構造、データ型、同じ長さである必要がありません。数字の前に\"ID:\"を付ける先ほどの例をmap2_chr()で書いてみましょう。\n\nmap2_chr(num_vec, \"ID:\", ~paste0(.y, .x))\n\n[1] \"ID:3\" \"ID:2\" \"ID:5\" \"ID:4\" \"ID:7\"\n\n\nそれでは3つ以上の引数について考えてみましょう。たとえば、num_vecの前に\"ID\"を付けるとします。そして\"ID\"とnum_vecの間に\":\"を入れたい場合はどうすればい良いでしょう。むろん、賢い解決方法は単純にpaste()関数を使うだけです。\n\npaste(\"ID\", num_vec, sep = \":\")\n\n[1] \"ID:3\" \"ID:2\" \"ID:5\" \"ID:4\" \"ID:7\"\n\n\nこの場合、引数は3つです3。この場合の書き方はどうなるでしょうか。map2_*()はデータを2つまでしか指定できません。3つ目以降は関数/ラムダ式の後ろに書くこととなります。ただし、関数/ラムダ式の後ろに指定される引数は長さ1のベクトルでなければなりません。また、ラムダ式内の引数は.xと.yでなく、..1、..2、..3、…となります。\n今回の例だとmap2_chr()内にnum_vecがデータ1、\"ID\"がデータ2です。そして、期待される結果は、「“ID” + sepの実引数 + num_vecの値」となります。したがって、ラムダ式はpaste(..2, ..1, sep = ..3)となります。\n\nmap2_chr(num_vec, \"ID\", ~paste(..2, ..1, sep = ..3), \"-\")\n\n[1] \"ID-3\" \"ID-2\" \"ID-5\" \"ID-4\" \"ID-7\"\n\n\nデータ2である\"ID\"は長さ1のcharacter型ベクトルであるため、以下のようにmap_chr()を使うことも可能です。\n\n# データ2も長さ1なのでmap_chr()もOK\nmap_chr(num_vec, ~paste(..2, ..1, sep = ..3), \"ID\", \"-\")\n\n[1] \"ID-3\" \"ID-2\" \"ID-5\" \"ID-4\" \"ID-7\"\n\n\nそれではデータを3つ以上使うにはどうすれば良いでしょうか。そこで登場するのがpmap_*()関数です。以下の3つのベクトルを利用し、「名前:数学成績」を出力してみましょう。ただし、不正行為がある場合（cheat_vecの値が1）は成績が0点になるようにしましょう。\n\nname_vec  <- c(\"Hadley\", \"Song\", \"Yanai\")\nmath_vec  <- c(70, 55, 80)\ncheat_vec <- c(0, 1, 0)\n\n賢い解決法は普通にpaste0()関数を使う方法です。\n\npaste0(name_vec, \":\", math_vec * (1 - cheat_vec))\n\n[1] \"Hadley:70\" \"Song:0\"    \"Yanai:80\" \n\n\n今回はあえてpmap_*()関数を使ってみましょう。pmap_*()の場合、第一引数であるデータはリスト型で渡す必要があります。したがって、3つのベクトルをlist()関数を用いてリスト化します。第二引数には既存の関数やラムダ式を入力し、各引数は..1、..2、…といった形で表記します。\n\npmap_chr(list(name_vec, math_vec, cheat_vec), \n         ~paste0(..1, \":\", ..2 * (1 - ..3)))\n\n[1] \"Hadley:70\" \"Song:0\"    \"Yanai:80\" \n\n\n第一引数はデータであるため、まずリストを作成し、パイプ演算子（%>%）でpmap_*()に渡すことも可能です。\n\nlist(name_vec, math_vec, cheat_vec) %>%\n  pmap_chr(~paste0(..1, \":\", ..2 * (1 - ..3)))\n\n[1] \"Hadley:70\" \"Song:0\"    \"Yanai:80\""
  },
  {
    "objectID": "tutorial/R/purrr_intro.html#iteration-df",
    "href": "tutorial/R/purrr_intro.html#iteration-df",
    "title": "purrr入門",
    "section": "データフレームと{purrr}",
    "text": "データフレームと{purrr}\nmap_*()関数のデータとしてデータフレームを渡すことも可能です。ここでは5行3列のデータフレームを作成してみましょう。\n\nDummy_df <- data.frame(X = seq(1,  5,  by = 1),\n                       Y = seq(10, 50, by = 10),\n                       Z = seq(2,  10, by = 2))\n\n各行のX、Y、Zの合計を計算するには{dplyr}のrowwise()とmutate()を組み合わせることで計算出来ることを「dplyr入門」で紹介しました。\n\nDummy_df %>%\n  rowwise() %>%\n  mutate(Sum = sum(X, Y, Z)) %>%\n  # rowwise()は1行を1グループとする関数であるため、最後にグループ化を解除\n  ungroup()\n\n# A tibble: 5 × 4\n      X     Y     Z   Sum\n  <dbl> <dbl> <dbl> <dbl>\n1     1    10     2    13\n2     2    20     4    26\n3     3    30     6    39\n4     4    40     8    52\n5     5    50    10    65\n\n\nそれでは各列の平均値を計算するにはどうすれば良いでしょうか。ここではR内蔵関数であるcolMeans()を使わないことにしましょう。\n\ncolMeans(Dummy_df)\n\n X  Y  Z \n 3 30  6 \n\n\nまず、mean(Dummy_df$X)を3回実行する方法や、{dplyr}のsummarise()関数を使う方法があります。\n\nDummy_df %>%\n  summarise(X = mean(X), \n            Y = mean(Y),\n            Z = mean(Z))\n\n  X  Y Z\n1 3 30 6\n\n\n実はこの操作、map_dbl()関数を使えば、より簡単です。\n\nmap_dbl(Dummy_df, mean)\n\n X  Y  Z \n 3 30  6 \n\n\nmap_dbl()はnumeric (double) 型ベクトルを返しますが、データフレーム（具体的にはtibble）に返すならmap_df()を使います。\n\nmap_df(Dummy_df, mean)\n\n# A tibble: 1 × 3\n      X     Y     Z\n  <dbl> <dbl> <dbl>\n1     3    30     6\n\n\nなぜこれが出来るでしょうか。これを理解するためにはデータフレームとtibbleが本質的にはリスト型と同じであることを理解する必要があります。たとえば、以下のようなリストについて考えてみましょう。\n\nDummy_list <- list(X = seq(1,  5,  by = 1),\n                   Y = seq(10, 50, by = 10),\n                   Z = seq(2,  10, by = 2))\n\nDummy_list\n\n$X\n[1] 1 2 3 4 5\n\n$Y\n[1] 10 20 30 40 50\n\n$Z\n[1]  2  4  6  8 10\n\n\nこのDummy_listをas.data.frame()関数を使用して強制的にデータフレームに変換してみましょう。\n\nas.data.frame(Dummy_list)\n\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n\nDummy_dfと同じものが出てきました。逆にDummy_dfを、as.list()を使用してリストに変換するとDummy_listと同じものが返されます。\n\nas.list(Dummy_df)\n\n$X\n[1] 1 2 3 4 5\n\n$Y\n[1] 10 20 30 40 50\n\n$Z\n[1]  2  4  6  8 10\n\n\nここまで理解できれば、map_*()関数のデータがデータフレームの場合、内部ではリストとして扱われることが分かるでしょう。実際、Dummy_listをデータとして入れてもDummy_dfを入れた結果と同じものが得られます。\n\nmap_df(Dummy_list, mean)\n\n# A tibble: 1 × 3\n      X     Y     Z\n  <dbl> <dbl> <dbl>\n1     3    30     6\n\n\n\ntibbleの話\nここまで「データフレーム」と「tibble」を区別せずに説明してきましたが、これからの話しではこの2つを区別する必要があります。tibbleは{tidyverse}のコアパッケージの一つである{tibble}が提供するデータ構造であり、データフレームの上位互換です。tibbleもデータフレーム同様、本質的にはリストですが、リストの構造をより的確に表すことが出来ます。\nデータフレームをリストとして考える場合、リストの各要素は必ずベクトルである必要があります。たとえば、Dummy_listには3つの要素があり、それぞれ長さ5のベクトルです。一方、リストの中にはリストを入れることも出来ます。たとえば、以下のようなDummy_list2について考えてみましょう。\n\nDummy_list2 <- list(ID   = 1:3, \n                    Data = list(Dummy_df, Dummy_df, Dummy_df))\n\nDummy_list2\n\n$ID\n[1] 1 2 3\n\n$Data\n$Data[[1]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n$Data[[2]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n$Data[[3]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n\nDummy_list2には2つの要素があり、最初の要素は長さ3のベクトル、2つ目の要素は長さ3のリストです。2つ目の要素がベクトルでないため、Dummy_list2をデータフレームに変換することはできません。\n\nDummy_df2 <- as.data.frame(Dummy_list2)\n\nError in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 3, 5\n\n\n一方、as_tibble()を使用してtibble型に変換することは可能です。\n\nDummy_tibble <- as_tibble(Dummy_list2)\n\nDummy_tibble\n\n# A tibble: 3 × 2\n     ID Data        \n  <int> <list>      \n1     1 <df [5 × 3]>\n2     2 <df [5 × 3]>\n3     3 <df [5 × 3]>\n\n\n2列目の各セルには5行3列のデータフレーム（df）が格納されていることが分かります。たとえば、Dummy_tibbleのData列を抽出してみましょう。\n\nDummy_tibble$Data\n\n[[1]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n[[2]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n[[3]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n\n長さ3のリスト出力されます。続いて、Dummy_tibble$Dataの2番目のセルを抽出してみましょう。\n\nDummy_tibble$Data[2]\n\n[[1]]\n  X  Y  Z\n1 1 10  2\n2 2 20  4\n3 3 30  6\n4 4 40  8\n5 5 50 10\n\n\nデータフレームが出力されました。簡単にまとめるとtibbleはデータフレームの中にデータフレームを入れることが出来るデータ構造です。むろん、これまでの通り、データフレームのように使うことも可能です4。これがtibbleの強みでもあり、{purrr}との相性も非常に高いです。たとえば、Data列をmap()関数のデータとして渡せば、複数のデータセットに対して同じモデルの推定が出来るようになります。以下ではその例を紹介します。"
  },
  {
    "objectID": "tutorial/R/purrr_intro.html#iteration-model1",
    "href": "tutorial/R/purrr_intro.html#iteration-model1",
    "title": "purrr入門",
    "section": "モデルの反復推定",
    "text": "モデルの反復推定\nここからはmap_*()関数群を使って複数のモデルを素早く推定する方法について解説します。サンプルデータは「ggplot2入門 [基礎編]」で使いました各国の政治経済データ（Countries.csv）を使用します。csv形式データ読み込みの際、これまではR内蔵関数であるread.csv()と{readr}5のread_csv()を区別せずに使用してきました。しかし、ここからはread_csv()を使用します。2つの使い方はほぼ同じですが、read_csv()は各列のデータ型を自動的に指定してくれるだけではなく、tibble構造として読み込みます。read.csv()を使用する場合、as.tibble(データフレーム名)でデータフレームをtibbleに変換してください。\n\nCountry_df <- read_csv(\"Data/Countries.csv\")\n\nRows: 186 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Country, Polity_Type, FH_Status, Continent\ndbl (14): Population, Area, GDP, PPP, GDP_per_capita, PPP_per_capita, G7, G2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nサンプルの分割とモデル推定（split()利用）\nまずは、サンプルを分割し、それぞれの下位サンプルを使った分析を繰り返す方法について解説します。サンプルを分割する方法は2つありますが、最初はR内蔵関数であるsplit()関数を使った方法について紹介します。\n# split()の使い方\n新しいオブジェクト名 <- split(データ名, 分割の基準となるベクトル)\nたとえば、Country_dfを大陸ごとにサンプルを分けるとします。大陸を表すベクトルはCountry_df$Continentです。したがって、以下のように入力します。\n\nSplit_Data <- split(Country_df, Country_df$Continent)\n\nサンプルが分割されたSplit_Dataのデータ構造はリスト型です。\n\nclass(Split_Data)\n\n[1] \"list\"\n\n\n中身を見るとリストの各要素としてtibble（データフレーム）が格納されています。リストの中にはあらゆるデータ型、データ構造を入れることができることが分かります。\n\nSplit_Data\n\n$Africa\n# A tibble: 54 × 18\n   Country   Population   Area    GDP    PPP GDP_per_capita PPP_per_capita    G7\n   <chr>          <dbl>  <dbl>  <dbl>  <dbl>          <dbl>          <dbl> <dbl>\n 1 Algeria     43851044 2.38e6 1.70e5 4.97e5          3876.         11324.     0\n 2 Angola      32866272 1.25e6 9.46e4 2.19e5          2879.          6649.     0\n 3 Benin       12123200 1.13e5 1.44e4 3.72e4          1187.          3067.     0\n 4 Botswana     2351627 5.67e5 1.83e4 4.07e4          7799.         17311.     0\n 5 Burkina …   20903273 2.74e5 1.57e4 3.76e4           753.          1800.     0\n 6 Burundi     11890784 2.57e4 3.01e3 8.72e3           253.           733.     0\n 7 Cabo Ver…     555987 4.03e3 1.98e3 3.84e3          3565.          6913.     0\n 8 Cameroon    26545863 4.73e5 3.88e4 9.31e4          1460.          3506.     0\n 9 Central …    4829767 6.23e5 2.22e3 4.46e3           460.           924.     0\n10 Chad        16425864 1.26e6 1.13e4 2.51e4           689.          1525.     0\n# … with 44 more rows, and 10 more variables: G20 <dbl>, OECD <dbl>,\n#   HDI_2018 <dbl>, Polity_Score <dbl>, Polity_Type <chr>, FH_PR <dbl>,\n#   FH_CL <dbl>, FH_Total <dbl>, FH_Status <chr>, Continent <chr>\n\n$America\n# A tibble: 36 × 18\n   Country   Population   Area    GDP    PPP GDP_per_capita PPP_per_capita    G7\n   <chr>          <dbl>  <dbl>  <dbl>  <dbl>          <dbl>          <dbl> <dbl>\n 1 Antigua …      97929 4.4 e2 1.73e3 2.08e3         17643.         21267.     0\n 2 Argentina   45195774 2.74e6 4.50e5 1.04e6          9949.         22938.     0\n 3 Bahamas       393244 1.00e4 1.28e4 1.40e4         32618.         35662.     0\n 4 Barbados      287375 4.3 e2 5.21e3 4.62e3         18126.         16066.     0\n 5 Belize        397628 2.28e4 1.88e3 2.82e3          4727.          7091.     0\n 6 Bolivia     11673021 1.08e6 4.09e4 1.01e5          3503.          8623.     0\n 7 Brazil     212559417 8.36e6 1.84e6 3.13e6          8655.         14734.     0\n 8 Canada      37742154 9.09e6 1.74e6 1.85e6         46008.         49088.     1\n 9 Chile       19116201 7.44e5 2.82e5 4.64e5         14769.         24262.     0\n10 Colombia    50882891 1.11e6 3.24e5 7.37e5          6364.         14475.     0\n# … with 26 more rows, and 10 more variables: G20 <dbl>, OECD <dbl>,\n#   HDI_2018 <dbl>, Polity_Score <dbl>, Polity_Type <chr>, FH_PR <dbl>,\n#   FH_CL <dbl>, FH_Total <dbl>, FH_Status <chr>, Continent <chr>\n\n$Asia\n# A tibble: 42 × 18\n   Country   Population   Area    GDP    PPP GDP_per_capita PPP_per_capita    G7\n   <chr>          <dbl>  <dbl>  <dbl>  <dbl>          <dbl>          <dbl> <dbl>\n 1 Afghanis…   38928346 6.53e5 1.91e4 8.27e4           491.          2125.     0\n 2 Bahrain      1701575 7.6 e2 3.86e4 7.42e4         22670.         43624.     0\n 3 Banglade…  164689383 1.30e5 3.03e5 7.34e5          1837.          4458.     0\n 4 Bhutan        771608 3.81e4 2.45e3 8.77e3          3171.         11363.     0\n 5 Brunei        437479 5.27e3 1.35e4 2.65e4         30789.         60656.     0\n 6 Burma       54409800 6.53e5 7.61e4 2.68e5          1398.          4932.     0\n 7 Cambodia    16718965 1.77e5 2.71e4 6.93e4          1620.          4142.     0\n 8 China     1447470092 9.39e6 1.48e7 2.20e7         10199.         15177.     0\n 9 India     1380004385 2.97e6 2.88e6 9.06e6          2083.          6564.     0\n10 Indonesia  273523615 1.81e6 1.12e6 3.12e6          4092.         11397.     0\n# … with 32 more rows, and 10 more variables: G20 <dbl>, OECD <dbl>,\n#   HDI_2018 <dbl>, Polity_Score <dbl>, Polity_Type <chr>, FH_PR <dbl>,\n#   FH_CL <dbl>, FH_Total <dbl>, FH_Status <chr>, Continent <chr>\n\n$Europe\n# A tibble: 50 × 18\n   Country  Population   Area    GDP     PPP GDP_per_capita PPP_per_capita    G7\n   <chr>         <dbl>  <dbl>  <dbl>   <dbl>          <dbl>          <dbl> <dbl>\n 1 Albania     2877797  27400 1.53e4  39658.          5309.         13781.     0\n 2 Andorra       77265    470 3.15e3     NA          40821.            NA      0\n 3 Armenia     2963243  28470 1.37e4  38446.          4614.         12974.     0\n 4 Austria     9006398  82409 4.46e5 502771.         49555.         55824.     0\n 5 Azerbai…   10139177  82658 4.80e4 144556.          4739.         14257.     0\n 6 Belarus     9449323 202910 6.31e4 183461.          6676.         19415.     0\n 7 Belgium    11589623  30280 5.30e5 597433.         45697.         51549.     0\n 8 Bosnia …    3280819  51000 2.00e4  49733.          6111.         15159.     0\n 9 Bulgaria    6948445 108560 6.79e4 156693.          9776.         22551.     0\n10 Croatia     4105267  55960 6.04e4 114932.         14717.         27996.     0\n# … with 40 more rows, and 10 more variables: G20 <dbl>, OECD <dbl>,\n#   HDI_2018 <dbl>, Polity_Score <dbl>, Polity_Type <chr>, FH_PR <dbl>,\n#   FH_CL <dbl>, FH_Total <dbl>, FH_Status <chr>, Continent <chr>\n\n$Oceania\n# A tibble: 4 × 18\n  Country    Population   Area    GDP    PPP GDP_per_capita PPP_per_capita    G7\n  <chr>           <dbl>  <dbl>  <dbl>  <dbl>          <dbl>          <dbl> <dbl>\n1 Australia    25499884 7.68e6 1.39e6 1.28e6         54615.         50001.     0\n2 Fiji           896445 1.83e4 5.54e3 1.25e4          6175.         13940.     0\n3 New Zeala…    4842780 2.64e5 2.07e5 2.04e5         42729.         42178.     0\n4 Papua New…    8947024 4.53e5 2.50e4 3.73e4          2791.          4171.     0\n# … with 10 more variables: G20 <dbl>, OECD <dbl>, HDI_2018 <dbl>,\n#   Polity_Score <dbl>, Polity_Type <chr>, FH_PR <dbl>, FH_CL <dbl>,\n#   FH_Total <dbl>, FH_Status <chr>, Continent <chr>\n\n\nそれでは分割された各サンプルに対してPolity_ScoreとFH_Totalの相関係数を計算してみましょう。その前に相関分析の方法について調べてみましょう。Rには相関分析の関数が2つ用意されています。単純に相関係数のみを計算するならcor()、係数の不確実性（標準誤差、信頼区間など）まで計算し、検定を行うならcor.test()を使用します。ここではより汎用性の高いcor.test()の使い方について紹介します。\n# 相関分析: 方法1\ncor.test(~ 変数名1 + 変数名2, data = データ名)\n\n# 相関分析: 方法2\ncor.test(データ名$変数名1, データ名$変数名2)\nそれでは全サンプルに対して相関分析をしてみましょう。\n\nCor_fit1 <- cor.test(~ Polity_Score + FH_Total, data = Country_df)\nCor_fit1\n\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 19.494, df = 156, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7896829 0.8821647\nsample estimates:\n      cor \n0.8420031 \n\n\nここから相関係数（0.8420031）のみを抽出するにはどうすれば良いでしょうか。それを確認するためにはCor_fit1というオブジェクトの構造を調べる必要があります。ここではR内蔵関数であるstr()を使って確認してみましょう。\n\nstr(Cor_fit1)\n\nList of 9\n $ statistic  : Named num 19.5\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named int 156\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 1.16e-43\n $ estimate   : Named num 0.842\n  ..- attr(*, \"names\")= chr \"cor\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"correlation\"\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Pearson's product-moment correlation\"\n $ data.name  : chr \"Polity_Score and FH_Total\"\n $ conf.int   : num [1:2] 0.79 0.882\n  ..- attr(*, \"conf.level\")= num 0.95\n - attr(*, \"class\")= chr \"htest\"\n\n\n相関係数は$estimateで抽出できそうですね。実際にCor_fit1から相関係数のみ抽出してみましょう。\n\nCor_fit1$estimate\n\n      cor \n0.8420031 \n\n\nそれではmap()関数を利用して分割された各サンプルを対象にPolity_ScoreとFH_Totalの相関係数を計算してみましょう。map()のデータはSplit_Dataとし、関数はラムダ式を書いてみましょう。cor.test()内data引数の実引数は.x、または..1となります。最後にmap_dbl(\"estimate\")を利用し、相関係数を抽出、numeric (double) 型ベクトルとして出力します。\n\nCor_fit2 <- Split_Data %>%\n  map(~cor.test(~ Polity_Score + FH_Total, data = .x))\n\n\n# Cor_fit2から相関係数のみ出力\nmap_dbl(Cor_fit2, \"estimate\")\n\n   Africa   America      Asia    Europe   Oceania \n0.7612138 0.8356899 0.8338172 0.8419547 0.9960776 \n\n\nもし、小数点3位までのp値が欲しい場合は以下のように入力します。\n\n# Cor_fit2から相関係数のp値を抽出し、小数点3位に丸める\nmap_dbl(Cor_fit2, \"p.value\") %>% round(3)\n\n Africa America    Asia  Europe Oceania \n  0.000   0.000   0.000   0.000   0.004 \n\n\n\n\nサンプルの分割とモデル推定（nest()利用）\nそれではデータフレーム内にデータフレームが格納可能なtibble構造の長所を活かした方法について解説します。ある変数に応じてデータをグループ化する方法については「dplyr入門」で解説しました。{tidyr}パッケージにはグループごとにデータを分割し、分割されたデータを各セルに埋め込むnest()関数を提供しています。具体的な動きを見るために、とりあえず、Country_dfを大陸（Continent）ごとに分割し、それぞれがデータが一つのセルに埋め込まれた新しいデータセット、Nested_Dataを作ってみましょう。\n\nNested_Data <- Country_df %>% \n  group_by(Continent) %>%\n  nest()\n\n\nNested_Data\n\n\n\n\n\n  \n\n\n\n2列目のdata変数の各セルにtibbleが埋め込まれたことがわかります。Nested_Dataのdata列から5つ目の要素（オセアニアのデータ）を確認してみましょう。\n\n# Nested_Data$data[5]の場合、長さ1のリストが出力される\n# tibble構造で出力する場合は[5]でなく、[[5]]で抽出\nNested_Data$data[[5]]\n\n\n\n\n\n  \n\n\n\nContinentがOceaniaの行のdata列にオセアニアのみのデータが格納されていることが分かります。このようなデータ構造を入れ子型データ（nested data）と呼びます。この入れ子型データはunnest()関数を使って解除することも可能です。unnest()関数を使う際はどの列を解除するかを指定する必要があり、今回はdata列となります。\n\n# 入れ子型データの解除\nNested_Data %>%\n  unnest(data)\n\n\n\n\n\n  \n\n\n\n{dplyr}のgroup_by()関数と{tidyr}のnest()、unnest()関数を組み合わせることでデータを入れ子型データへ変換したり、解除することができます。以上の流れを図式化したものが@fit-nested-dataです。\n\n\n\n\n\n図 2: 入れ子型データの生成過程\n\n\n\n\nそれではこの入れ子型データを使用して大陸ごとのPolity_ScoreとFH_Totalの相関係数を計算してみましょう。data列をデータとした相関係数のラムダ式はどう書けば良いでしょうか。これまでの内容が理解できましたら、答えは難しくないでしょう。cor.test()関数のdata引数の実引数として.xを指定するだけです。この結果をCor_testという列として追加し、Nested_Data2という名のオブジェクトとして保存します。\n\nNested_Data2 <- Nested_Data %>%\n  mutate(Cor_test = map(data, ~cor.test(~ Polity_Score + FH_Total, data = .x)))\n\n\nNested_Data2\n\n\n\n\n\n  \n\n\n\nCor_testという列が追加され、htestというクラス（S3クラス）オブジェクトが格納されていることが分かります。ちゃんと相関分析のオブジェクトが格納されているか、確認してみましょう。\n\nNested_Data2$Cor_test\n\n[[1]]\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 9.0626, df = 36, p-value = 8.05e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7009872 0.9107368\nsample estimates:\n      cor \n0.8338172 \n\n\n[[2]]\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 9.7452, df = 39, p-value = 5.288e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7210854 0.9130896\nsample estimates:\n      cor \n0.8419547 \n\n\n[[3]]\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 7.9611, df = 46, p-value = 3.375e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6087424 0.8594586\nsample estimates:\n      cor \n0.7612138 \n\n\n[[4]]\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 7.6082, df = 25, p-value = 5.797e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6677292 0.9226837\nsample estimates:\n      cor \n0.8356899 \n\n\n[[5]]\n\n    Pearson's product-moment correlation\n\ndata:  Polity_Score and FH_Total\nt = 15.92, df = 2, p-value = 0.003922\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8197821 0.9999220\nsample estimates:\n      cor \n0.9960776 \n\n\n5つの相関分析結果が格納されています。続いて、ここから相関係数のみを抽出してみましょう。相関分析オブジェクトから相関係数を抽出するにはオブジェクト名$estimateだけで十分です。mpa_*()関数を使うなら第2引数として関数やラムダ式を指定せず、\"estimate\"のみ入力するだけです。\n\nNested_Data2 <- Nested_Data2 %>%\n  mutate(Cor_coef = map(Cor_test, \"estimate\"))\n\n\nNested_Data2\n\n\n\n\n\n  \n\n\n\nCor_coef列が追加され、それぞれのセルにnumeric型の値が格納されていることが分かります。map()関数はリスト型でデータを返すため、このように出力されます。このCor_coef列の入れ子構造を解除してみましょう。\n\nNested_Data2 %>%\n  unnest(cols = Cor_coef)\n\n# A tibble: 5 × 4\n# Groups:   Continent [5]\n  Continent data               Cor_test Cor_coef\n  <chr>     <list>             <list>      <dbl>\n1 Asia      <tibble [42 × 17]> <htest>     0.834\n2 Europe    <tibble [50 × 17]> <htest>     0.842\n3 Africa    <tibble [54 × 17]> <htest>     0.761\n4 America   <tibble [36 × 17]> <htest>     0.836\n5 Oceania   <tibble [4 × 17]>  <htest>     0.996\n\n\n\nNested_Data2\n\n\n\n\n\n  \n\n\n\nCor_coefの各列に格納された値が出力されました。以上の作業を一つのコードとしてまとめることも出来ます。また、相関係数の抽出の際にmap()でなく、map_dbl()を使えば、numeric型ベクトルが返されるのでunnest()も不要となります。\n\nCountry_df %>%\n  group_by(Continent) %>%\n  nest() %>%\n  mutate(Cor_test = map(data, ~cor.test(~ Polity_Score + FH_Total, data = .x)),\n         Cor_coef = map_dbl(Cor_test, \"estimate\"))\n\n# A tibble: 5 × 4\n# Groups:   Continent [5]\n  Continent data               Cor_test Cor_coef\n  <chr>     <list>             <list>      <dbl>\n1 Asia      <tibble [42 × 17]> <htest>     0.834\n2 Europe    <tibble [50 × 17]> <htest>     0.842\n3 Africa    <tibble [54 × 17]> <htest>     0.761\n4 America   <tibble [36 × 17]> <htest>     0.836\n5 Oceania   <tibble [4 × 17]>  <htest>     0.996\n\n\nたった5行のコードで大陸ごとの相関分析が出来ました。これをmap_*()を使わずに処理するなら以下のようなコードとなります6。\n\nCor_Test1 <- cor.test(~ Polity_Score + FH_Total, \n                      data = subset(Country_df, Country_df$Continent == \"Asia\"))\nCor_Test2 <- cor.test(~ Polity_Score + FH_Total, \n                      data = subset(Country_df, Country_df$Continent == \"Europe\"))\nCor_Test3 <- cor.test(~ Polity_Score + FH_Total, \n                      data = subset(Country_df, Country_df$Continent == \"Africa\"))\nCor_Test4 <- cor.test(~ Polity_Score + FH_Total, \n                      data = subset(Country_df, Country_df$Continent == \"America\"))\nCor_Test5 <- cor.test(~ Polity_Score + FH_Total, \n                      data = subset(Country_df, Country_df$Continent == \"Oceania\"))\n\nCor_Result <- c(\"Asia\" = Cor_Test1$estimate, \"Europe\" = Cor_Test2$estimate,\n                \"Africa\" = Cor_Test3$estimate, \"America\" = Cor_Test4$estimate,\n                \"Oceania\" = Cor_Test5$estimate)\n\nprint(Cor_Result)\n\n   Asia.cor  Europe.cor  Africa.cor America.cor Oceania.cor \n  0.8338172   0.8419547   0.7612138   0.8356899   0.9960776 \n\n\nそれでは応用例として回帰分析をしてみましょう。今回もNested_Dataを使用し、PPP_per_capitaを応答変数に、FH_TotalとPopulationを説明変数とした重回帰分析を行い、政治的自由度（FH_Total）の係数や標準誤差などを抽出してみましょう。まずは、ステップごとにコードを分けて説明し、最後には一つのコードとしてまとめたものをお見せします。\n\nNested_Data %>%\n  mutate(Model = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)))\n\n\n\n\n\n  \n\n\n\nModel列からから推定結果の要約を抽出するにはどうすれば良いでしょうか。1つ目の方法はsummary(lmオブジェクト名)$coefficientsで抽出する方法です。\n\nlm_fit1 <- lm(PPP_per_capita ~ FH_Total + Population, data = Country_df)\n\nsummary(lm_fit1)$coefficients\n\n\n\n\n\n  \n\n\n\nもっと簡単な方法は{broom}のtidy()関数を使う方法です。tidy()関数は推定値に関する様々な情報をデータフレームとして返す便利な関数です。デフォルトだと95%信頼区間は出力されませんが、conf.int = TRUEを指定すると信頼区間も出力されます。tidy()関数を提供する{broom}パッケージは{tidyverse}の一部ですが、{tidyverse}読み込みの際に一緒に読み込まれるものではないため、予め{broom}パッケージを読み込んでおくか、broom::tidy()で使うことができます。\n\nbroom::tidy(lm_fit1, conf.int = TRUE)\n\n\n\n\n\n  \n\n\n\nそれではModel列にあるlmオブジェクトから推定値の情報を抽出し、Model2という列に格納してみましょう。今回はラムダ式を使う必要がありません。なぜなら、tidy()の第一引数がデータだからです（?tidy.lm参照）。他にも必要な引数（conf.int = TRUEなど）があれば、map()の第3引数以降で指定します。\n\nNested_Data %>%\n  mutate(Model  = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)),\n         Model2 = map(Model, broom::tidy, conf.int = TRUE))\n\n\n\n\n\n  \n\n\n\nModel2列の各セルに7列のtibbleが格納されているようです。ここからはdataとModel列は不要ですのでselect()関数を使って除去し、Model2の入れ子構造を解除してみます。\n\nNested_Data %>%\n  mutate(Model  = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)),\n         Model2 = map(Model, broom::tidy, conf.int = TRUE)) %>%\n  select(-c(data, Model)) %>%\n  unnest(cols = Model2)\n\n\n\n\n\n  \n\n\n\n各大陸ごとの回帰分析の推定結果が一つのデータフレームとして展開されました。ここではFH_Totalの推定値のみがほしいので、filter()関数を使用し、termの値が\"FH_Total\"の行のみを残します。また、term列も必要なくなるので除外しましょう。\n\nNested_Data %>%\n  mutate(Model  = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)),\n         Model2 = map(Model, broom::tidy, conf.int = TRUE)) %>%\n  select(-c(data, Model)) %>%\n  unnest(cols = Model2) %>%\n  filter(term == \"FH_Total\") %>%\n  select(-term)\n\n\n\n\n\n  \n\n\n\nこれで終わりです。政治的自由度と所得水準の関係はアジアとアフリカでは連関の程度が小さく、統計的有意ではありません。オセアニアの場合、連関の程度は大きいと考えられますが、サンプルサイズが小さいため統計的有意な結果は得られませんでした。一方、ヨーロッパとアメリカでは連関の程度も大きく、統計的有意な連関が確認されました。\n以上のコードをよりコンパクトにまとめると以下のようなコードとなります。\n\n# {purrr}使用\nNested_Data %>%\n  mutate(Model = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)),\n         Model = map(Model, broom::tidy, conf.int = TRUE)) %>%\n  unnest(cols = Model) %>%\n  filter(term == \"FH_Total\") %>%\n  select(-c(Data, term))\n\n{purrr}パッケージを使用しない例も紹介します。むろん、以下のコードは可能な限り面倒な書き方をしています。for()文やsplit()と*apply()関数を組み合わせると以下の例よりも簡潔なコードは作成できます。R上級者になるためには{tidyverse}的な書き方だけでなく、ネイティブRの書き方にも慣れる必要があります。ぜひ挑戦してみましょう。\n\n# {purrr}を使用しない場合\n# FH_Totalの係数と標準誤差のみを抽出する例\nlm_fit1 <- lm(PPP_per_capita ~ FH_Total + Population, \n              data = subset(Country_df, Country_df$Continent == \"Asia\"))\nlm_fit2 <- lm(PPP_per_capita ~ FH_Total + Population, \n              data = subset(Country_df, Country_df$Continent == \"Europe\"))\nlm_fit3 <- lm(PPP_per_capita ~ FH_Total + Population, \n              data = subset(Country_df, Country_df$Continent == \"Africa\"))\nlm_fit4 <- lm(PPP_per_capita ~ FH_Total + Population, \n              data = subset(Country_df, Country_df$Continent == \"America\"))\nlm_fit5 <- lm(PPP_per_capita ~ FH_Total + Population, \n              data = subset(Country_df, Country_df$Continent == \"Oceania\"))\n\nlm_df <- data.frame(Continent = c(\"Asia\", \"Europe\", \"Africa\", \"America\", \"Oceania\"),\n                    estimate  = c(summary(lm_fit1)$coefficients[2, 1],\n                                  summary(lm_fit2)$coefficients[2, 1],\n                                  summary(lm_fit3)$coefficients[2, 1],\n                                  summary(lm_fit4)$coefficients[2, 1],\n                                  summary(lm_fit5)$coefficients[2, 1]),\n                    se        = c(summary(lm_fit1)$coefficients[2, 2],\n                                  summary(lm_fit2)$coefficients[2, 2],\n                                  summary(lm_fit3)$coefficients[2, 2],\n                                  summary(lm_fit4)$coefficients[2, 2],\n                                  summary(lm_fit5)$coefficients[2, 2]))\n\n\n\nデータの範囲を指定したモデル推定\nこれまでの例は名目変数でグループ化を行いましたが、連続変数を使うことも可能です。たとえば、人口1千万未満の国、1千万以上5千万未満、5千万以上1億未満、1億以上でサンプルを分割することです。まず、Country_dfからPPP_per_capita、FH_Total、Population列のみを抽出し、Country_df2に格納します。\n\nCountry_df2 <- Country_df %>%\n  select(PPP_per_capita, FH_Total, Population)\n\n続いて、case_when()関数を使用し、Populationの値を基準にケースがどの範囲内に属するかを表す変数Groupを作成します。\n\nCountry_df2 <- Country_df2 %>%\n  mutate(Group = case_when(Population <  10000000  ~ \"1千万未満\",\n                           Population <  50000000  ~ \"1千万以上5千万未満\",\n                           Population <  100000000 ~ \"5千万以上1億未満\",\n                           Population >= 100000000 ~ \"1億以上\"))\n\n中身を確認してみましょう。\n\nCountry_df2\n\n\n\n\n\n  \n\n\n\nあとはこれまでの例と同じ手順となります。まず、データをGroup変数でグループ化し、入れ子構造に変換します。\n\nCountry_df2 <- Country_df2 %>%\n  group_by(Group) %>%\n  nest()\n\n変換後のデータを確認してみます。\n\nCountry_df2\n\n\n\n\n\n  \n\n\n\n続いて、data列をデータとし、map()関数でモデルの推定をしてみましょう。目的変数は所得水準、説明変数は政治的自由度とします。推定後、{broom}のtidy()変数で推定値の情報のみを抽出し、入れ子構造を解除します。最後にtermがFH_Totalの行のみを残します。\n\nCountry_df2 <- Country_df2 %>%\n  mutate(Model = map(data, ~lm(PPP_per_capita ~ FH_Total, data = .x)),\n         Est   = map(Model, broom::tidy, conf.int = TRUE)) %>%\n  unnest(Est) %>%\n  filter(term == \"FH_Total\") %>%\n  select(!term)\n\n\nCountry_df2\n\n\n\n\n\n  \n\n\n\nせっかくなので推定結果を可視化してみましょう。横軸は人口規模（Group）とし、縦軸は推定値とします。係数の点推定値と95%信頼区間を同時に出力するために、geom_pointrange()幾何オブジェクトを使用します。\n\nCountry_df2 %>%\n  mutate(\n    # 横軸の表示順番を指定するために、Group変数をfactor化する\n    Group = factor(Group, levels = c(\"1千万未満\", \"1千万以上5千万未満\",\n                                     \"5千万以上1億未満\", \"1億以上\")),\n    # 統計的有意か否かを表すSig変数の作成\n    Sig   = if_else(conf.low * conf.high > 0, \"統計的有意\", \"統計的非有意\")\n    ) %>%\n  ggplot() + \n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_pointrange(aes(x = Group, y = estimate, \n                      ymin = conf.low, ymax = conf.high, color = Sig), \n                  size = 0.75) +\n  labs(x = \"人口\", y = \"政治的自由度が所得に与える影響\", color = \"\") +\n  scale_color_manual(values = c(\"統計的有意\" = \"black\",\n                                \"統計的非有意\" = \"gray70\")) +\n  theme_minimal(base_family = \"HiraKakuProN-W3\",\n                base_size   = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n政治的自由度が高くなるほど所得水準も高くなる傾向が確認されますが、人口が1億人以上の国においてはそのような傾向が見られないことが分かります。\n上記の例は、ある行は一つのグループに属するケースです。しかし、ある行が複数のグループに属するケースもあるでしょう。たとえば、ある変数の値が一定値以上である行のみでグループ化する場合です。FH_Scoreが一定値以上のSub-sampleに対して回帰分析を行う場合、FH_Scoreが最大値（100）である国はすべてのSub-sampleに属することになります。この場合はgroup_by()でグループ化することが難しいかも知れません。しかし、{dplyr}のfilter()を使えば簡単に処理することができます。\nここでは人口を説明変数に、所得水準を応答変数とした単回帰分析を行います。データは政治的自由度が一定値以上の国家のみに限定します。FH_Scoreが0以上の国（すべてのケース）、10以上の国、20以上の国、…などにサンプルを分割してみましょう。まずはFH_Scoreの最小値のみを格納したRnage_dfを作成します。\n\nRange_df <- tibble(Min_FH = seq(0, 80, by = 10))\n\nRange_df\n\n# A tibble: 9 × 1\n  Min_FH\n   <dbl>\n1      0\n2     10\n3     20\n4     30\n5     40\n6     50\n7     60\n8     70\n9     80\n\n\n9行1列のtibbleが出来ました。続いて、Subsetという列を生成し、ここにデータを入れてみましょう。ある変数の値を基準にサンプルを絞るには{dplyr}のfilter()関数を使用します。たとえば、Counter_dfのFH_Totalが50以上のケースに絞るにはfilter(Country_df, FH_Total >= 50)となります。パイプ演算子を使った場合はCountry_df %>% filter(FH_Total >= 50)になりますが、ラムダ式とパイプ演算子の相性はあまり良くありませんので、ここではパイプ演算子なしとします。重要なのはMin_FHの値をデータとしたmap()関数を実行し、実行内容を「Country_dfからFH_TotalがMin_FH以上のケースのみを抽出せよ」にすることです。\n\nRange_df <- Range_df %>%\n  mutate(Subset = map(Min_FH, ~filter(Country_df, FH_Total >= .x)))\n\n\nRange_df\n\n\n\n\n\n  \n\n\n\n入れ子構造になっているのは確認できましたが、ちゃんとフィルタリングされているかどうかも確認してみましょう。たとえば、Range_df$Subset[[9]]だとFH_Totalが80以上の国に絞られていると考えられます。18列の表ですからCountryとFH_Total列のみを確認してみましょう。\n\nRange_df$Subset[[9]] %>%\n  select(Country, FH_Total)\n\n\n\n\n\n  \n\n\n\n問題なくフィルタリングされているようですね。ここまで出来ればあとはこれまでの内容の復習でしょう。\n\nRange_df <- Range_df %>%\n  mutate(Model  = map(Subset, ~lm(PPP_per_capita ~ Population, data = .x)),\n         Model  = map(Model,  broom::tidy, conf.int = TRUE)) %>%\n  unnest(cols = Model) %>%\n  filter(term == \"Population\") %>%\n  select(-c(Subset, term))\n\n今回も可視化してみましょう。\n\nRange_df %>%\n  ggplot() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_pointrange(aes(x = Min_FH, y = estimate, \n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"データ内FH_Scoreの最小値\", y = \"Populationの係数\") +\n  scale_x_continuous(breaks = seq(0, 80, by = 10),\n                     labels = seq(0, 80, by = 10)) +\n  theme_minimal(base_family = \"HiraKakuProN-W3\")\n\n\n\n\n\n\n\n\n以上の例は実際の分析では多く使われる方法ではないかも知れません。しかし、ノンパラメトリック回帰不連続デザイン（Regression Discontinuity Design; RDD）の場合、感度分析を行う際、バンド幅を色々と調整しながら局所処置効果を推定します。RDDの詳細については矢内の授業資料、およびSONGの授業資料などに譲りますが、ハンド幅の調整はデータの範囲を少しずつ拡張（縮小）させながら同じ分析を繰り返すことです。以下ではアメリカ上院選挙のデータを例に、方法を紹介します。\n使用するデータは{rdrobust}パッケージが提供するrdrobust_RDsenateというデータセットです。{pacman}パッケージのp_load()関数を使ってパッケージのインストールと読み込みを行い、data()関数を使って、データを読み込みます。\n\npacman::p_load(rdrobust)  # {rdrobust}のインストール & 読み込み\ndata(\"rdrobust_RDsenate\") # rdrobust_RDsenateデータの読み込み\n\n# データをSenate_dfという名で格納\nSenate_df <- rdrobust_RDsenate\n\nSenate_dfの中身を確認してみます。\n\nSenate_df\n\n\n\n\n\n  \n\n\n\n本データの詳細は Cattaneo, Frandsen, and Titiunik (2015)7 に譲りますが、ここでは簡単に説明します。marginはある選挙区の民主党候補者の得票率から共和党候補者の投票率を引いたものです。100なら民主党候補者の圧勝、-100なら共和党候補者の圧勝です。これが0に近いと辛勝または惜敗となり、この辺の民主党候補者と共和党候補者は候補者としての資質や資源が近いと考えられます。voteは次回の選挙における民主党候補者の得票率です。ある候補者が現職であることによるアドバンテージを調べる際、「民主党が勝った選挙区における次回選挙での民主党候補者の得票率」から「民主党が負けた選挙区における次回選挙での民主党候補者の得票率」を引くだけでは不十分でしょう。圧勝できた選挙区は次回でも得票率が高いと考えられますが、これは現職というポジションによるアドバンテージ以外にも、候補者がもともと備えている高い能力・資源にも起因するからです。だから辛勝・惜敗の選挙区のみに限定することで、現職と新人の能力・資源などを出来る限り均質にし、「現職」というポジションだけが異なる状況を見つけることになります。\n問題は辛勝と惜敗の基準をどう決めるかですが、広く使われている方法としてはImbens and Kalyanaraman (2012)8の最適バンド幅（optimal bandwidth）が広く使われます。しかし、最適バンド幅を使うとしても感度分析（sensitivity analysis）を行うケースも多く、この場合、バンド幅を少しずつ変えながら現職の効果を推定することになります。推定式は以下のようになります。\\(I(\\text{margin} > 0)\\)はmarginが0より大きい場合、1となる指示関数（indicator function）であす。\n\\[\n\\hat{\\text{vote}} = \\beta_0 + \\beta_1 \\cdot \\text{margin} + \\tau \\cdot I(\\text{margin} > 0) \\quad \\text{where} \\quad -h \\leq \\text{margin} \\leq -h\n\\]\nそれではまずは\\(h\\)が100、つまり全データを使った分析を試しにやってみましょう。\n\nRDD_Fit <- lm(vote ~ margin + I(margin > 0), data = Senate_df)\n\nsummary(RDD_Fit)\n\n\nCall:\nlm(formula = vote ~ margin + I(margin > 0), data = Senate_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.930  -6.402   0.132   7.191  46.443 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       47.33084    0.54192  87.340  < 2e-16 ***\nmargin             0.34806    0.01335  26.078  < 2e-16 ***\nI(margin > 0)TRUE  4.78461    0.92290   5.184 2.51e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.78 on 1294 degrees of freedom\n  (93 observations deleted due to missingness)\nMultiple R-squared:  0.5781,    Adjusted R-squared:  0.5774 \nF-statistic: 886.4 on 2 and 1294 DF,  p-value: < 2.2e-16\n\n\nI(margin > 0)TRUEの係数4.785が現職効果です。この作業を\\(h = 100\\)から\\(h = 10\\)まで、5ずつ変えながら分析を繰り返してみます。まずは、RDD_dfというtibbleを作成し、BW列にはc(100, 95, 90, ... 10)を入れます。続いて、filter()関数を利用してSenate_dfからmarginが-BW以上、BW以下のデータを抽出し、Subsetという名の列として格納します。\n\nRDD_df <- tibble(BW = seq(100, 10, by = -5))\n\nRDD_df <- RDD_df %>%\n  mutate(Subset = map(BW, ~filter(Senate_df, margin >= -.x & margin <= .x)))\n\n\nRDD_df\n\n\n\n\n\n  \n\n\n\n\nRDD_df <- RDD_df %>%\n  mutate(\n    # 各Subsetに対し、回帰分析を実施し、Model列に格納\n    Model  = map(Subset, ~lm(vote ~ margin * I(margin > 0), data = .x)),\n    # Model列の各セルから{broom}のtidy()で推定値のみ抽出\n    Est    = map(Model, broom::tidy, conf.int = TRUE)\n    ) %>%\n  # Est列の入れ子構造を解除\n  unnest(Est) %>% \n  # 現職効果に該当する行のみを抽出\n  filter(term == \"I(margin > 0)TRUE\") %>%\n  # 不要な列を削除\n  select(!c(Subset, Model, term, statistic, p.value))\n\n\nRDD_df\n\n\n\n\n\n  \n\n\n\n19回の回帰分析が数行のコードで簡単に実施でき、必要な情報も素早く抽出することができました。\n以上の例は、交互作用なしの線形回帰分析による局所処置効果の推定例です。しかし、ノンパラメトリックRDDでは、割当変数（running variable）処置変数の交差項や割当変数の二乗項を入れる場合が多いです。今回の割当変数はmarginです。また、閾値（cutpoint）に近いケースには高い重みを付けることが一般的な作法であり、多く使われるのが三角（triangular）カーネルです。上記の例はすべてのケースに同じ重みを付ける矩形（rectagular）カーネルを使った例です。\n以上の理由により、やはりRDDは専用のパッケージを使った方が良いかも知れません。既に読み込んである{rdrobust}も推定可能ですが、ここでは{rdd}パッケージを使ってみましょう。{rdd}パッケージによるRDDはRDestimate()関数を使います。実際の例を確認してみましょう。map()を使う前に、オブジェクトの構造を把握しておくことは重要です。\n# cutpoint引数の既定値は0であるため、今回の例では省略可能\nRDestimate(応答変数 ~ 割当変数, data = データオブジェクト, cutpoint = 閾値, bw = バンド幅)\n\npacman::p_load(rdd) # パッケージの読み込み\n\n# バンド幅を指定したRDD推定\nRDD_Fit <- RDestimate(vote ~ margin, data = Senate_df, bw = 100)\n\n# RDDの推定結果を見る\nsummary(RDD_Fit)\n\n\nCall:\nRDestimate(formula = vote ~ margin, data = Senate_df, bw = 100)\n\nType:\nsharp \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(>|z|) \nLATE       100        1258          5.637     0.8845      6.374    1.842e-10\nHalf-BW     50        1127          6.480     1.0040      6.455    1.084e-10\nDouble-BW  200        1297          5.842     0.8568      6.818    9.210e-12\n              \nLATE       ***\nHalf-BW    ***\nDouble-BW  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F      Num. DoF  Denom. DoF  p\nLATE       316.0  3         1254        0\nHalf-BW    179.7  3         1123        0\nDouble-BW  506.0  3         1293        0\n\n\n現職効果は5.637、その標準誤差は0.884です。これらの情報はどこから抽出できるか確認してみます。\n\n# 推定値の情報がどこに格納されているかを確認\nstr(RDD_Fit)\n\nList of 12\n $ type     : chr \"sharp\"\n $ call     : language RDestimate(formula = vote ~ margin, data = Senate_df, bw = 100)\n $ est      : Named num [1:3] 5.64 6.48 5.84\n  ..- attr(*, \"names\")= chr [1:3] \"LATE\" \"Half-BW\" \"Double-BW\"\n $ bw       : num [1:3] 100 50 200\n $ se       : num [1:3] 0.884 1.004 0.857\n $ z        : num [1:3] 6.37 6.45 6.82\n $ p        : num [1:3] 1.84e-10 1.08e-10 9.21e-12\n $ obs      : num [1:3] 1258 1127 1297\n $ ci       : num [1:3, 1:2] 3.9 4.51 4.16 7.37 8.45 ...\n $ model    : list()\n $ frame    : list()\n $ na.action: int [1:93] 28 29 57 58 86 113 114 142 143 168 ...\n - attr(*, \"class\")= chr \"RD\"\n\n\n$estと$seに私たちが探している数値が入っていますね。それぞれ抽出してみると長さ3のnumericベクトルが出力され、その中で1番目の要素がLATEということが分かります。\n\n# est要素の1番目の要素がLATE\nRDD_Fit$est\n\n     LATE   Half-BW Double-BW \n 5.637474  6.480344  5.842049 \n\n# se要素の1番目の要素がLATEの標準誤差\nRDD_Fit$se\n\n[1] 0.8844541 1.0039734 0.8568150\n\n\nそれでは分析に入ります。今回も\\(h\\)を予めBWという名の列で格納したRDD_dfを作成します。\n\nRDD_df <- tibble(BW = seq(100, 10, by = -5))\n\n今回はラムダ式を使わず、事前に関数を定義しておきましょう。関数の自作については「Rプログラミング入門の入門」を参照してください。\n\n# 引数をxとするRDD_Func()の定義\nRDD_Func <- function(x) {\n  # バンド幅をxにしたRDD推定\n  Temp_Est <- RDestimate(vote ~ margin, data = Senate_df, bw = x)\n  # LATEとその標準誤差をtibbleとして返す\n  tibble(LATE = Temp_Est$est[1],\n         SE   = Temp_Est$se[1])\n}\n\n問題なく動くかを確認してみましょう。\n\nRDD_Func(100)\n\n# A tibble: 1 × 2\n   LATE    SE\n  <dbl> <dbl>\n1  5.64 0.884\n\n\nそれでは分析をやってみましょう。今回の場合、map()の第二引数はラムダ式ではないため~で始める必要ありません。関数名だけで十分です。\n\nRDD_df <- RDD_df %>%\n  mutate(RDD  = map(BW, RDD_Func)) %>%\n  unnest(RDD)\n\n\nRDD_df\n\n\n\n\n\n  \n\n\n\nせっかくなので可視化してみましょう。今回はgeom_pointrange()を使わず、geom_line()とgeom_ribbon()を組み合わせます。geom_line()はLATEを、geom_ribbion()は95%信頼区間を表します。作図の前に95%信頼区間を計算しておきます。\n\nRDD_df %>%\n  mutate(CI_lwr = LATE + qnorm(0.025) * SE,\n         CI_upr = LATE + qnorm(0.975) * SE) %>%\n  ggplot() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_ribbon(aes(x = BW, ymin = CI_lwr, ymax = CI_upr), alpha = 0.5) +\n  geom_line(aes(x = BW, y = LATE), size = 1) +\n  labs(x = \"Bandwidth\", y = \"Local Average Treatment Effect (%p)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n説明・応答変数を指定したモデル推定\n続いて、同じデータに対して推定式のみを変えた反復推定をやってみましょう。たとえば、応答変数は固定し、グループ変数のみを変えながらt検定を繰り返すケースを考えてみましょう。たとえば、OECDに加盟しているかどうか（OECD）で購買力平価GDP（PPP）の差があるかを検定してみましょう。使用する関数はt.test()です。第一引数には応答変数 ~ 説明変数とし、data引数にはデータフレームやtibbleオブジェクトを指定します。\n\nDiff_test <- t.test(PPP ~ G20, data = Country_df)\n\nDiff_test\n\n\n    Welch Two Sample t-test\n\ndata:  PPP by G20\nt = -3.4137, df = 18.012, p-value = 0.003094\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -7667830 -1825482\nsample estimates:\nmean in group 0 mean in group 1 \n         211287         4957943 \n\n\nここからいくつかの情報が読み取れます。まず、OECD加盟国のPPPは4957943、非加盟国は211287です。その差分は-4746656です。また、t値は-3.4136、p値は0.003です。これらの情報を効率よく抽出するにはbroom::tidy()が便利です。\n\nbroom::tidy(Diff_test)\n\n\n\n\n\n  \n\n\n\n差分、t値、p値、95%信頼区間などが抽出できます。これをOECDだけでなく、G7やG20に対しても同じ検定を繰り返してみましょう。\nt.test()の第一引数だけを変えながら推定をすれば良いでしょう。この第一引数、たとえばPPP ~ G20の方はformula型と呼ばれるRにおけるデータ型の一つです。これはcharacter型でないことに注意してください。\n\nFormula1 <- \"PPP ~ G20\"\nt.test(Formula1, data = Country_df)\n\nWarning in mean.default(x): argument is not numeric or logical: returning NA\n\n\nWarning in var(x): NAs introduced by coercion\n\n\nError in t.test.default(Formula1, data = Country_df): not enough 'x' observations\n\n\nこのようにエラーが出ます。このFormulaをas.formula()関数を使ってformula型に変換し、推定をやってみると問題なく推定できることが分かります。\n\nFormula2 <- as.formula(Formula1)\nt.test(Formula2, data = Country_df)\n\n\n    Welch Two Sample t-test\n\ndata:  PPP by G20\nt = -3.4137, df = 18.012, p-value = 0.003094\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -7667830 -1825482\nsample estimates:\nmean in group 0 mean in group 1 \n         211287         4957943 \n\n\nこれから私たちがやるのは\"PPP ~ \"の次に\"G7\"、\"G20\"、\"OECD\"をpaste()やpaste0()関数を結合し、formula型に変換することです。formula型の列さえ生成できれば、あとはmap()関数にformulaを渡し、データはCountry_dfに固定するだけです。\nまずはどの変数を説明変数にするかをGroup列として格納したDiff_dfを作成します。\n\nDiff_df <- tibble(Group = c(\"G7\", \"G20\", \"OECD\"))\n\nDiff_df\n\n# A tibble: 3 × 1\n  Group\n  <chr>\n1 G7   \n2 G20  \n3 OECD \n\n\n続いて、Formula列を作成し、\"PPP ~ \"の次にGroup列の文字列を結合します。\n\nDiff_df <- Diff_df %>%\n  mutate(Formula = paste0(\"PPP ~ \", Group))\n\nDiff_df\n\n# A tibble: 3 × 2\n  Group Formula   \n  <chr> <chr>     \n1 G7    PPP ~ G7  \n2 G20   PPP ~ G20 \n3 OECD  PPP ~ OECD\n\n\n今のFormula列のデータ型を確認してみましょう。\n\nclass(Diff_df$Formula[[1]])\n\n[1] \"character\"\n\n\ncharacter型ですね。続いて、map()関数を使用してFormula列をformula型に変換します。\n\nDiff_df <- Diff_df %>%\n  mutate(Formula = map(Formula, as.formula))\n\nDiff_df\n\n# A tibble: 3 × 2\n  Group Formula  \n  <chr> <list>   \n1 G7    <formula>\n2 G20   <formula>\n3 OECD  <formula>\n\n\nデータ型も確認してみましょう。\n\nclass(Diff_df$Formula[[1]])\n\n[1] \"formula\"\n\n\nformula型になっていることが分かります。それではt検定を行います。ここでもラムダ式を使います。式が入る場所には.xを指定し、データはCountry_dfに固定します。\n\nDiff_df <- Diff_df %>%\n  mutate(Model = map(Formula, ~t.test(.x, data = Country_df)))\n\n\nDiff_df\n\n\n\n\n\n  \n\n\n\nbroom::tidy()で推定値の要約を抽出し、入れ子構造を解除します。\n\nDiff_df <- Diff_df %>%\n  mutate(Tidy = map(Model, broom::tidy)) %>%\n  unnest(Tidy)\n\n\nDiff_df\n\n\n\n\n\n  \n\n\n\nいくつか使用しない情報もあるので、適宜列を削除します。\n\nDiff_df <- Diff_df %>%\n  select(-c(Formula, Model, estimate1, estimate2, \n            p.value, method, alternative))\n\n\nDiff_df\n\n\n\n\n\n  \n\n\n\n以上のコードをパイプ演算子を使用し、簡潔にまとめると以下のようになります。\n\nDiff_df <- tibble(Group = c(\"G7\", \"G20\", \"OECD\"))\n\nDiff_df <- Diff_df %>%\n  mutate(Formula = paste0(\"PPP ~ \", Group),\n         Formula = map(Formula, as.formula),\n         Model   = map(Formula, ~t.test(.x, data = Country_df)),\n         Tidy    = map(Model, broom::tidy)) %>%\n  unnest(Tidy) %>%\n  select(-c(Formula, Model, estimate1, estimate2, \n            p.value, method, alternative))\n\n推定結果を可視化してみましょう。\n\nDiff_df %>%\n  mutate(Group = fct_inorder(Group),\n         Sig   = if_else(conf.low * conf.high > 0, \"統計的有意\",\n                         \"統計的非有意\")) %>%\n  ggplot() +\n  geom_vline(xintercept = 0, linetype = 2) +\n  geom_pointrange(aes(x = estimate, xmin = conf.low, xmax = conf.high,\n                      y = Group, color = Sig), size = 0.75) +\n  scale_color_manual(values = c(\"統計的有意\"   = \"black\",\n                                \"統計的非有意\" = \"gray70\")) +\n  labs(x = \"非加盟国のPPP - 加盟国のPPP\", y = \"グループ\", color = \"\") +\n  theme_bw(base_family = \"HiraKakuProN-W3\",\n           base_size   = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n以上の方法を応用すると応答変数を変えながら分析を繰り返すこともできます。他にも説明変数が2つ以上のケースにも応用可能です。"
  },
  {
    "objectID": "tutorial/JASP/JASP08.html",
    "href": "tutorial/JASP/JASP08.html",
    "title": "JASP入門: 独立性の検定",
    "section": "",
    "text": "今回は新しいデータを使います。JASP_Sample2.csvファイルをダウンロードし、ExcelまたはLibreOfficeなどでで開きます。\nこのファイルの中身は以下の通りです\n\nID\nPref: 出身都道府県 (番号はISO/JIS番号に従ってます。詳細はネットで検索)\nUniv: 在学中の大学 (1 = K大学, 2 = O大学)\nGrade: 学年\nSex: 性別 (1 = 女性, 2 = 男性)\nTako: たこ焼き器を所有しているか (0 = いいえ; 1 = はい)\nTakoN: たこ焼きを何個持っているか\nBlood: 血液型 (1 = A; 2 = B, 3 = O, 4 = AB, 5 = その他、不明)\nPhone: 使用スマホ (1 = Android, 2 = iPhone)\n\nクロス表を作成するためには名目変数か順序変数を用いますが、都道府県は名目変数とはいえ、47種類もあるので、今回は「大阪出身か否か」の変数を作成します。\n\n\n\nデータの右端に新しい変数名（Osaka）を作成します。\n\n\n\n下のセルに以下のように入力します。\n\n=IF(B2 = 27, 1, 0)\n\nこれは、「もしB2セルの値が27なら、1を、それ以外の場合は0を表示する」という意味です。ちなみに27は大阪府の番号ですね。\n\n\n\n同じ数式を何回も書くのは面倒くさいですね。先ほど入力したセル (J2)の右下に小さい「■」がありますね。これをクリックしたままデータの最下段までドラッグします。\n\n\n\n同じ数式の中で「B2」と書かれている部分だけが修正され、自動的にコピペされます。このまま保存し、JASPで読み込みます。"
  },
  {
    "objectID": "tutorial/JASP/JASP08.html#jaspによる分析",
    "href": "tutorial/JASP/JASP08.html#jaspによる分析",
    "title": "JASP入門: 独立性の検定",
    "section": "JASPによる分析",
    "text": "JASPによる分析\n\nJASPでファイルを読み込みます。\n\n\n\nまずは値ラベルを付けます。ラベルを付ける変数名「Tako」をクリックしたら、上段にラベル作成の画面が出てきますね。ここで「0 = Not Having」、「1 = Having」と入力し、「X」をクリックします。\n同じく「Osaka」変数も「0 = From non-Osaka」、「1 = From Osaka」のラベルを付けます。\n\n\n\n分析メニュの「Frequencies」→「Contingency Tables」をクリックします。\n\n\n\n行と列に入る変数を選択します。ここでは「Tako」は「Columns (列)」へ、「Osaka」は「Rows (行)」へ入れます。\n\n行と列を逆に入れても問題ありません。自分が見やすい表になるように、色々試してみましょう。\n\n\n\n\nつづいて、下段の「Statistics」と「Cells」をクリックします。\n\n\n\nまずは「Statistics」からです。ここでは「\\(\\chi^2\\)(カイ二乗統計量)」と「\\(\\chi^2\\) continuity correction (連続補正)」にチェックをします。通常、2 x 2のクロス表の場合は、連続補正された統計量で解釈した方が望ましいと言われています。\n\nちなみに、JASP 0.8.1ではFisherの直接法 (exact test)の結果は表示されません。\n\n\n\n\nつづいて、「Cell」画面です。\nここではCountsの「Observed」と「Expected」を選択します。それぞれ、「観測値」と「期待値」を意味します。\nPercentagesでは「Row」のみチェックをしました。もし、行単位だけでなく、列単位の割合も見たい場合は「Column」、全体における割合なら「Total」にチェックします。解釈しやすくなるよう、自分で試してみてください。\n\n\n\nこれがクロス表です。全体的に大阪出身の方がたこ焼き器を持っている傾向が見えますね。この「出身地とたこ焼き器の有無」の関係は統計的に有意な関係でしょうか。\n\n\n\n下に表示される表がカイ二乗検定の結果です。今回は2 x 2のクロス表なので連続補正 (\\(\\chi^2\\) continuity correction)のp値をみたら0.063でした。一般的に使われる有意水準 (= 0.05)より大きいので\n\n「出身地とたこ焼き器の有無は連関しているとは言えない」という結果になります。\nむろん、単純なχ2統計量を基準にした場合は、統計的に有意な連関が見られますね。どっちを見るかは分析者の責任です。"
  },
  {
    "objectID": "tutorial/JASP/JASP09.html",
    "href": "tutorial/JASP/JASP09.html",
    "title": "JASP入門: 相関分析",
    "section": "",
    "text": "分析メニューの「Regression」から「Correlation Matrix」を選択します。\n\n\n\n相関係数を算出する変数を投入します。相関係数は連続変数と順序変数のみ計算可能です。今回は財政力指数 (Zaisei)と65歳以上人口比 (Over65)、自民党の得票率 (LDP)を選択し、右の変数リストへ移動させます。\n\n\n\nつづいて、下の「Correlation Coefficients」を見てみましょう。基本的には「Pearson」にチェックされていますが、今回は全ての変数が連続変数なのでこのままにしておきます。\n\n順序変数同士の場合は「Spearman (スペアマンの順位相関係数)」が「Kendall’s tau-b (ケンダルの順位相関係数)」を選択します。\n順序変数と連続変数同士なら好みによりますが、筆者なら順位相関係数を使います。全てにチェックを入れてみて、結果を比較してみるのもいいでしょう。\n\nづぎは、「Plot」です。基本的にはチェックが入っていませんが、今回は全てチェックします。\n\n\n\nまず、相関係数からです。読み方としては列と行がクロスするところです。対角線上は同じ変数同士なので常に相関係数は1であり、省略されています。また対角線の下は上と対称なので、ここも省略ですね。\nたとえば、「Zaisei」と「LDP」の相関係数は-0.457であり、p値は約0.001です。つまり、財政力指数が高い都道府県ほど、自民党の得票率が低いことを意味し、この相関関係は統計的に有意な関係と言えます。\n同じく、「Over65」と「LDP」の相関係数は0.546、p値は0.001未満ですね。ここもまた、高齢者が多い都道府県ほど、自民党の得票率は高いことを意味し、統計的に有意な関係が確認できます。\n\n\n\nつづいてプロットですね。まず、対角線上は各変数のヒストグラムです。これは先ほど「Plot」で「Densities for variables」をチェックすることで表示されます。\n対角線上の上は散布図です。「Zaisei」と「Over65」の散布図を見ると負の関係が見られますね。\n対角線上の下は相関係数です。「Zaisei」と「LDP」の相関係数は-0.457ですね。先ほどの表の結果とも一致します。"
  },
  {
    "objectID": "tutorial/JASP/index.html",
    "href": "tutorial/JASP/index.html",
    "title": "JASP入門講座",
    "section": "",
    "text": "このページが私が担当しているデータ分析系授業のために作成されたものです。したがって、ここで用いる例は全て私の担当科目の中で配布・作成したデータを使ったものです。\nこのページは以下の環境で作成されました。\n\nMacbook Pro 13″ (2016 Late)\nmacOS Sierra 10.12.4\nmacOSの言語設定は英語\nJASPのバージョンは 0.8.8.1\n\nWindowsも操作法に関しましては大きな違いはないかと思います。\n内容についてご不明な点、ご指摘がありましたら宋までご連絡頂けましたら幸いです。"
  },
  {
    "objectID": "tutorial/JASP/index.html#目次",
    "href": "tutorial/JASP/index.html#目次",
    "title": "JASP入門講座",
    "section": "目次",
    "text": "目次\n\nJASPの入手・インストール\nデータの読み込み\nデータの編集・ラベル作成\n記述統計\n可視化\n差の検定 (1)：t検定\n差の検定 (2)：一元配置分散分析 (ANOVA)\n独立性の検定\n相関分析\n線形回帰分析"
  },
  {
    "objectID": "tutorial/JASP/JASP02.html",
    "href": "tutorial/JASP/JASP02.html",
    "title": "JASP入門: データの読み込み",
    "section": "",
    "text": "これがJASPの画面です。SPSSなどに比べればかなりシンプルですね。出来る分析もSPSSに比べて貧弱ですが、入門のデータ分析講義で扱う分析手法はほとんど可能です。\nまず、先ほどダウンロードしたファイルを開きましょう。画面左上の「File」をクリックします。\n\n\n\n既に「Open」は選択されていると思いますが、続いて「Computer」をクリックし、最後に「Browse」をクリック。\nファイルが選択できるダイアログボックスが表示されますね。そこで先ほどダウンロードしたJASP_Sample.csvを選択し、開きます。\n\n\n\nJASPは大きく分けて3つの領域で構成されています。\n\n分析メニュー：記述統計、t検定、ANOVA、…など様々な分析手法を選択するメニューです。\nデータ：現在、開いているデータが表示されます。現在のJASP (Ver. 0.8.8.1)では、データ領域で中身を修正することはできません。中身をクリックするとExcelやLibreOfficeなどのスプレッドシートのソフトが起動されます。\n分析結果：今は何も表示されていませんが、これから何らかの分析をすると分析結果が表示されます。\n\n※注意※: JASPは日本語が含まれているデータセットの読み込みが可能です。しかし、分析結果には日本語が表示されません。したがって、データセットは事前に日本語を英語に書き換えておく必要があります。\n結果の保存などについては今度の講義で説明します。まずは、データを開くことができたらOKです。"
  },
  {
    "objectID": "tutorial/JASP/JASP03.html",
    "href": "tutorial/JASP/JASP03.html",
    "title": "JASP入門: データの編集・ラベル作成",
    "section": "",
    "text": "変数名の左には可愛いアイコンが付いていますね。これはこの変数の尺度を表します。このアイコンをクリックしてみましょう。\n\n\n\n3種類のアイコンがあります。各アイコンの意味は\n\n上のアイコン (定規)：連続変数 (間隔・比率変数)\n中間のアイコン (棒グラフ)：順序変数\n下のアイコン (3つの円)：名目変数\n\nIDは定規のアイコンなので、デフォルト上では連続変数扱いされていることが分かります。しかし、IDは名目変数ですね。なので、一番下にあるアイコンをクリックし、名目変数として指定してあげましょう。\n\n尺度については私が担当する授業で説明しました。\nIDの場合、数値自体には意味がありませんので、名目変数です。別に北海道が2で、青森が1でも問題ないからです。あくまでも便宜上割り当てたものです。\n\n他に財政力指数 (Zaisei)や高齢者・若者人口比 (Over65, Under30)、各政党の得票率 (LDPからSDPまで)は連続変数ですし、ちゃんと連続変数として指定されているのでこのままでいいでしょう。\n\n\n\n一番右の変数 (Region2とRegion3)は「地域区分」(東日本、西日本など)です。地域区分もまた名目変数です。つまり、「東日本 = 1 / 西日本 =2」でも、「東日本 = 9 / 西日本 = 5」でも問題ないということです。これはちゃんと名目変数になっていますね。\nしかし、1と2といった数値で表示されているとちょっと意味が分かりづらくなりますね。このような名目変数・順序変数にはラベルを付けることができます。\n今回は変数名のアイコンではなく、変数名そのものをクリックします。\n\n\n\nこのようなラベル指定画面がデータ領域に表示されます。Labelの「1」を「East_Japan」に、「2」を「West_Japan」に書き換えましょう。\n\n\n\n先までデータ領域で「1」と表示された値が「East_Japan」になりました。ここではお見せしませんが、下の方に行けば、「2」は「West_Japan」になっていることが分かります。\nここまでしたら、ラベル指定画面右上の「X」ボタンをクリックします。\n「Region3」変数もやってみましょう。各値の意味は以下のとおりです。\n\n１：Hokkaido_Tohoku\n２：Kanto\n３：Chubu\n４：Kinki\n５：Chugoku_Shikoku\n６：Kyushu_Okinawa\n\n「Region3」列も数値が文字列に変換されたことが確認できます。"
  },
  {
    "objectID": "tutorial/JASP/JASP01.html",
    "href": "tutorial/JASP/JASP01.html",
    "title": "JASP入門: JASPの入手・インストール",
    "section": "",
    "text": "DOWNLOADをクリックします。\n\n\n\n自分のパソコン環境に合わせてインストーラをダウンロードします。\n\nWindowsの場合：「Windows XP and above」\nmacOSの場合：「Sierra (10.12) and above」あるいは「El Capitan (10.11) and below」\n\n※macOSの場合はXQuartzを事前にインストールしておいてください。\n\nLinuxの場合：下段のInstallation Guideを参照してください。\n\nダウンロードしたファイルを開き、指示に従ってインストールしてください。"
  },
  {
    "objectID": "tutorial/JASP/JASP04.html",
    "href": "tutorial/JASP/JASP04.html",
    "title": "JASP入門: 記述統計",
    "section": "",
    "text": "ここでは記述統計の出し方を実習します。ここでは以下の項目について実習します。\n\n名目変数・順序変数\n\n度数分布表\n\n連続変数\n\n平均値、中央値、分散、標準偏差、四分位数"
  },
  {
    "objectID": "tutorial/JASP/JASP04.html#名目変数の記述統計",
    "href": "tutorial/JASP/JASP04.html#名目変数の記述統計",
    "title": "JASP入門: 記述統計",
    "section": "名目変数の記述統計",
    "text": "名目変数の記述統計\n\n名目変数の場合、主に「度数分布表」と「最頻値」を使います。これは、各値が何個あるかを表で表したものです。\n\n名目変数の場合、ほとんどの場合において平均値、中央値、分散に意味がありません。\n\n今回の実習データに名目変数は「ID」、「Region2」、「Region3」があります。ここでは「Region3」の度数分布表を作成しましょう。\n\n\n\n変数選択領域の左側にはデータ内の変数名が並んでいます。ここから「Region3」を選択し、「▶」をクリックして右側のボックスに移動します。\nこの時点で右側の分析結果画面に何か表が出てきますが、無視して先に進みましょう。\n\n\n\n「Display frequency tables (nominal and ordinal variables)」にチェックを入れます。\n\n「度数分布表の表示 (名目・順序変数)」という意味です。つまり、連続変数を入れてからチェックをしても何も変わりません。\n\nここまでしたら度数分布表が表示されます。\n\n\n\n分析結果画面の見方について説明します。\n\nDescriptive Statics (記述統計)\n\nValid（有効ケースの数）：欠損値を除外した値の数です。今回は欠損値がないので47個です。\nMissing（欠損値の数）：欠損値、つまり空白になっているセルの数です。今回は欠損値がないので0です。\n以下の部分：平均値、標準偏差、最小値、最大値です。しかし、名目変数の場合、これらの統計量に意味がないので無視します。しかし、順序変数の場合は、ここも見ます。\n\nFrequencies (度数 / 頻度)\n\n1列目: 各項目のラベルが表示されます。もし、ラベルを付けなかった場合は、元の数字が出ます。\nFrequency（2列目）：各項目がデータ内に何個あるかです。たとえば、データ内にKanto (関東)は7個、Chubu (中部)は9個含まれていることを意味します。\n\nここから最頻値 (mode)が算出できます。ここではChubu (中部)とChugoku_Shikoku (中国・四国)の度数が9個であり、最も多い値ですね。したがって、最頻値はChubuとChugoku_Shikokuです。\n\nPercent（3列目）：各項目がデータの何割を占めているかです。たとえば、Hokkaido_Tohoku (北海道・東北)は全体の14.9%を占めています。\nValid Percent（4列目）：上のPercentと似ていますが、この場合は、ケースの数ではなく、上の表にあるValid (有効ケース数)で割った値です。今回は欠損値がないので「ケース数 = 有効ケース数」なのでPercentと一致します。\nCumulative Percent（5列目）：累積割合です。北海道・東北の累積割合は14.9%、関東の累積割合は北海道・東北の累積割合 + 関東の割合 = 29.8のような感じです。最終的には100.0%になりますね。\n\n\nこれで名目変数の記述統計の出し方は終わりです。順序変数の度数分布表も同様に出します。"
  },
  {
    "objectID": "tutorial/JASP/JASP04.html#連続変数の記述統計",
    "href": "tutorial/JASP/JASP04.html#連続変数の記述統計",
    "title": "JASP入門: 記述統計",
    "section": "連続変数の記述統計",
    "text": "連続変数の記述統計\n\nこれからは連続変数の記述統計を出してみましょう。ここでの連続変数は各政党の得票率です。先ほどと同様、「記述統計」画面を出します。\n\n\n\n各政党の得票率は「LDP」から「SDP」変数です。これを同時に選択し (LDPを選択→Shiftキーを押したままSDP選択)、「▶」をクリックしてから、下段の「Statistics」(統計量)を選択します。\n\n\n\nどのような統計量を表示させるかが選択できます。ここでは私の担当講義で取り上げた統計量について説明します。\n\n分位数\n\n「Quartiles」：四分位数\n\n代表値\n\n「Mean」：平均値\n「Median」：中央値\n「Mode」：最頻値\n\n散らばり具合\n\n「Std. deviation」：標準偏差\n「Variance」：分散\n「Minimum」：最小値\n「Maximum」：最大値\n\n歪度・尖度：ここでは説明しません。\n\nこの中から、「Quartiles」、「Mean」、「Median」、「Std. deviation」、「Variance」、「Minimum」、「Maximum」にチェックを入れます。\n\n\n\n分析結果画面に記述統計の表が表示されます。\n代表値と散らばり具合は先ほどの図と同様です。ここでは分位数についてのみ説明します。\n\n「25th percentile」: 第1四分位点 (Q1/4)\n「50th percentile」: 第2四分位点 (Q2/4) =中央値\n「75th percentile」: 第3四分位点 (Q3/4)"
  },
  {
    "objectID": "tutorial/JASP/JASP10.html",
    "href": "tutorial/JASP/JASP10.html",
    "title": "JASP入門: 線形回帰分析",
    "section": "",
    "text": "\\[\\text{自民党得票率} = \\beta_0+ \\beta_1 \\cdot \\text{65歳以上人口比} + \\beta_2 \\cdot \\text{財政力指数} + \\varepsilon\\]\n\n回帰分析から推定される値は切片 (\\(\\beta_0\\))と傾き (\\(\\beta_{1, 2}\\))、誤差項 (\\(\\varepsilon\\))の標準偏差です。\n\n私の講義において誤差項 (\\(\\varepsilon\\))については説明しませんでした。簡単に説明すると、切片・傾きが推定されれば自民党得票率の予測値が計算できます。この予測値を実際の観測値から引いたものを「残差 (residuals)」といい、これは「誤差 (errors)」の推定値となります。この残差の標準偏差が誤差項の標準偏差の推定値となります。\n回帰分析が最良線形不偏推定量 (Best Linear Unbiased Estimator; BLUE)であるためには、いくつかの条件がありますが、中には「誤差項が正規分布に従うこと」、「誤差項の分散が説明変数と独立していること」、「誤差項間の相関 (自己相関)がないこと」など、誤差項に関するものが多いです。したがって、実際のデータ分析では残差のグラフなどを見て回帰分析の診断 (diagnostic)を行います。\n\n\n\n\n分析メニューの「Regression」から「Linear Regression」を選択します。\n\n\n\n応答変数と説明変数を指定します。自民党得票率 (LDP)変数を選択し、「Dependent」へ投入します。「Dependent variable」は従属変数とも呼ばれ、私の授業では「応答変数」と説明しました。\n続いて財政力指数 (Zaisei)と65歳以上人口比 (Over65)を「Covariates」へ投入します。「Covariates」は「共変量」と訳されますが、この授業では「説明変数」と説明しました。\n\n\n\n実はこれだけでも線形回帰分析は出来ますが、可能なら記述統計も一緒に見る習慣を付けましょう。下にある「Statistics」をクリックします。\n\n\n\n「Descriptive」にチェックを入れます。これで記述統計が表示されます。\n\n\n\n右側の結果画面に表が4つ出てきますが、ここでは「Model Summary」と「Coefficients」の表のみを紹介します。まず、「Model Summary」で見るところは「Adjusted \\(R^2\\)」です。これが調整済み決定係数です。この値が高いほど良いモデルと言われています。\n\n\n\nつづいて、「Coefficients」の表です。まず、第2列目には変数名が書いてあります。「Intercept」は切片を意味します。\n「Unstandardized」列が係数です。つまり、最初にお見せしたモデルだと以下のような数式に表現できます。\n\n\\[自民得票率の予測値 = 16.479 + 0.864 \\times 65歳以上人口比 – 4.482 \\times 財政力指数 \\]\n\n「Standard Error」は標準誤差です。レポートや論文などに掲載する際には係数と一緒に出す場合が多いです。\n「Standardized」は標準化係数です。係数同士の大きさを比較する際に用いられます。\n「t」は係数を標準誤差で割った値です。\n「p」は有意確率 (p値)です。この結果だと「65歳以上人口比」の係数が95%水準で有意ですね。\n65歳以上人口比が1%ポイント上がると、自民党の得票率は0.864%ポイント上がりますね。高齢者が0%で、財政力指数が0の都道府県なら自民得票率の予測値は16.479%になります。\n財政力指数や高齢者比は「中心化」あるいは「標準化」した方が良いですが、現在、JASP内でデータの修正はできません。中心化や単位変換などをする場合は他の表計算ソフトや統計ソフトでデータを修正することになります。"
  },
  {
    "objectID": "tutorial/JASP/JASP05.html",
    "href": "tutorial/JASP/JASP05.html",
    "title": "JASP入門: 可視化",
    "section": "",
    "text": "まずは棒グラフを作ってみましょう。棒グラフの横軸は各項目を意味し、縦軸は各項目の数 (度数)、あるいはデータ内で占める割合を意味します。棒グラフは名目変数あるいは順序変数を用います。とりわけ、名目変数なら選択の余地がなく、棒グラフくらいしか使えません。\n今回は地域区分 (6区分)の棒グラフを作ってみましょう。\n\n\n\n「Descriptive Statistics」画面で今回は「Region3」変数を選択し、「▶」をクリックします。続いて、グラフを表示させるために「Plots」をクリックします。\n\n\n\n「Display distribution plots」にチェックします。もし、変数が名目変数・順序変数なら自動的に棒グラフを、連続変数ならヒストグラムを表示してくれます。「Region3」変数は名目変数なので棒グラフですね。\n\n\n\n右の領域に棒グラフを表示されましたが、横軸のラベルが省略されている箇所がありますね。これはグラフのサイズが小さいため、文字が入り切れないためです。グラフの右下にマウスのポインターを持っていけば、赤字四角のようにポインターの形が変わります。\n\n\n\nこれで棒グラフは完成ですね。"
  },
  {
    "objectID": "tutorial/JASP/JASP05.html#ヒストグラム",
    "href": "tutorial/JASP/JASP05.html#ヒストグラム",
    "title": "JASP入門: 可視化",
    "section": "ヒストグラム",
    "text": "ヒストグラム\n\nヒストグラムの作成は棒グラフと全く同じです。変数を連続変数にするだけです。以下の図は平成27年度参議院議員通常選挙における維新の得票率（比例区）のヒストグラムです。各自やってみましょう。変数名は「Ishin」です。\n\n\n\nJASPもまだ開発初期段階なので作成されたグラフを微調整する機能がありませんね（軸ラベルの変更、棒の幅の調整など）。今後の発展が楽しみです。"
  },
  {
    "objectID": "tutorial/JASP/JASP05.html#箱ひげ図",
    "href": "tutorial/JASP/JASP05.html#箱ひげ図",
    "title": "JASP入門: 可視化",
    "section": "箱ひげ図",
    "text": "箱ひげ図\n\nまずは変数一つ全体の箱ひげ図を作ってみます。前回と同様、「Descriptive Statistics」画面を表示させます。\n\n\n\n今回も維新の得票率を使います。変数リストから「Ishin」を選択し、「▶」をクリックします。続いて、図を表示させるために「Plots」をクリックします。\n\n\n\n「Display boxplots」、「Label Outliers (任意)」、「Boxplot Element」にチェックします。これで完成です。\n\n\n\n点に横にある数値は「外れ値の行番号」です。JASPは現在、外れ値のラベルを指定することはできません。各値がどの都道府県を指しているのかは実際のデータを見る必要があります。たとえば、27行目は大阪府です。\nつづいて、維新の得票率を各地域ごとに分けて箱ひげ図を出してみましょう。\n先ほどの箱ひげ図の練習とほぼ同一です。\n\n\n\n異なる箇所は分割の基準となる変数、ここでは「Region3」を「Split by (optional)」の方に入れることです。\n\n\n\nもうちょっと見栄をよくするために、「Color」にチェックをしましょう。\n\n\n\nこれで完成です。維新の得票率の箱ひげ図を地域ごとに見ることができますね。"
  },
  {
    "objectID": "tutorial/JASP/JASP07.html",
    "href": "tutorial/JASP/JASP07.html",
    "title": "JASP入門: 一元配置分散分析 (ANOVA)",
    "section": "",
    "text": "分析メニューの「ANOVA」から「ANOVA」を選択します。\n\n\n\nまず、「どの変数の平均値の差をみるか」ですね。これを従属変数になります。今回は財政力指数の差を検証します。したがって、「Zaisei」変数を選択し、「Dependent Variable」の「▶」をクリックします。\nつづいて、グループを指定します。グループは「Fixed Factor」で指定します。今回は6地域間における財政力指数の平均値の差を検証するので「Region6」変数を選択し、「Fixed Factor」の「▶」をクリックします。\n\n\n\nここまでだと、右の結果画面に分散分析の結果が表示されます。p値が有意水準 (一般的に0.05)より小さいので、「6地域間には財政力指数の平均値に差がある」と解釈します。\nしかし、これはどこかの組み合わせに差がある可能性を示しただけであり、必ずしも\n\n「北海道・東北≠関東≠中部≠近畿≠中国・四国≠九州・沖縄」\n\nを意味するものではありません。具体的にどの組み合わせ (対)に財政力指数の差があるかを確認してみましょう（これを多重検定・多重比較といいます）。\n\n\n\n左の分析画面にもどり、「Post Hoc Tests」をクリックします。\n\n\n\nまず、「Region6」変数を選択し、「▶」をクリックします。\nつづいて、「Correction」ですが、今回はデフォルトの「Tukey」で結構です。多重比較は基本的に全ての2群の組み合わせでt検定を行うものですが、検定後はp値を補正します。この「Correction」はp値の補正方法です。したがって、他の補正方法を選択することによって結果が若干変わる可能性があります。興味のある人は色々とやってみましょう。\n\n\n\nせっかくなので、グループごとの財政力指数の記述統計も出してみましょう。「Additional Options」をクリックします。\n\n\n\n「Display」の「Descriptive statistics」にチェックします。\n\n\n\nこの表が多重比較の結果です。基本的な見方はt検定と同じです。\nたとえば、一行目は「北海道・東北」と「関東」間における財政力指数の平均値の差の検定結果です。「Mean Difference」が-0.355ということは、関東の方が北海道・東北より0.355高いことを意味しますね。p値 (\\(p_{\\text{tukey}}\\)) は0.001未満なので、この二地域間には財政力指数に統計的有意な差があることを意味します。\n三行目は「北海道・東北」と「近畿」間の差であり、統計的有意な差はありませんね。\n\n{.table .pure-table .pure-table-horizontal .table-striped .table-responsive} ||北海道・東北|関東|中部|近畿|中国・四国|九州・沖縄| |—|—|—|—|—|—|—| |北海道・東北||||||| |関東|差O|||||| |中部|差X|差O|-|||| |近畿|差X|差X|差X|-||| |中国・四国|差X|差O|差X|差X|-|| |九州・沖縄|差X|差O|差X|差X|差X|-|\n\n表でまとめると大雑把にはこんか感じですかね。多重比較の結果を効果的に示す方法にはいくつかあります。ネットなどで調べてみましょう。\n\n\n\n最後に記述統計です。多重比較の「Mean Difference」を見れば、どっちの地域の財政力指数が高いかは分かりますが、レポートや論文には基本的に記述統計を示すことも大事なので確認しておきましょう。"
  },
  {
    "objectID": "tutorial/JASP/JASP06.html",
    "href": "tutorial/JASP/JASP06.html",
    "title": "JASP入門: t検定",
    "section": "",
    "text": "今回は2群の平均値の差に統計的に有意な差があるか否かを検定する\\(t\\)検定のやり方を紹介します。"
  },
  {
    "objectID": "tutorial/JASP/JASP06.html#独立したサンプルのt検定",
    "href": "tutorial/JASP/JASP06.html#独立したサンプルのt検定",
    "title": "JASP入門: t検定",
    "section": "独立したサンプルの\\(t\\)検定",
    "text": "独立したサンプルの\\(t\\)検定\n\nここでは東日本と西日本における維新の得票率の差を検定してみましょう。\n\nここでは「維新の得票率」変数 (Ishin)を東/西日本変数 (Region2)変数で分割して平均値の差を比較する分析なので「独立サンプルのt検定」となります。\nまだはっきり理解できていない場合は色々とクリックしてみながら試行錯誤してみましょう。そのうちに慣れてきます。\n\n\n\n\n分析メニューの「T-Test」から「Independent Samples T-Test」を選択します。\n\n\n\nまず、どの変数の差を見るかを決めるのが「Dependent Variables (従属変数)」になります。今回は維新の得票率の差を検定するので「Ishin」変数を選択し、「▶」をクリックして従属変数リストへ入れます。\n次は、維新の得票率変数をどのようにグループ分けするかを決めます。これが「Grouping Variable」です。今回は東日本/西日本に分けたいので、この二つの地域情報が格納されている「Region2」変数をクリックし、「▶」をクリックしてグループ化変数へ入れます。\n\n\n\n選択項目がかなり多いですが、今回はできるかぎりSPSSの結果に近い結果が得られるようにします。\n\n「Tests」は「Students」のチェックを外し、「Welch」にチェツクします。「Student」は等分散を仮定したt検定であり、「Welch」は等分散を仮定しないt検定です。私の講義では基本的に等分散を仮定しない分析結果を採用すると申し上げましたね。したがって、「Welch」にチェックを入れます。むろん、他の項目にチェックを入れても結果が変わることはありませんので、試してみてください。\n「Additional Statistics」は分析結果をより楽に解釈できるような情報を提供してくれます。\n\n「Mean Difference」: 二つのグループの平均値の差を表示します。この場合、東/西日本における維新の得票率の差ですね。\n「Confidential Interval」: 本講義では説明しませんでしたが、95%信頼区間です。教科書や講義によっては「この区間内に真の差分が含まれる確率は95%です！」と説明されますが、ウソです。注意してください。具体的な意味に関してはネットを調べてみましょう。\n「Descriptive Statistics」: 記述統計です。維新の得票率の記述統計を東日本/西日本に分けて表示してくれるので便利です。\n\n\n\n\n\n分析結果です。下段の「Descriptives」は記述統計なので説明は省きます。ここでは上段の「t」、「df」、「p」、「Mean Difference」について説明します。\n\nt: t統計量です。具体的な説明は省きますが、レポート・論文なので結果を報告する際には必ず必要です。\ndf: 自由度です。これも報告の際には必要になる項目です。\np: 有意確率 (p値)です。今回は0.033ですね。一般的に有意水準 (α)は0.05であり、この結果はp値がαより小さいので帰無仮説は棄却されます。つまり、「東日本と西日本の維新の得票率には統計的に有意な差がある (95%水準で差がある)。」と解釈できます。\nMean Difference: 平均値の差分です。この場合は「東日本における維新の得票率 – 西日本における維新の得票率」となります。つまり、西日本が3.370%ポイント高いという意味です。"
  },
  {
    "objectID": "tutorial/JASP/JASP06.html#対応サンプルのt検定",
    "href": "tutorial/JASP/JASP06.html#対応サンプルのt検定",
    "title": "JASP入門: t検定",
    "section": "対応サンプルのt検定",
    "text": "対応サンプルのt検定\n\n今回は公明党と共産党の得票率に統計的に有意な差があるか否かを検定してみましょう。\n\n\n\n分析メニューの「T-Tests」の「Paired Samples T-Test」を選択します。\n\n\n\n比較する二つの変数を右のリストへ投入します。今回は公明党と共産党の得票率を比較するので、「Komei」変数と「JCP」変数を選択し、「▶」をクリックして右のリストへ移動させます。\n\n\n\n独立サンプルのt検定と似たような画面ですね。右の「Additional Statistics」は先ほどの画面と同一なので説明は省略します。ここで異なるのは「Tests」の方です（下段のAssumption Checksにも等分散検定の項目がなくなりました）。\n\nここでは「Student」一択です。「Wilcoxonの符号順位検定 (signed rank)」についてはネットで調べてみましょう。\n\n\n\n\n以上が分析結果です。結果の読み方は独立サンプルのt検定と同じです。\n\nこの場合、有意確率 (\\(p\\)値)が0.001未満であり、一般的な有意水準 (α = 0.05)より小さいので帰無仮説は棄却されます。つまり、「公明党と共産党の得票率には統計的に有意な差がある (5%水準で差がある)。」と解釈できます。また、「Mean Difference」は4.150であり、公明党の方が高いので「公明党の得票率は共産党より統計的に有意に高い (5%水準で高い)。」とも言えるでしょうね。"
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "宋財泫・矢内勇生『私たちのR:ベストプラクティスの探求』\n\nR Not for Everyone: An Esoteric Guide\nRプログラミングの入門教材\n現在、執筆中です\n\n\n以下の記事のほとんどは『私たちのR』用の原稿です。執筆後のアップデートは『私たちのR』で行います。最新版をご覧になる場合はRN4Eをクリックしてください。\n\nプログラミング\n\nRプログラミング入門の入門 RN4E (1) RN4E (2)\npurrr入門 RN4E\nオブジェクト指向型プログラミング入門 RN4E \nモンテカルロ・シミュレーション入門 RN4E\n\nデータハンドリング\n\ndplyr入門 RN4E (1) RN4E (2) RN4E (3)\ntidyr入門 RN4E\n\n可視化\n\nggplot2入門 [理論編] RN4E\nggplot2入門 [基礎編] RN4E\nggplot2入門 [応用編] RN4E\nggplot2入門 [発展編] RN4E\n\nR Markdown\n\nR Markdown入門 RN4E"
  },
  {
    "objectID": "tutorial/index.html#qualtrics",
    "href": "tutorial/index.html#qualtrics",
    "title": "Tutorials",
    "section": "Qualtrics",
    "text": "Qualtrics\n\nQualtrics入門 PDF\n\n2020年12月8日に高知工科大学で行った『Qualtrics講習会』資料です。\n現在（2022年5月）、QualtricsのUIは大幅に変更され、当資料のUIと一致しない箇所が多々あります。"
  },
  {
    "objectID": "tutorial/index.html#jasp",
    "href": "tutorial/index.html#jasp",
    "title": "Tutorials",
    "section": "JASP",
    "text": "JASP\n\nJASP入門講座\n\n本内容は私が担当しました2016/2017年に担当しました京都女子大学の「データ処理論 I / II」の補助教材です。SPSSの授業でしたが、家でもデータ分析ができるように無料ツールであるJASPの使用を推奨しました。現在はSPSSを用いた講義を担当しておりませんので、今後のアップデートの予定はございません。\nJASP 0.8.8.1に対応しています。2022年5月現在、最新バージョンは0.16.2です。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaehyun Song, Ph.D.",
    "section": "",
    "text": "メニュー案内\n\n\n\n\n宋のCVはCV\nこれまでの研究内容はResearch\n担当科目についてはTeaching\n宋ゼミについてはSeminar\nこれまで作成したアプリ/パッケージ/スクリプトはSoftware\n備忘録はNote\n分析/調査ツールのチュートリアルはTutorial\n\n\n\n\n\n\n\n\n\nInformation\n\n\n\n\nLast Update: 2022/07/07\nEnvironment\n\nmacOS 12.4 “Monterey”\nFirefox 102.0.0\nR version 4.2.1 (2022-06-23)\nRStudio 2022.07.0+545 “Spotted Wakerobin”\nQuarto 0.9.649\nR package {quarto} 1.1"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software / Packages / Scripts / Datasets",
    "section": "",
    "text": "BalanceR GitHub\n\n実験データの共変量チェック; A balance check for experimental data using standaradized biases\n\nSimpleConjoint GitHub\n\nサーバーなしで（ていうか、宋のサーバーで）コンジョイント実験を行う（要Qualtrics）\n\nPRcalc GitHub\n\n比例代表計算機; Proportional Representation Calculator\n\nwoRdle GitHub\n\nRパッケージ名で遊ぶWordle; woRdle: Enjoying Wordle with R package names."
  },
  {
    "objectID": "software/index.html#web-application",
    "href": "software/index.html#web-application",
    "title": "Software / Packages / Scripts / Datasets",
    "section": "Web Application",
    "text": "Web Application\n\nPRclac for Web ShinyApps GitHub"
  },
  {
    "objectID": "software/index.html#scripts",
    "href": "software/index.html#scripts",
    "title": "Software / Packages / Scripts / Datasets",
    "section": "Scripts",
    "text": "Scripts\n\nFor Qualtrics\n\nリスト実験をする時に項目をランダマイズするスクリプト\n画像URLをランダムに取得するスクリプトGitHub\nSimpleConjoint.js GitHub\n\nサーバーなしでコンジョイント実験を行う（要Qualtrics）"
  },
  {
    "objectID": "software/index.html#datasets",
    "href": "software/index.html#datasets",
    "title": "Software / Packages / Scripts / Datasets",
    "section": "Datasets",
    "text": "Datasets\n\nM-1 グランプリ決勝データGitHub\nインスタントラーメンの栄養成分データGitHub"
  },
  {
    "objectID": "software/scripts/list_randomizer.html",
    "href": "software/scripts/list_randomizer.html",
    "title": "リスト実験の項目をランダマイズする",
    "section": "",
    "text": "以下のようなリスト実験を行うとする。\n統制群\nQ1. 以下のうち、経験したことのあるものはいくつありますか？\n実験群\nQ1. 以下のうち、経験したことのあるものはいくつありますか？\nQualtricsなら質問の順番や選択肢の順番をランダマイズする機能を基本的に搭載しているが、質問文の中にある項目するランダマイズする機能は持っていない。ここでは質問文内の内容をランダマイズする方法を紹介する。"
  },
  {
    "objectID": "software/scripts/list_randomizer.html#phpスクリプトのダウンロード修正",
    "href": "software/scripts/list_randomizer.html#phpスクリプトのダウンロード修正",
    "title": "リスト実験の項目をランダマイズする",
    "section": "1. PHPスクリプトのダウンロード&修正",
    "text": "1. PHPスクリプトのダウンロード&修正\n\nPHPスクリプトをダウンロードする。Download\nPHPファイルの中身をみると3行目に項目が書いてある。 $listarray = array(\"1番目の項目\", \"2番目の項目\", \"3番目の項目\", \"4番目の項目\", \"5番目の項目\");の部分を$listarray = array(\"タバコ\", \"お酒\", \"コーヒー\", \"紅茶\");に修正する。\n適当な名前を付けて保存する。(たとえばList1.php)\n先に修正した部分にマリファナを付けて他の名前を付けて保存する(List2.php)。たとえば、$listarray = array(\"タバコ\", \"お酒\", \"コーヒー\", \"紅茶\", \"マリファナ\");\nPHPを走らせるサーバーにアップロードする。"
  },
  {
    "objectID": "software/scripts/list_randomizer.html#qualtricsのweb-service登録",
    "href": "software/scripts/list_randomizer.html#qualtricsのweb-service登録",
    "title": "リスト実験の項目をランダマイズする",
    "section": "2. QualtricsのWeb Service登録",
    "text": "2. QualtricsのWeb Service登録\n\nQualtircsの質問紙作成画面でSurvey Flowをクリック\nAdd a New Element hereをクリック\nWeb Serviceを選択し、URLにphpファイルのアドレスを入力\n\nたとえば、http://www.jaysong.net/cjoint/List_Rand.php\n\nTest URLをクリック\nSelectの隣のAllを選択し、Add Embedded Dataをクリック\nWeb Serviceのブロックをリスト実験の質問ブロックの上へ移動 (必ず!!!)"
  },
  {
    "objectID": "software/scripts/list_randomizer.html#質問文の作成",
    "href": "software/scripts/list_randomizer.html#質問文の作成",
    "title": "リスト実験の項目をランダマイズする",
    "section": "3. 質問文の作成",
    "text": "3. 質問文の作成\n\n普通のSingle Answer形式の質問文を作成するが、質問文を以下のように書く。たとえば、\n\n\nQ1. 以下の項目の中で該当することはいくつありますか。\n${e://Field/List-1}\n${e://Field/List-2}\n${e://Field/List-3}\n…\n\n\n同じ手順でもう一つのPHPファイルをWeb Serviceで読み込んで、質問文を作る。今回は項目が一個増えたから1番目より１行長くなる。\nSurvey Flowから見て以下の中の一つのように設定する。\n\nWeb Service(統制群) → 質問文(統制群) → Web Service(実験群) → 質問文(実験群)になるようにする。\nあるいはWeb Serviceを埋め込む時にKeyを設定すればWS→WS→統制質問→実験質問の形でも可能である。\n\n確実(かつ、面倒くさい)な方法としてはスクリプト12行目のList-の部分を変えながらアップロードする。\n出現する質問をランダマイズする。\n完了"
  },
  {
    "objectID": "notes/Etc/TwoPartySystem.html",
    "href": "notes/Etc/TwoPartySystem.html",
    "title": "二大政党制の指標に関する私案",
    "section": "",
    "text": "Two-party System Index (TSI)\n\n\\({SS}_1\\): 第一政党の議席率（\\({SS}_1 > 0\\)）\n\\({SS}_2\\): 第二政党の議席率（\\({SS}_2 > 0\\)）\n\n\n\\[\n({SS}_1 + {SS}_2)\\frac{{SS}_2}{{SS}_1}, \\quad \\text{where} \\quad {SS}_1 \\geq {SS}_2\n\\]"
  },
  {
    "objectID": "notes/Etc/TwoPartySystem.html#何を重視するか",
    "href": "notes/Etc/TwoPartySystem.html#何を重視するか",
    "title": "二大政党制の指標に関する私案",
    "section": "何を重視するか",
    "text": "何を重視するか\n\n（A）全議席における二大政党議席数の割合（\\(({SS}_1 + {SS}_2)\\)）\n\n1に近いほど二大政党制\n\n（B）二大政党の勢力均衡（\\(\\frac{{SS}_2}{{SS}_1}\\)）\n\n1に近いほど二大政党制\n0に近いほど一党優位制か多党制\n\n（A）と（B）の積が1なら完全なる二大政党制（2つの政党がちょうど50%、50%）\n\n0に近いほど多党制か、一党優位制"
  },
  {
    "objectID": "notes/Etc/TwoPartySystem.html#有効政党数laakso-and-taagepera-1979との比較",
    "href": "notes/Etc/TwoPartySystem.html#有効政党数laakso-and-taagepera-1979との比較",
    "title": "二大政党制の指標に関する私案",
    "section": "有効政党数（Laakso and Taagepera, 1979）との比較",
    "text": "有効政党数（Laakso and Taagepera, 1979）との比較\n\nTSIはあくまでも二大政党制の指標\n第3政党以下の議席率は計算に用いない。\n\nいくつかの例を使って比較してみる。\n\nenp <- function(x) {\n    return((1 / sum(x^2)))\n}\n\ntsi <- function(x) {\n    ss1 <- rev(sort(x))[1]\n    ss2 <- rev(sort(x))[2]\n    return((ss1 + ss2) * (ss2 / ss1))\n}\n\ncompare <- function(x) {\n    cat(\"ENP: \", enp(x), \"\\n\")\n    cat(\"TSI: \", tsi(x), \"\\n\")\n}\n\n\n議会内に2政党のみ存在する場合の比較\n\n比較のためにENPは元のENPから1を引く\n\n\n\nlibrary(tidyverse)\ntibble(PartyA = 50:99 / 100,\n       PartyB = 50:1 / 100) %>%\n    rowwise() %>%\n    mutate(ENP = enp(c(PartyA, PartyB)) - 1,\n           TSI = tsi(c(PartyA, PartyB))) %>%\n    pivot_longer(cols      = ENP:TSI,\n                 names_to  = \"Type\",\n                 values_to = \"Index\") %>%\n    ggplot() +\n    geom_line(aes(x = PartyA, y = Index, color = Type), \n              size = 1) +\n    labs(x = \"第一政党の議席率\", \n         y = \"多党制/一党優位制　←　指数　→　二大政党制\", color = \"\") +\n    theme_bw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n実は政党が2つのみだと、そこまで大きな差はないかも知れない。ただ、2つの政党の議席率がアンバランスしている時、ENPはそれでもより二大政党制と評価し、TSIは一党優位制と評価する。2つの指標が大きく異なるケースは、主に議会内政党数が3以上の場合（ここはENPから1を引かない）。\n\n4政党: 40%、40%、10%、10%\n\n\nexample1 <- c(0.4, 0.4, 0.1, 0.1)\ncompare(example1)\n\nENP:  2.941176 \nTSI:  0.8 \n\n\n\n4政党: 25%、25%、25%、25%\n\n\nexample2 <- c(0.25, 0.25, 0.25, 0.25)\ncompare(example2)\n\nENP:  4 \nTSI:  0.5 \n\n\n\n4政党: 60%、30%、5%、5%\n\n\nexample3 <- c(0.6, 0.3, 0.05, 0.05)\ncompare(example3)\n\nENP:  2.197802 \nTSI:  0.45 \n\n\n\n7政党: 25%、23%、20%、10%、10%、7%、5%\n\n\nexample4 <- c(0.25, 0.23, 0.20, 0.10, 0.10, 0.07, 0.05)\ncompare(example4)\n\nENP:  5.47046 \nTSI:  0.4416 \n\n\n\n10政党: 10%、10%、10%、10%、10%、10%、10%、10%、10%、10%\n\n\nexample5 <- rep(0.1, 10)\ncompare(example5)\n\nENP:  10 \nTSI:  0.2 \n\n\n\n8政党: 57%、7%、21%、9%、2%、2%、1%、1%\n\n\nexample6 <-c(0.57, 0.07, 0.21, 0.09, 0.02, 0.02, 0.01, 0.01)\ncompare(example6)\n\nENP:  2.610966 \nTSI:  0.2873684 \n\n\n二大政党制の指標としては\n\nTSIを使ってみる。\n上位二政党に限定し、ENPを計算する。\n\nどっちでもいけそうな気はする。"
  },
  {
    "objectID": "notes/simulation/kurohige.html",
    "href": "notes/simulation/kurohige.html",
    "title": "黒ひげ危機一発",
    "section": "",
    "text": "それでやってみた。100万回のシミュレーション。\n　まずは、設計から。黒ひげ危機一発のシミュレーションのためには以下の要素を設定しとかないといけない。それは\n\n穴の数\n当たり（負け）の穴の位置\nプレイヤーの数\n\n　当たりの位置はランダムで決まるから、関数の引数として用いるのは穴の数（hole）とプレイヤーの数（player）、そしてシミュレーションの試行回数（n）である。 誰が当たるのかを特定するために以下のようなアルゴリズムを使う。\n\n一様分布 (\\(\\mbox{Uniform}(1, \\text{hole})\\)) から一個 (ただし、1以上の整数) を抽出し、これを「当たり」とする。\n\\(X \\in \\{1, 2, ..., \\text{hole}\\}\\) からholeの数だけ無作為抽出する。ただし、一回抽出された数字は二度と選ばれない (非復元抽出)。これがプレイヤーが「短剣を差し込む順番」である。\nもし、当たりが1番の穴であり、順番のベクトルが（3, 4, 1, 5, 2）なら3番目のプレイヤーが当たることになる。\nただし、通常のようにplayer < holeの場合、mod(当たりの位置、プレイヤーの数)番目のプレイヤーが当たる。 3の例で４人のプレイヤーがいるとすると、当たりが２番の穴である場合、当たりはベクトルの5番目である。mod(5, 4) = 1、つまり、1番目のプレイヤーが当たる。\nこれをn回繰り返す。\n\n以上のアルゴリズムをRで表現すると\n\nkuro.sim <- function(hole = 24, player = 4, trial = 100){ #関数を定義する\n    result <- rep(NA, trial) #結果を書き込むベクトルを用意\n    for(i in 1:trial){\n        kurohige <- seq(from = 1, to = hole, by = 1) #U(1, hole)の生成\n        atari    <- sample(kurohige, 1) #当たりの指定\n        sasikomi <- sample(kurohige, hole, rep = FALSE) #差し込みの順番\n        \n        if(match(atari, sasikomi) %% player == 0){     #atariをplayerで割って余りが0なら\n            result[i] <- player} else{ #最後のプレイヤーが当たり、それ以外の場合は\n            result[i] <- match(atari, sasikomi) %% player\n            } #atariをplayerで割った余り番目のプレイヤーが当たる\n    }\n    \n    return(table(result)) #結果を表形式で返す\n}\n\n　結果を確認してみよう。\nケース1: 24穴、4人、10万回試行\n\n\n\n\nKuro1 <- kuro.sim(hole = 24, player = 4, trial = 100000)\nKuro1\n\nresult\n    1     2     3     4 \n25014 24986 25121 24879 \n\n\n　結果を見ると順番なんか関係なく、当たりの確率は全てのブレイヤーにおいて約25%、つまり2万5千回くらいである。黒ひげ危機一発って、わりと公平なゲームだな…\n　でも、もし5人でやったら？7人でやったら？つまり、穴の数をプレイヤーの数で割って余りがある場合はどうなるか。せっかく関数も作ったからやってみよう。今回こそ100万回だ！\nケース2: 24穴、5人、100万回試行\n\nKuro2 <- kuro.sim(hole = 24, player = 5, trial = 1000000)\nKuro2\n\nresult\n     1      2      3      4      5 \n207372 208727 209611 208214 166076 \n\n\nケース3: 24穴、7人、100万回試行\n\nKuro3 <- kuro.sim(hole = 24, player = 7, trial = 1000000)\nKuro3\n\nresult\n     1      2      3      4      5      6      7 \n166755 166706 166899 125104 124205 124898 125433 \n\n\n　5人の場合は5番目のプレイヤーが、7人の場合は4番目以降のプレイヤーが当たる確率が確実に下る。つまり、mod(hole, player)番目以降のプレイヤーが当たる確率が低い。"
  },
  {
    "objectID": "notes/simulation/mhp.html",
    "href": "notes/simulation/mhp.html",
    "title": "モンティ・ホール問題",
    "section": "",
    "text": "統計学の教科書でしばしば紹介される例の一つがモンティ・ホール問題である。これはアメリカのテレビ番組の中のゲームであり、様々な論争まで行われてきた。\nゲームのルールは簡単\n\n3つのドアがあり、1つのドアの後ろに商品がある。\n参加者はドアを選択する。\n司会者（モンティ）が残りのドア2つの中で商品がないドアを開けて中身を見せる。\nここで参加者はドアの選択を変える機会が与えられる。\n\n　直観的に考えて、司会者が外れのドアを1つ教えてくれたなら、残りの2つのドアの1つに絶対に商品がある。そのうち一つは自分が既に選んだドア。どう見ても当たる確率は半々であって、変えても、変えなくても当たる確率は同じだと思われる。\n　しかし、結果からいうと選択を変えた方が変えなかった場合より当たる確率が2倍である。これは条件付き確率とベイズの定理を用いることで数学的に説明できる。興味のある人はWikipediaの項目を参照。この問題を巡る論争とかも紹介されていてなかなか面白い。"
  },
  {
    "objectID": "notes/simulation/mhp.html#シミュレーションの設定",
    "href": "notes/simulation/mhp.html#シミュレーションの設定",
    "title": "モンティ・ホール問題",
    "section": "シミュレーションの設定",
    "text": "シミュレーションの設定\n\n商品があるドアを決める（atari）\n参加者が選択するドアを決める（selection）\n\nもし、atariとselectionが一致すると\n\n既に参加者は当たりを選んでいるので、選択を変えると商品がもらえなくなる（if_switch = 0）\n既に参加者は当たりを選んでいるので、選択を変えなかったら商品がもらえる（if_not_switch = 1）\n\n一方、atariとselectionが不一致すると\n\n司会者は参加者が選んだドア（ハズレ）を除く二つのドアの中でハズレのドアを教えてくれる。つまり、残りの一つのドアに商品がある。\nこの場合、選択を変えると商品がもらえる（if_switch = 1）\nもし、選択を変えなかったら商品はもらえない(if_not_switch = 0)\n\n\nこれをN回やってみて、選択を変えた場合の当たり率を確認する。"
  },
  {
    "objectID": "notes/simulation/mhp.html#シミュレーションのrコード",
    "href": "notes/simulation/mhp.html#シミュレーションのrコード",
    "title": "モンティ・ホール問題",
    "section": "シミュレーションのRコード",
    "text": "シミュレーションのRコード\n\n#シミュレーションの関数の定義\nMH_sim <- function (trials = 1000) {\n    #結果を返す空のデータフレームの作成\n    result_df <- data.frame(id            = rep(NA, trials),\n                            atari         = rep(NA, trials),\n                            selection     = rep(NA, trials),\n                            if_switch     = rep(NA, trials),\n                            if_not_switch = rep(NA, trials))\n    \n    for(i in 1:trials){\n        atari     <- sample(1:3, 1) #当たりのドア番号\n        selection <- sample(1:3, 1) #参加者が選んだドア番号\n        \n        if (atari == selection) { \n            #もし参加者が当たりのドアを選んでいたら\n            if_switch     <- 0 #選択を変えると商品がもらえない\n            if_not.switch <- 1 #選択を変えないなら商品がもらえる\n        } else { \n            #参加者がハズレのドアを選んでいたら\n            if_switch     <- 1 #選択を変えたら商品がもらえる\n            if_not.switch <- 0 #選択を変えないと商品がもらえない\n        }\n        \n        #結果をデータフレームに保存する\n        result_df[i,] <- c(i,\n                           atari,\n                           selection,\n                           if_switch,\n                           if_not.switch)\n    }\n    \n    return(result_df)  #結果を返す\n}"
  },
  {
    "objectID": "notes/simulation/mhp.html#シミュレーションの実行",
    "href": "notes/simulation/mhp.html#シミュレーションの実行",
    "title": "モンティ・ホール問題",
    "section": "シミュレーションの実行",
    "text": "シミュレーションの実行\n\n\n\n\nMH_Result <- MH_sim(trials = 10000)\n\ntable(MH_Result$if_switch)\n\n\n   0    1 \n3335 6665 \n\n\n　1万回のシミュレーションの結果、選択肢を変えた場合の当たり率は66.65%、変えなかった場合は33.35%であり、ほぼ2倍になることが分かる。"
  },
  {
    "objectID": "notes/simulation/llm.html",
    "href": "notes/simulation/llm.html",
    "title": "大数の法則",
    "section": "",
    "text": "統計学を勉強するなら「大数の法則(Law of large numbers)」は一度くらいは聞くはず。この法則が知らなくても統計的手法を「使う」のは問題ない。しかし、その手法の背後にあるこれらの法則を知らないと「使う」ことは出来ても「理解」することは難しい。\n　とにかく、この法則は強法則と弱法則があるが、簡単に言えば「ある母集団から相互独立的に選ばれたサンプルのサイズが大きければ大きいほど、そのサンプルの平均値(標本平均)は母集団の平均値(母平均)に収斂する」ということ。\n…？\n　簡単に言ったつもりだが、今見たらそこまで分かりやすい表現ではなさそうだ。\n　例えば地球上にいる人々の身長の平均値はどうだろうか。適当に一人を選んで身長を測ったとしても、それが地球上の人々の平均値と一致する可能性は極めて低い(厳密に言うとゼロ)し、直観的に考えてみて平均値からかなり外れている可能性も高いと考えられる。 しかし、１億人くらいを適当に選んで平均をとったら？その場合は先と比べてかなり平均値に近いと考えられる。これが大数の法則である。 今後やってみようと思うが、「中心極限定理」とは違う。中心極限定理は「標本平均の分布」の話であって、「標本平均そのもの」の話ではない。 ならば早速シミュレーションしてみよう。"
  },
  {
    "objectID": "notes/simulation/llm.html#シミュレーション設計",
    "href": "notes/simulation/llm.html#シミュレーション設計",
    "title": "大数の法則",
    "section": "シミュレーション設計",
    "text": "シミュレーション設計\n　太平洋の島国、ナウルの人々の収入の平均値はいくらだろうか。なぜナウルかというと理由はない。ちょうど人口が１万くらいの何らかの例が欲しかっただけ。あくまでも用いるデータは仮想のデータである。\n\nナウルの人口は1万210人とする。\nナウル国民の所得の平均値は約5千ドルであって、標準偏差2千ドルの正規分布をしている1。つまり、所得が5千ドルくらいの人が最も多く、9千ドル以上とか1千ドル以下は非常に少ない。\nこの1万210人の所得データから無作為にナウル人を抽出し、抽出されたナウル人の所得の平均値をとる。\nただし、無作為に「何人」ととるかが問題。ここでは大数の法則を経験的に確認するために1人から1000人まで徐々に増やしていく。\n一回抽出したデータはまた抽出されることもある（復元抽出）。これはサンプリングが相互独立であるためである。"
  },
  {
    "objectID": "notes/simulation/llm.html#シミュレーションコード",
    "href": "notes/simulation/llm.html#シミュレーションコード",
    "title": "大数の法則",
    "section": "シミュレーション・コード",
    "text": "シミュレーション・コード\n　まずは、ナウル国民の所得を生成する。ナウル国民の所得は\\(\\text{Normal}(\\mu = 5000, \\sigma = 2000)\\)の分布をしている。\n\n# まずはtidyverseを読み込んでおく\nlibrary(tidyverse)\n\nnauru_income <- round(rnorm(10210, mean = 5000, sd = 2000))\n\n　これで10210人のナウル国民の所得を設定した。ならば生成されたデータを見てみる。\n\nsummary(nauru_income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -2155    3621    4996    5004    6383   13048 \n\n\n　ナウル国民の所得の平均は5017ドル、最小値は-1761だそうだ。「所得にマイナスなんてあり得るか！」と思うかも知れないが、細かいことは無視しよう。知りたいのは大数の法則を経験的に確認することであって、ナウル国民の所得ではない。\n　次はヒストグラムで見てみよう。\n\nnauru_income %>%\n    enframe(name = \"ID\", value = \"Income\") %>%\n    ggplot() + \n    geom_histogram(aes(x = Income, y = ..density..),\n                   color = \"white\", binwidth = 500) +\n    stat_function(fun = dnorm, args = list(mean = 5000, sd = 2000),\n                  color = \"red\", size = 1) +\n    labs(x = \"所得\", y = \"密度\")\n\n\n\n\n\n\n\n\n　きれいな正規分布の形をしている。 次は、この1万210人の国民からランダムに抽出し、抽出された人の所得の平均値を見てみる。抽出される人数は最初は一人とし、徐々に増やしていく。\n\n\n\n\n#まずは空のデータフレイムを作っとく\nresult_df <- data.frame(n = NA, mean = NA) \n\nfor(i in 1:1000){\n  result_df[i, ] <- c(i, mean(sample(nauru_income, i, replace = TRUE)))\n}\n\n\nhead(result_df)\n\n  n     mean\n1 1 4989.000\n2 2 7278.500\n3 3 6658.667\n4 4 5291.500\n5 5 4412.400\n6 6 6140.000\n\n\n　抽出された人が一人の場合、彼(女)の所得は7710ドルであって、6人の場合、彼(女)らの所得の平均値は5502ドルである。 これをグラフで見ると\n\nresult_df %>%\n    ggplot() + \n    geom_hline(yintercept = mean(result_df$mean), color = \"red\") +\n    geom_line(aes(x = n, y = mean)) +\n    labs(x = \"サンプルサイズ\", y = \"平均値\")\n\n\n\n\n\n\n\n\n　抽出される人が多くなるほど、その平均値はナウル国民の平均値に近づいていくことが確認できる。\n\n「でも、こんなの偶然かも知れないじゃん！」\n\nと思うこともできる。この作業を10回くらいやってみよう。\n\nresult_df <- as.data.frame(matrix(rep(NA, 1000 * 11), ncol = 11))\nnames(result_df) <- c(\"n\", paste0(\"mean\", 1:10))\n\nfor (i in 1:1000) {\n    result_df[i, 1] <- i\n    for (j in 2:11) {\n        result_df[i, j] <- c(mean(sample(nauru_income, i, replace = TRUE)))   \n    }\n}\n\nresult_df %>%\n    pivot_longer(cols      = mean1:mean10,\n                 names_to  = \"trial\",\n                 values_to = \"mean\") %>%\n    ggplot() +\n    geom_hline(yintercept = 5000, color = \"red\") +\n    geom_line(aes(x = n, y = mean, group = trial), alpha = 0.2) +\n    labs(x = \"サンプルサイズ\", y = \"平均値\")\n\n\n\n\n\n\n\n\n　だいたいこんなもんだ。むしろ、偶然とろろかよりきれいなグラフが出来上がったと思う。もうちょっと拡大してみると\n\nresult_df %>%\n    pivot_longer(cols      = mean1:mean10,\n                 names_to  = \"trial\",\n                 values_to = \"mean\") %>%\n    filter(n <= 100) %>%\n    ggplot() +\n    geom_hline(yintercept = 5000, color = \"red\") +\n    geom_line(aes(x = n, y = mean, group = trial), alpha = 0.2) +\n    labs(x = \"サンプルサイズ\", y = \"平均値\")\n\n\n\n\n\n\n\n\n　こんなもんだ。抽出される人が多くなると彼(女)らの所得の平均値は全体国民の平均値に近づく。これが大数の法則である。 直観的に考えれば当然のことでもあるが、この大数の法則と今後紹介する中心極限定理などに基づいて（伝統的）統計学2における「仮説検定」が成り立っているとも言える。"
  },
  {
    "objectID": "notes/simulation/clt.html",
    "href": "notes/simulation/clt.html",
    "title": "中心極限定理",
    "section": "",
    "text": "set.seed(19861008)\n\n\n正規分布の例\n\n\\(\\mu\\)と\\(\\sigma\\)\n期待値は\\(\\mu\\)\n分散は\\(\\sigma^2\\)であり、標準偏差は\\(\\sigma\\)\n\\(\\mu = 10\\)、\\(\\sigma = 5\\)の正規分布（ 図 1 ）の場合、期待値は10、標準偏差は5\n\n\n\n\n\n\n図 1: 正規分布の確率密度曲線（\\(\\mu\\) = 10、\\(\\sigma\\) = 5）\n\n\n\n\n\nvec1 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec1[i] <- mean(rnorm(1, mean = 10, sd = 5))\n}\n\nvec1 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    coord_cartesian(xlim = c(-5, 25)) +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec1)\n\n[1] 9.993612\n\nsd(vec1)\n\n[1] 5.052632\n\n\n\nvec2 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec2[i] <- mean(rnorm(10, mean = 10, sd = 5))\n}\n\nvec2 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    coord_cartesian(xlim = c(-5, 25)) +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec2)\n\n[1] 9.994122\n\nsd(vec2)\n\n[1] 1.584378\n\n\n\nvec3 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec3[i] <- mean(rnorm(100, mean = 10, sd = 5))\n}\n\nvec3 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    coord_cartesian(xlim = c(-5, 25)) +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec3)\n\n[1] 10.00129\n\nsd(vec3)\n\n[1] 0.4980742\n\n\n\n\n一様分布の例\n\n最小値（\\(a\\)）と最大値（\\(b\\)）\n期待値は\\(\\frac{a + b}{2}\\)\n分散は\\(\\frac{(b - a)^2}{12}\\)であり、標準偏差は\\(\\frac{b - a}{\\sqrt{12}}\\)\n\\(a = 5\\)、\\(b = 15\\)の一様分布（ 図 2 ）の場合、期待値は10、標準偏差は約2.89\n\n\n\n\n\n\n図 2: 一様分布の確率密度曲線（\\(a\\) = 5、\\(b\\) = 15）\n\n\n\n\n\nvec4 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec4[i] <- mean(runif(100, min = 5, max = 15))\n}\n\nvec4 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec4)\n\n[1] 9.999566\n\nsd(vec4)\n\n[1] 0.2892989\n\n\n\n\nポアソン分布の例\n\n\\(\\lambda\\)のみ\n期待値は\\(\\lambda\\)\n分散は\\(\\lambda\\)であり、標準偏差は\\(\\sqrt{\\lambda}\\)\n\\(\\lambda = 10\\)のポアソン分布（ 図 3 ）の場合、期待値は10、標準偏差は約3.16\n\n\n\n\n\n\n図 3: ポアソン分布の確率密度曲線（\\(\\lambda\\) = 10）\n\n\n\n\n\nvec5 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec5[i] <- mean(rpois(100, lambda = 10))\n}\n\nvec5 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec5)\n\n[1] 10.00047\n\nsd(vec5)\n\n[1] 0.3174378\n\n\n\n\nガンマ分布の例\n\n形状パラメーター（\\(k\\)）と尺度パラメーター（\\(\\theta\\)）\n期待値は\\(k\\theta\\)\n分散は\\(k\\theta^2\\)であり、標準偏差は\\(\\sqrt{k}\\theta\\)\n\\(k = 2\\)、\\(\\theta = 5\\)のガンマ分布（ 図 4 ）の場合、期待値は10、標準偏差は約7.07\n\n\n\n\n\n\n図 4: ガンマ分布の確率密度曲線（\\(k\\) = 2、\\(\\theta\\) = 5）\n\n\n\n\n\nvec6 <- rep(NA, 10000)\nfor (i in 1:10000) {\n    vec6[i] <- mean(rgamma(100, shape = 2, scale = 5))\n}\n\nvec6 %>%\n    enframe(name = \"trial\", value = \"mean\") %>%\n    ggplot() +\n    geom_histogram(aes(x = mean), color = \"white\") +\n    geom_vline(xintercept = 10, color = \"red\") +\n    labs(x = \"標本平均\", y = \"度数\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nmean(vec6)\n\n[1] 10.00008\n\nsd(vec6)\n\n[1] 0.7052151"
  },
  {
    "objectID": "notes/simulation/consistency.html",
    "href": "notes/simulation/consistency.html",
    "title": "一致性・不偏性・効率性",
    "section": "",
    "text": "ここでは統計学を勉強する上でよく混同される一致性と不偏性について解説します。"
  },
  {
    "objectID": "notes/simulation/consistency.html#一致性とは",
    "href": "notes/simulation/consistency.html#一致性とは",
    "title": "一致性・不偏性・効率性",
    "section": "一致性とは",
    "text": "一致性とは\n　一致性はサンプルサイズが大きくなると、推定値が母数へ収束することを意味します。たとえば、\\(\\mu = 0\\)、\\(\\sigma = 100\\)の正規分布から独立的に抽出された\\(n\\)個の確率変数\\(X\\)があるとします。つまり、以下のような状況を考えます。\n\\[\nX_1, X_2, X_3, ..., X_n \\overset{\\text{iid}}{\\sim} N(0, 100).\n\\]\n　ただし、ここでは\\(\\mu = 0\\)、\\(\\sigma = 100\\)を仮定しましたが、実際において私たちは\\(\\mu\\)と\\(\\sigma\\)の値は分かりません。たとえば、手元に男女100人の身長データがあっても、日本人全体の身長の平均値と分散は分かりません。したがって、私たちは手元にあるこの\\(X\\)のみを用いて、未知である母集団の特徴を推論する必要があります。手元の\\(X\\)から母集団における平均値（\\(\\mu\\)）を推定する際によく使われるが平均値（\\(\\bar{X}\\)）です。\n　たとえば、\\(n = 5\\)の場合を考えてみましょう。\n\nset.seed(19861009) # 乱数のseedを指定\nX1 <- rnorm(5, 0, 100) # N(0, 100)から乱数を5個抽出する\nprint(X1) # 抽出された乱数を出力する\n\n[1] -195.662813  153.088455   25.989905   -3.071453  -53.218536\n\nmean(X1) # 抽出された乱数の平均値を出力する\n\n[1] -14.57489\n\n\n　\\(n = 5\\)の場合、\\(\\bar{X}\\)は-14.5748884です。それでは\\(n = 100\\)ならどうでしょうか。\n\nX2 <- rnorm(100, 0, 100) # N(0, 100)から乱数を100個抽出する\nprint(X2) # 抽出された乱数を出力する\n\n  [1]  -12.513448   16.473814  -55.827125  -86.033925  196.563039  -59.660676\n  [7]  178.879309  -95.615549 -117.455299   53.094037   80.033960  -47.082599\n [13] -102.146620   89.866926   -7.909525  -31.347797   30.952894  107.081687\n [19] -195.426684  -71.759205   48.629528  116.743532   51.594264 -211.544831\n [25]  160.766456  308.931296   36.696450   78.402401   13.495426  -19.456266\n [31] -120.529780    6.163411  -24.332314  138.872863  -30.410003   39.268512\n [37] -124.424084   59.200185   54.027469   72.515763 -178.219784 -179.507844\n [43]   47.350023  -52.366831   98.083261  -44.570705  217.776695   62.598027\n [49]  -90.665200  103.918722 -142.114845   58.524963  -33.849776   75.790091\n [55]  -89.727866    5.718650  -14.870296  -42.457583  -39.518473   49.421506\n [61]   17.351046  -39.949024  -62.180725  -21.830242   42.782942   35.902584\n [67]  -28.100441  -86.049005   37.312739  -43.398827 -288.436043   40.767558\n [73]   18.953100   -4.163539   59.720674   97.730085   48.131342  -83.222430\n [79]   71.003268  181.770553   82.021292  -59.789360   61.338302 -154.959971\n [85] -173.761830 -177.086379  147.810801  -58.044487  140.592706   32.867267\n [91]   60.883589  -50.408130 -124.858910   32.480463 -104.253371  -26.842073\n [97]  -76.278107  -92.971755   16.245089  -78.430475\n\nmean(X2) # 抽出された乱数の平均値を出力する\n\n[1] -2.732595\n\n\n　\\(n = 100\\)の場合の\\(\\bar{X}\\)は-2.732595ですね。最後に、\\(n = 10000\\)ならどうでしょうか。このサンプルを出力するのはあまりにも結果が長くなるので、ここでは平均値だけを出力します。\n\nX3 <- rnorm(10000, 0, 100) # N(0, 100)から乱数を1万個抽出する\nmean(X3) # 抽出された乱数の平均値を出力する\n\n[1] -0.9447894\n\n\n　\\(n = 10000\\)の場合の\\(\\bar{X}\\)は-0.9447894ですね。このように\\(n\\)が大きくなると、\\(\\bar{X}\\)は母集団の平均値（ここでは\\(\\mu = 0\\)）へ近づきます。このようにサンプルサイズが大きくなると推定値が母数へ近づくことを「一致性 (consistency)」と呼びます。この例だと、「標本の平均値（\\(\\bar{X}\\)）は母集団の平均値（\\(\\mu\\)）の一致推定量である」と言えます。これは\\(\\bar{X} \\overset{p}{\\rightarrow} \\mu\\)と表記されます。より一般的に書くと推定値\\(\\hat{\\theta}\\)が母数\\(\\theta\\)の一致推定量である場合、\\(\\hat{\\theta} \\overset{p}{\\rightarrow} \\theta\\)と表記します。\n　この表記は大数の弱法則とも共通しますが、実際、一致性は大数の弱法則によって成り立ちます。\n\nlibrary(tidyverse)\n\nX_bar_vec <- rep(NA, 1000)     # 長さ1000の空ベクトルを作成\nfor (i in 1:1000) {            # iを1から1000へ増やしながら反復\n  temp_vec <- rnorm(i, 0, 100) # N(0, 100)からi個の乱数を抽出し、temp_vecに格納\n  # temp_vecの平均値をX_bar_vecのi番目要素として格納\n  X_bar_vec[i] <- mean(temp_vec)\n}\n\n# 可視化\nggplot() +\n  geom_line(aes(x = 1:1000, y = X_bar_vec)) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"n\", y = expression(hat(theta)))\n\n\n\n\n　\\(n\\)を大きくすることによって\\(\\bar{X}\\)が\\(\\mu (=0)\\)へ近づくことが分かります。この\\(n\\)を無限にすると (\\(n \\rightarrow \\infty\\))、\\(\\bar{X} = \\mu\\)になるでしょう。したがって、\\(\\bar{X}\\)は\\(\\mu\\)の一致推定量となります。"
  },
  {
    "objectID": "notes/simulation/consistency.html#不偏性とは",
    "href": "notes/simulation/consistency.html#不偏性とは",
    "title": "一致性・不偏性・効率性",
    "section": "不偏性とは",
    "text": "不偏性とは\n　一致性と混同されやすい概念が「不偏性（unbiasedness）」です。これは推定値\\(\\hat{\\theta}\\)の期待値が母数\\(\\theta\\)と一致することを意味します。数式で表すと\\(\\mathbb{E}[\\hat{\\theta}] = \\theta\\)であり、一致性と違って、サンプルサイズ（\\(n\\)）とは無関係な性質です。\n　たとえば、\\(\\mu = 0\\)、\\(\\sigma = 100\\)の正規分布から独立的に5個の値を抽出します（\\(n = 5\\)）。そして、この5個の値の平均値（\\(\\bar{X}\\)）を計算します。この作業を1万回繰り返したいと思います。\n\ndf <- tibble(Trial = 1:10000,\n             X1    = rnorm(10000, 0, 100),\n             X2    = rnorm(10000, 0, 100),\n             X3    = rnorm(10000, 0, 100),\n             X4    = rnorm(10000, 0, 100),\n             X5    = rnorm(10000, 0, 100))\n\nhead(df)\n\n# A tibble: 6 × 6\n  Trial    X1     X2     X3     X4     X5\n  <int> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1     1 -91.6  -20.6   42.1 -28.8  -134. \n2     2 -22.7 -204.    66.6 134.     66.2\n3     3 -26.1   89.4   37.4 146.   -108. \n4     4 -27.9  -68.9   94.8  39.6   -50.7\n5     5  48.4  -65.0  110.  -23.1   -17.2\n6     6  99.2  -29.2 -142.   -9.03   39.9\n\n\n　それでは\\(X_1\\)から\\(X_5\\)までの平均値を計算します。\n\ndf <- df %>%\n  mutate(X_bar = (X1 + X2 + X3 + X4 + X5) / 5)\n\nhead(df)\n\n# A tibble: 6 × 7\n  Trial    X1     X2     X3     X4     X5  X_bar\n  <int> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1     1 -91.6  -20.6   42.1 -28.8  -134.  -46.5 \n2     2 -22.7 -204.    66.6 134.     66.2   8.13\n3     3 -26.1   89.4   37.4 146.   -108.   27.9 \n4     4 -27.9  -68.9   94.8  39.6   -50.7  -2.62\n5     5  48.4  -65.0  110.  -23.1   -17.2  10.6 \n6     6  99.2  -29.2 -142.   -9.03   39.9  -8.18\n\n\n　この\\(\\bar{X}\\)の平均値（\\(\\bar{\\bar{X}}\\)）、つまり\\(\\mathbb{E}[\\bar{X}]\\)はどうでしょうか。厳密には\\(\\mathbb{E}[\\bar{X}]\\)ではありませんが、試行回数が無限回となると\\(\\mathbb{E}[\\bar{X}]\\)となります。試行回数1万は十分多いと考えられるので、期待値に近い結果が得られると考えられます。\n\nmean(df$X_bar)\n\n[1] 0.09291811\n\n\n　\\(\\mathbb{E}[\\bar{X}]\\)は0.0929181であり、ほぼ0であることが分かります。この0が母数（\\(\\mu\\)）であり、\\(\\mathbb{E}[\\bar{X}] = \\mu\\)の関係が成り立つことが分かります。したがって、\\(\\bar{X}\\)は\\(\\mu\\)の不偏推定量となります。\n　不偏推定量は複数存在することもあります。ここまで見てきました標本平均\\(\\bar{X}\\)も\\(\\mu\\)の不偏推定量の一つに過ぎません。たとえば、以下のような推定量を考えてみましょう。\n\\[\n\\begin{eqnarray}\n  Y_1 & = X_3 \\\\\n  Y_2 & = \\frac{1}{9}(X_1 + 2 \\cdot X_2 + 3 \\cdot X_3 + 2 \\cdot X_4 + X+5).\n\\end{eqnarray}\n\\]\n\ndf <- df %>%\n  mutate(Y1 = X3,\n         Y2 = (1/9) * (X1 + 2 * X2 + 3 * X3 + 2 * X4 + X5))\n\nhead(df)\n\n# A tibble: 6 × 9\n  Trial    X1     X2     X3     X4     X5  X_bar     Y1    Y2\n  <int> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl>\n1     1 -91.6  -20.6   42.1 -28.8  -134.  -46.5    42.1 -22.0\n2     2 -22.7 -204.    66.6 134.     66.2   8.13   66.6  11.6\n3     3 -26.1   89.4   37.4 146.   -108.   27.9    37.4  50.0\n4     4 -27.9  -68.9   94.8  39.6   -50.7  -2.62   94.8  16.4\n5     5  48.4  -65.0  110.  -23.1   -17.2  10.6   110.   20.6\n6     6  99.2  -29.2 -142.   -9.03   39.9  -8.18 -142.  -40.3\n\n\n　それでは\\(\\mathbb{E}[Y_1]\\)と\\(\\mathbb{E}[Y_2]\\)を計算してみましょう。\n\nmean(df$Y1)\n\n[1] 0.5527504\n\nmean(df$Y2)\n\n[1] 0.1050386\n\n\n　それぞれ0.5527504と0.1050386の結果が得られており、こちらも\\(\\mu\\)に非常に近いことが分かります。標本平均以外にも母平均の不偏推定量がいくらでもあり得ることが分かります。\n　しかし、なぜ\\(\\mu\\)の不偏推定量として\\(\\bar{X}\\)が使われているのでしょうか。それは\\(\\bar{X}\\)が最も効率的な推定量であるからです。"
  },
  {
    "objectID": "notes/simulation/consistency.html#効率性とは",
    "href": "notes/simulation/consistency.html#効率性とは",
    "title": "一致性・不偏性・効率性",
    "section": "効率性とは",
    "text": "効率性とは\n　不偏推定量の中で一つ選ぶとしたら「効率性（efficiency）」というものを基準とします。同じ不偏推定量であれば、精度の高い推定量、つまり分散が小さい推定量がいいでしょう。この精度の良さが「効率性」です。他にも、「有効性」と呼ばれる場合もあります。\n　効率性の最も良い推定量のことを有効推定量と呼びます。この効率性は簡単にいうと推定量の分散を意味します。これまで\\(\\mu\\)の不偏推定量として\\(\\bar{X}\\)、\\(Y_1\\)、\\(Y_2\\)を見てきました。これら3つの不偏推定量の効率性は\\(V[\\bar{X}]\\)、\\(V[Y_1]\\)、\\(V[Y_2]\\)を用いて調べることが可能です。\n\nvar(df$X_bar)\n\n[1] 2004.154\n\nvar(df$Y1)\n\n[1] 9974.554\n\nvar(df$Y2)\n\n[1] 2359.181\n\n\n　それぞれの分散は2004.1535382、9974.5536133、2359.1813379であり、\\(V[\\bar{X}]\\)が最も分散が小さい、つまり最も効率的な推定量であることが分かります。\\(\\bar{X}\\)は一致推定量でありながら不偏推定量であり、そして最も効率的な\\(\\mu\\)の推定量となります。"
  },
  {
    "objectID": "notes/simulation/consistency.html#一致推定量不偏推定量",
    "href": "notes/simulation/consistency.html#一致推定量不偏推定量",
    "title": "一致性・不偏性・効率性",
    "section": "一致推定量=不偏推定量?",
    "text": "一致推定量=不偏推定量?\n　先ほどの標本平均は一致推定量でありながら、不偏推定量でした。しかし、世の中には一致推定量でありながら、不偏推定量ではない推定量が存在します。その代表的な例が標本分散（\\(s^2\\)）です。標本分散（\\(s^2\\)）は母集団における分散（\\(\\sigma^2\\)）の一致推定量ですが、不偏推定量ではありません。\n　たとえば、\\(\\mu = 0\\)、\\(\\sigma = 100\\)の正規分布から独立的に1000万個の乱数を生成したとします。この場合、母分散（\\(\\sigma^2\\)）は10000となります。そして、この1000万個の値の標本分散（\\(s^2\\)）と不偏分散（\\(\\hat{\\sigma}^2\\)）を計算します。ちなみに、Rのvar()は不偏分散を計算するため、標本分散を求めるためには不偏分散に\\(\\frac{n-1}{n}\\)を掛ける必要があります。\n\nX <- rnorm(10000000, 0, 100)\nvar(X) * ((length(X) - 1) / length(X)) # 標本分散\n\n[1] 9996.52\n\nvar(X) # 不偏分散\n\n[1] 9996.521\n\n\n　どれも母分散に非常に近い値が得られており、\\(n\\)を無限にすると母分散に一致すると予想されます。したがって、\\(s^2 \\overset{p}{\\rightarrow} \\sigma^2\\)、\\(\\hat{\\sigma}^2 \\overset{p}{\\rightarrow} \\sigma^2\\)であり、どれも一致推定量であることが分かります。\n\ndf <- tibble(Trial = 1:10000,\n             X1    = rnorm(10000, 0, 100),\n             X2    = rnorm(10000, 0, 100),\n             X3    = rnorm(10000, 0, 100),\n             X4    = rnorm(10000, 0, 100),\n             X5    = rnorm(10000, 0, 100))\n\ndf <- df %>%\n  rowwise() %>% # 行単位の演算\n  mutate(S_2         = var(c(X1, X2, X3, X4, X5)) * (4/5), # 各試行における標本分散\n         Sigma_hat_2 = var(c(X1, X2, X3, X4, X5)))         # 各試行における不偏分散\n\nhead(df)\n\n# A tibble: 6 × 8\n# Rowwise: \n  Trial     X1    X2    X3    X4     X5    S_2 Sigma_hat_2\n  <int>  <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>       <dbl>\n1     1  -59.1  29.6  44.2 -70.3  -14.7  2097.       2621.\n2     2  149.   37.8 -25.9 -19.8  145.   5847.       7308.\n3     3   47.6 158.  -20.8 123.  -219.  17846.      22307.\n4     4 -175.  -17.2 138.   12.2  -93.2 11005.      13756.\n5     5  137.  173.   53.6 -33.2   29.7  5521.       6902.\n6     6  118.   28.8  40.6 -15.6   76.7  2021.       2526.\n\n\n　それでは\\(s^2\\)と\\(\\hat{\\sigma}^2\\)の平均値（\\(\\simeq\\)期待値）を計算してみましょう。\n\nmean(df$S_2) # 標本分散の期待値\n\n[1] 8067.114\n\nmean(df$Sigma_hat_2) # 不偏分散の期待値\n\n[1] 10083.89\n\n\n　標本分散の期待値は約8000で、母分散の\\(\\frac{n-1}{n}\\)、つまり\\(\\frac{4}{5}\\)となります。一方、不偏分散の場合期待値が母分散に非常に近いことが分かります。したがって、不偏分散は一致性と不偏性があり、標本分散は一致性のみあることが分かります。\n　逆に不偏推定量でありながら一致推定量ではない推定量も存在します。たとえば、以下のような例を考えてみましょう。\n\n\\(X_1, X_2, X_3, ..., X_n \\overset{\\text{iid}}{\\sim} N(0, 100)\\)とする\n\\(|X_i| = \\max{(|X_1|, |X_2|, |X_3|, ..., |X_i|, ..., |X_n|)}\\)の場合、\\(\\hat{\\theta} = X_i\\)\n\n　要するに、絶対値の最も高かった値を\\(\\theta\\)の推定量（\\(\\hat{\\theta}\\)）として用いることを意味します。この場合、\\(\\hat{\\theta}\\)は一致推定量でしょうか。\\(n\\)を1から1000まで増やしながら確認してみましょう。\n\ntheta_hat_vec <- rep(NA, 1000) # 長さ1000の空ベクトルを作成\nfor (i in 1:1000) {            # iを1から1000へ増やしながら反復\n  temp_vec <- rnorm(i, 0, 100) # N(0, 100)からi個の乱数を抽出し、temp_vecに格納\n  # temp_vecの中で最も絶対値の高い値はtheta_hat_vecのi番目要素として格納\n  theta_hat_vec[i] <- temp_vec[abs(temp_vec) == max(abs(temp_vec))]\n}\n\n# 可視化\nggplot() +\n  geom_line(aes(x = 1:1000, y = theta_hat_vec)) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"n\", y = expression(hat(theta)))\n\n\n\n\n　このように\\(n\\)をいくら増やしても\\(\\hat{\\theta}\\)は母数である0へ収束しないことが分かります。つまり、この推定量は一致推定量ではありません。それでは、この推定量は不偏推定量でしょうか。ここでは話を単純にするために\\(n = 3\\)の例を考えてみましょう。不偏性はサンプルサイズ（\\(n\\)）と無関係ですので、これでも問題ないでしょう。試行数は100万とします。\n\n# シミュレーション用のデータを作成\ndf <- tibble(Trial = paste0(\"Trial\", 1:1000000),\n             X1    = rnorm(1000000, 0, 100),\n             X2    = rnorm(1000000, 0, 100),\n             X3    = rnorm(1000000, 0, 100))\n\ndf <- df %>% \n  # X1, X2, X3の中で絶対値の最も高い値をTheta_Hatとする\n  mutate(Theta_Hat = case_when(abs(X1) > abs(X2) & abs(X2) > abs(X3) ~ X1,\n                               abs(X2) > abs(X3)                     ~ X2,\n                               TRUE                                  ~ X3)) \n\nhead(df) # dfの中身を確認する\n\n# A tibble: 6 × 5\n  Trial       X1    X2     X3 Theta_Hat\n  <chr>    <dbl> <dbl>  <dbl>     <dbl>\n1 Trial1   48.6   73.3  -70.9      73.3\n2 Trial2 -377.   152.  -182.     -182. \n3 Trial3  105.   -46.6   90.8      90.8\n4 Trial4   52.5  -83.3  -66.8     -83.3\n5 Trial5 -121.   -56.9   83.9      83.9\n6 Trial6    8.23  51.4   92.5      92.5\n\n\n\n# dfのTheta_Hat列の期待値（平均値）を計算する。\nmean(df$Theta_Hat)\n\n[1] -0.02872534\n\n\n　非常に0に近い結果が得られており、これはほぼ\\(\\mu\\)と同じ値となります。施行数が無限回になると、\\(\\mathbb{E}[\\hat{\\theta}]\\)は\\(\\mu\\)と一致していくだろうと考えられます。\n　我々が教科書でよく見る推定量は一般的に一致性と不偏性を同時に満たす推定量が多いでしょう。しかし、例外も常に存在します。標本分散もその一つの例です。他にも、最尤推定法から得られた分散の推定量は一致推定量ではあるものの、不偏推定量ではないことが知られています。"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "社会調査\n\nQualtricsによるコンジョイント分析(Conjoint Analysis)の設問票作成方法\nConjoint SDTの用いる際、一部の属性のみを固定する方法\n\n\n\n統計学\n\np値に関するアメリカ統計学会の声明 \n分類\n\nロジスティック回帰による分類と可視化・評価\nk近傍法による分類と可視化・評価 RPubs  \n\n\n\n\nシミュレーション\n\n一致性・不偏性・効率性\nハミルトニアン・モンテカルロ・シミュレーション(Hamiltonian Monte Carlo Simulation; HMC)のRコード RPubs\n中心極限定理 (Central Limit Theorem)\nモンティ・ホール問題 (Monty Hall Problem)\n大数の法則 (Law of Large Number)\n黒ひげ危機一発 (Pop-up Pirate)\n\n\n\nその他\n\n二大政党制の指標に関する私案"
  },
  {
    "objectID": "notes/qualtrics/cjoint_fix_attr.html",
    "href": "notes/qualtrics/cjoint_fix_attr.html",
    "title": "Conjoint SDTの用いる際、一部の属性のみを固定する方法",
    "section": "",
    "text": "候補者1\n候補者2\n\n\n\n\n性別\n男性\n女性\n\n\n年齢\n39歳\n35歳\n\n\n二郎の値段\nラーメンの値下げ\nラーメンの値段の現状維持\n\n\n二郎のトッピング\nやさい増々の義務化\nにんにくの義務化\n\n\n\nQ1. あなたはどっちの候補者を支持しますか。\n\n候補者1\n候補者2\n\nこの場合、Conjoint SDT(以下、SDTと略す)で属性の順番ををランダマイズすると、場合によっては「二郎のトッピング、性別、年齢、二郎の値段」などの順番で表示されうる。しかし、直感的に考えると年齢や性別などの候補者の個人情報は政策の前に来るのが自然である。\nSDTはこのように一部の属性のみを固定する機能を提供しないため、生成されたPHPファイルを直接修正する必要がある。\nここから提示する方法はこのポストに従ってPHPファイルまで生成されたと仮定したうえでの方法である。ただし、1.1の画像でRandomize order of attributes for each respondentsにチェックを入れておく必要がある。\n\nPHPファイルを読み込む\n93行目を見ると以下のような項目が見える。\n\nshuffle($featureArrayKeys);\n修正するのはこの箇所である。\n\n\n\n基本的な考え方\n\n\n\nアドレス\n0\n1\n2\n3\n\n\n\n\n項目名\n性別\n年齢\n二郎の値段\n二郎のトッピング\n\n\n\n\nPHPの配列のアドレスはRとは違って、一般的なプログラミング言語と同様、0から始まる。\nshuffle()関数は配列の順番をランダマイズする機能を持つが、ここでは0から1を切り取って臨時のオブジェクトに格納し(オブジェクト1)、続いて2から3を切り取って他のオブジェクトに格納する(オブジェクト2)。\n次に、オブジェクト2の中身だけshuffle()関数を使って順番をランダマイズし、オブジェクト1とオブジェクト2を統合する。\n具体的な方法は以下のようである。\n\n$temp_obj1 = array_slice($featureArrayKeys, 0, 2);\n$temp_obj2 = array_slice($featureArrayKeys, 2, 2);\nshuffle($temp_obj2);\n$featureArrayKeys = array_merge($temp_obj1, $temp_obj2);\n\n説明すると\n\n$featureArrayKeys配列で0から2個の要素（性別、年齢）を切り取り、$temp_obj1に格納する。\n$featureArrayKeys配列で2から2個の要素（二郎の値段、二郎のトッピング）を切り取り、$temp_obj2に格納する。\n$temp_obj2内の要素をランダマイズする。\n$temp_obj1と$temp_obj2を統合し、元の$featureArrayKeysに格納する。\n\n\n結果的に見るとPHPファイルの93行目は\nBefore\n// Re-randomize the $featurearray keys\nshuffle($featureArrayKeys);\nAfter\n// Re-randomize the $featurearray keys\n// shuffle($featureArrayKeys); <- 元の部分はコメントアウト\n$temp_obj1 = array_slice($featureArrayKeys, 0, 4);\n$temp_obj2 = array_slice($featureArrayKeys, 4);\nshuffle($temp_obj2);\n$featureArrayKeys = array_merge($temp_obj1, $temp_obj2);\nのようになる。これだけで、年齢と性別のみ固定し、残りの部分はランダマイズすることが可能である。以降は普通のやり方でQualtricsに埋め込めばいい。"
  },
  {
    "objectID": "notes/qualtrics/cjoint_sdt.html",
    "href": "notes/qualtrics/cjoint_sdt.html",
    "title": "Conjoint SDT + Qualtricsでコンジョイント分析",
    "section": "",
    "text": "この記事はConjoint Survey Design Toolの旧バージョンのものです。\n\n\n\n2019年4月にConjoint Survey Design Tool Version 2が公開され、python3へ移行されました。python3ではユニコードがデフォルトになっているため、phpスクリプトの修正は不要となります。以下の記事は旧バージョンの例です（実行方法のみ最新版に対応させました）。最新バージョンであるConjoint Survey Design Tool Version 3に対応する形で書き直したいと思います。\nQualtricsでコンジョイント(Conjoint)分析のための調査票を作成する手順を紹介します。QualtricsにもConjoint機能がありますが、ここで目指すのはHoriuchi, Smith, and Yamamoto(2015)のような調査SSRN Linkを目指します。"
  },
  {
    "objectID": "notes/qualtrics/cjoint_sdt.html#目標",
    "href": "notes/qualtrics/cjoint_sdt.html#目標",
    "title": "Conjoint SDT + Qualtricsでコンジョイント分析",
    "section": "目標",
    "text": "目標\nこのような調査をしたいと想定する。\nQ1. 最も好きな候補者を選択してください。\n\n\n\n\n候補者1\n候補者2\n候補者3\n\n\n\n\n消費税\n10% (引き上げ)\n0% (廃止)\n8% (維持)\n\n\n集団的自衛権\n容認\n容認\n否認\n\n\n福祉\n拡充\n縮小\n維持\n\n\n\n\n候補者1\n候補者2\n候補者3\n\nここで政策の順番やその中身はランダムである。ここで用語を統一しておきたい。\n\nタスク(tasks): 質問の繰り返す回数を意味する。\nプロフィール(profiles): 選択肢の数を意味し、ここでは3である。(候補者1~3 / A~C)\n属性(attributes): 各プロフィールが有する特性であり、ここでは3である。(消費税、集団的自衛権、福祉)\n水準(levels): 各属性がとりうる値である。たとえば福祉の水準は3つである。(拡充、維持、縮小)"
  },
  {
    "objectID": "notes/qualtrics/cjoint_sdt.html#conjoint-survey-design-tool",
    "href": "notes/qualtrics/cjoint_sdt.html#conjoint-survey-design-tool",
    "title": "Conjoint SDT + Qualtricsでコンジョイント分析",
    "section": "1. Conjoint Survey Design Tool",
    "text": "1. Conjoint Survey Design Tool\n\nConjoint Survey Design ToolのGitHubレポジトリーへアクセスし、ファイルを入手する。GitHub\n\nmacOS / Linuxの場合はconjointSDT.pyを、Windowsの場合はconjointSDT.exeをダウンロードする。\n\nConjoint Survey Design Tool(SDT)を起動する。\n\nmacOS / Linuxの場合はターミナルでSDT保存したフォルダへ移動し、python3 conjointSDT.pyと入力する。\nWindowsの場合はconjointSDT.exeを実行する。\n\n\n\n1.1. TaskとProfileの設定\n\n\nメニューから[Edit] → [Settings]へ\n\n\n\nNumber of Tasks per Respondentを2と入力\nNumber of Profiles per Taskを3と入力\nまた、ここでは消費税、集団的自衛権、福祉といった固定された順番で表示されるのでRandomize order of attributes for each respondentsのチェックを外す。\n[Save Setting]をクリック\n\n\n\n1.2. 属性と水準を入力\n\n\n[Attribute]下の[Add]をクリックし、taxと入力。すくなくともOS X環境では、Conjoint SDTから直接日本語入力はできない。テキストエディタで入力してコピペする方法は出来るが、phpで出力が出来ないため意味がない。まずは、全部英語で入力する。\n同じ手順でanpoとwelfareを追加する。\n\n\n\n[Attribute]でtaxを選択し、[Levels]の下のAddをクリックし、10%と入力\n同じ手順で8%, 5%, 0%も入力する。\n3~4の手順をanpo, welfareにも適用する。anpoにはyonin, hininを、welfareにはkakujyu, iji, shukushoを追加する。\n\n\n\n1.3. ファイルで出力\n\n\nメニューの[Edit] → [Export to PHP]を選択し、適当に名前をつけて保存。\n同じく[Edit] → [Create Qualtrics Question Templates]を選択し、適当な名前を付ける。ただし、この手順は必須ではない。"
  },
  {
    "objectID": "notes/qualtrics/cjoint_sdt.html#phpファイルの修正とアップロード",
    "href": "notes/qualtrics/cjoint_sdt.html#phpファイルの修正とアップロード",
    "title": "Conjoint SDT + Qualtricsでコンジョイント分析",
    "section": "2. phpファイルの修正とアップロード",
    "text": "2. phpファイルの修正とアップロード\n\n2.1. phpファイルの修正\n\nテキストエディタ(私の場合はSublime Text)で先ほど保存したphpファイルを開く。\n適当にtaxとかanpoで検索すれば、このような行が確認できる。\n\n$featurearray = array(\"tax\" => array(\"10%\",\"8%\",\"5%\",\"0%\"),\"anpo\" => array(\"yonin\",\"hinin\"),\"welfare\" => array(\"kakujyu\",\"iji\",\"shukusho\"));\n\nここを以下のように修正する。\n\n$featurearray = array(\"消費税\" => array(\"10%(引き上げ)\",\"8%(維持)\",\"5%(引き下げ)\",\"0%(廃止)\"),\"集団的自衛権\" => array(\"容認\",\"否認\"),\"福祉\" => array(\"拡充\",\"維持\",\"縮小\"));\n\n保存する。\n\n\n\n2.2. phpファイルのアップロード\n\nTransmitやCyberduckを利用してphpがインストールされているサーバーにアップロードする。\n\n学生が利用できる大学のWeb空間はphpが利用できないところが多いので、多分出来ないと思います\n\n自分のサーバーがない人はサーバーをレンタルするか、持っている人にご飯をおごる。\nちなみに私の場合、http://tintstyle.cafe24.com/untitled.phpでアップロードしておきました。"
  },
  {
    "objectID": "notes/qualtrics/cjoint_sdt.html#qualtricsで調査票作成",
    "href": "notes/qualtrics/cjoint_sdt.html#qualtricsで調査票作成",
    "title": "Conjoint SDT + Qualtricsでコンジョイント分析",
    "section": "3. Qualtricsで調査票作成",
    "text": "3. Qualtricsで調査票作成\n\n3.1. phpファイルを埋め込む\n\nQualtricsへログインし、[Create Survey]を選択。\n[Quick Survey Builder]をクリックし、適当に名前を付ける。\n[Survey Flow]をクリック。\n新しいウィンドウに[Add a New Element Here]があるのでそれをクリック。\n[Web Service]ボタンをクリック\n先ほどアップロードしたphpファイルのアドレスを入力し、右の[Test URL]をクリック\n[Select fields to include as embedded data]ウィンドウが表示される。[Select]の隣の[All]をクリックし、下の[Add Embedded Data]をクリック\nそのまま[Save Flow]をクリックし、調査票作成画面へ戻る。\n\n\n\n3.2. 調査票作成\n\n[Create a New Question]でMultiple Choiceを選択する。\nオプションはChoiceは3と入力し、Single Answerを選択。\n選択肢の[Click to write Choice 1]を「候補者1」にし、残りの2つも同様に「候補者2, 3」とする。\n質問文を作成するが、ここでは2つの方法がある。\n\n1.3で[Create Qualtrics Question Templates]を選択した場合は3.2.1へ\n選択しなかったら3.2.2へ\n\n\n\n3.2.1. Qualtrics Question Templateを利用した方法\n\n1.3で[Create Qualtrics Question Templates]を選択したらTaskの数だけhtmlファイルが生成される。\nテキストエディタでTask1のファイルを開き、全てのコピーする。\n質問文をクリックし、ボックス上段の[HTML View]をクリックし、先ほどコピーしたものを貼り付ける。\n[Normal View]をクリックし、質問文を修正する。\nここで注意するものは「${〜}」で囲まれている部分は修正しないこと。\nその他の部分を修正すればいい。\n次の質問を同様に作成し、今回はTask 2のhtmlファイルの内容を貼り付ける。\n\n\n\n3.2.2. 直接打ち込む方法\n\n自分なりに質問文を作成するが、属性は以下のように書く。\n\n${e://Field/F-[タスク]-[属性]}\n\n${e://Field/F-1-3}ならタスク1の3番めの属性であり、つまり「福祉」である。Conjoint SDTで属性のランダム化のチェックを外したため、すぐに分かる。\n水準は以下のように表記する。\n\n${e://Field/F-[タスク]-[プロフィール]-[属性]}\n\n${e://Field/F-1-3-1}ならタスク1、候補者3、1番目の属性(福祉)の値である。これは0%, 5%, 8%, 10%からランダムに出る。\n同じ手順でタスク2の質問文を作成する。その時は${e://Field/F-2-[プロフィール]-[属性]}のように書く。\n以上の作業は[HTML View]でなく[Normal View]で行う。表を挿入するなら[Rich Content Editor]で作業すれば楽"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html",
    "href": "notes/toolbox/free_r_book.html",
    "title": "無料で読めるR教材",
    "section": "",
    "text": "少しずつ埋めていきます"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#全般",
    "href": "notes/toolbox/free_r_book.html#全般",
    "title": "無料で読めるR教材",
    "section": "全般",
    "text": "全般\n\nSONG Jaehyun・矢内勇生.『私たちのR: ベストプラクティスの探求』\n森知晴.『卒業論文のためのR入門』\nNathaniel D. Phillips. YaRrr! The Pirate’s Guide to R\nChester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\nHadley Wickham and Garrett Grolemund. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data [Korean]\nHadley Wickham. Advanced R (2nd Ed.)\nHadley Wickham and Jennifer Bryan. R Packages: Organize, Test, Document and Share Your Code\nColin Gillespie and Robin Lovelace. Efficient R programming: A Practical Guide to Smarter Programming\nRoger D. Peng. R Programming for Data Science\nGarrett Grolemund. Hands-On Programming with R: Write Your Own Functions and Simulations\nThe Carpentries. R for Reproducible Scientific Analysis\nMonash Data Fluency . Reproducible Research in R\nGarrett Grolemund. Master the Tidyverse\nHadley Wickham. The tidyverse style guide\nOscar Baruffa. Big Book of R\nMark van der Laan, Jeremy Coyle, Ivana Malenica, Rachael Phillips, and Rachael Phillips. Targeted Learning in R: Causal Data Science with the tlverse Software \nAndrew Ba Tran. R for Journalists\nnissinbo. 「医療情報DB解析のためのデータハンドリング大全」\n김길환. 2021. R 프로그래밍 (개정판)\nYang Feng and Jianan Zhu. R Programming: Zero to Pro\nSocio-Econ. 『資料 R&Rパッケージを利用した社会経済データの取得と利用方法(作成中)』\n山本雅資『2021年度データ分析入門』\nJaeyeon Kim. Computational Thinking for Social Scientists.\nSyunsuke Fukuda.『Rビギナーズガイド』\nR Bootcamp"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#可視化",
    "href": "notes/toolbox/free_r_book.html#可視化",
    "title": "無料で読めるR教材",
    "section": "可視化",
    "text": "可視化\n\nKieran Healy. Data Visualization: A practical introduction\nClaus O. Wilke. Fundamentals of Data Visualization\nWinston Chang. R Graphics Cookbook (2nd Ed.)\nThe R Graph Gallery\nWorld Bank. R Econ Visual Library"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#r-markdown",
    "href": "notes/toolbox/free_r_book.html#r-markdown",
    "title": "無料で読めるR教材",
    "section": "R Markdown",
    "text": "R Markdown\n\nYihui Xie, J. J. Allaire, and Garrett Grolemund. R Markdown: The Definitive Guide\nYihui Xie, Christophe Dervieux, and Emily Riederer. R Markdown Cookbook\n\n『R Markdown クックブック』\n\nYihui Xie, Amber Thomas, and Alison Presmanes Hill. blogdown: Creating Websites with R Markdown\nYihui Xie. bookdown: Authoring Books and Technical Documents with R Markdown\nResul Umit. Writing Reproducible Research Papers with R Markdown\nMatthew Crump. Open tools for writing open interactive textbooks\n高橋康介.「[連載] R Markdownで楽々レポートづくり」"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#shiny",
    "href": "notes/toolbox/free_r_book.html#shiny",
    "title": "無料で読めるR教材",
    "section": "Shiny",
    "text": "Shiny\n\nHadley Wickham. Mastering Shiny.\nDavid Granjon. Outstanding User Interfaces with Shiny."
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#計量経済学計量政治学統計学",
    "href": "notes/toolbox/free_r_book.html#計量経済学計量政治学統計学",
    "title": "無料で読めるR教材",
    "section": "計量経済学、計量政治学、統計学",
    "text": "計量経済学、計量政治学、統計学\n\n土井翔平.『Rで計量政治学入門』\n津田裕之.『Rによる統計入門』\n安藤道人・三田匡能.『Rで学ぶ計量経済学と機械学習』\nChristoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer. Introduction to Econometrics with R\nFlorian Heiss. Using R for Introductory Econometrics, 2nd edition\n\nPythonバージョン\n\nNeale Batra (Ed.). The Epidemiologist R Handbook\nAdam Fleischhacker. A Business Analyst’s Introduction to Business Analytics"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#ベイジアン統計学",
    "href": "notes/toolbox/free_r_book.html#ベイジアン統計学",
    "title": "無料で読めるR教材",
    "section": "ベイジアン統計学",
    "text": "ベイジアン統計学\n\nA Solomon Kurz. Statistical Rethinking with brms, ggplot2, and the tidyverse.\nAlicia A. Johnson, Miles Ott and Mine Dogucu. Bayes Rules! An Introduction to Bayesian Modeling with R.\nVirgilio Gómez-Rubio. Bayesian inference with INLA."
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#因果推論",
    "href": "notes/toolbox/free_r_book.html#因果推論",
    "title": "無料で読めるR教材",
    "section": "因果推論",
    "text": "因果推論\n\n矢内勇生.『KUT 計量経済学応用: 統計的因果推論入門』\nPaul C. Bauer. Applied Causal Analysis (with R)\nGertler, Paul J., Sebastian Martinez, Patrick Premand, Laura B. Rawlings, and Christel M. J. Vermeersch. Impact Evaluation in Practice, Second Edition\n\n[Solution] Liam F. Beiser-McGrath. Impact Evaluation in Practice: Solutions for Second Edition in R"
  },
  {
    "objectID": "notes/toolbox/free_r_book.html#その他",
    "href": "notes/toolbox/free_r_book.html#その他",
    "title": "無料で読めるR教材",
    "section": "その他",
    "text": "その他\n\n国里愛彦・竹林由武.『今日からできる再現可能な論文執筆』\nMax Kuhn and Julia Silge. Tidy Modeling with R\nPaul Roback and Julie Legler. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R\nRafael A. Irizarry. Introduction to Data Science: Data Analysis and Prediction Algorithms with R\nJulia Silge and David Robinson. Text Mining with R: A Tidy Approach\nPablo Casas. Data Science Live Book\nMathias Harrer, Pim Cuijpers, Toshi A. Furukawa, and David D. Ebert. Doing Meta-Analysis in R: A Hands-on Guide\n슬기로운 통계생활. 딥러닝 공략집 with R\nChris Brown, Murray Cadzow, Paula A Martinez, Rhydwyn McGuire, David Neuzerling, David Wilkinson, Saras Windecker. Github actions with R.\nYan Lu and Sharon L. Lohr. R Companion for Sampling: Design and Analysis (3rd Ed.)"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html",
    "href": "notes/toolbox/ci_materials.html",
    "title": "因果推論の道具箱",
    "section": "",
    "text": "少しずつ埋めていきます"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#因果推論全般",
    "href": "notes/toolbox/ci_materials.html#因果推論全般",
    "title": "因果推論の道具箱",
    "section": "因果推論全般",
    "text": "因果推論全般\n\nMiguel A. Hernán and James M. Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC. [Web]\nScott Cunningham. 2021. Causal Inference: The Mixtape. Yale University Press. [Web]\nNick Huntington-Klein. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. [Web]\nGuido W. Imbens and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nStephen L. Morgan and Christopher Winship. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. Cambridge University Press.\nJudea Pearl, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Wiley.\n\n『入門 統計的因果推論』\n\n安井翔太. 2020.『効果検証入門』技術評論社\nThad Dunning. 2008. “Improving Causal Inference: Strengths and Limitations of Natural Experiments.” Political Research Quarterly. 61 (2): 282-293\nMichael G. Findley, Kyosuke Kikuta, and Michael Denly. 2021. “External Validity.” Annual Review of Political Science. 24: 365-393\nSeonho Kim. Awesome Causal Inference\nBrady Neal. Introduction to Causal Inference\n嶌田栄樹・依田高典. 2020.「因果性と異質性の経済学①：限界介入効果」『京都大学大学院経済学研究科ディスカッションペーパーシリーズ』No.J-20-002\n嶌田栄樹・依田高典. 2020.「因果性と異質性の経済学②：Causal Forest」『京都大学大学院経済学研究科ディスカッションペーパーシリーズ』No.J-20-004\nBryan, Christopher J., Elizabeth Tipton and David S. Yeager. 2021. “Behavioural science is unlikely to change the world without a heterogeneity revolution,” Nature Human Behaviour.\n川田恵介. 『Rによる比較・予測・因果推論入門』\n古川知志雄.『統計推論再考 – 概念と技法 –』"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#potential-outcome-framework",
    "href": "notes/toolbox/ci_materials.html#potential-outcome-framework",
    "title": "因果推論の道具箱",
    "section": "Potential Outcome Framework",
    "text": "Potential Outcome Framework\n\nFrancesca Dominici, Falco J. Bargagli-Stoffi, and Fabrizia Mealli. 2020. “From controlled to undisciplined data: estimating causal effects in the era of data science using a potential outcome framework.” arXiv:2012.06865\nLaura Forastiere, Edoardo M. Airoldi, and Fabrizia Mealli. 2020. “Identification and Estimation of Treatment and Interference Effects in Observational Studies on Networks.” Journal of the American Statistical Association."
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#dag",
    "href": "notes/toolbox/ci_materials.html#dag",
    "title": "因果推論の道具箱",
    "section": "DAG",
    "text": "DAG\n\nPeter W G Tennant, Eleanor J Murray, Kellyn F Arnold, Laurie Berrie, Matthew P Fox, Sarah C Gadd, Wendy J Harrison, Claire Keeble, Lynsie R Ranker, Johannes Textor, Georgia D Tomova, Mark S Gilthorpe, and George T H Ellison. 2020. “Use of directed acyclic graphs (DAGs) to identify confounders in applied health research: review and recommendations,” International Journal of Epidemiology. dyaa213\nCausal Inference Korea. The Book of Why"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#randomized-controlled-trials",
    "href": "notes/toolbox/ci_materials.html#randomized-controlled-trials",
    "title": "因果推論の道具箱",
    "section": "Randomized Controlled Trials",
    "text": "Randomized Controlled Trials\n\nSusan Athey and Guido Imbens. 2016. “The Econometrics of Randomized Experiments,” arXiv:1607.00698\n長谷川龍樹・多田奏恵・米満文哉・池田鮎美・山田祐樹・高橋康介・近藤洋史. 2021.「実証的研究の事前登録の現状と実践ーOSF事前登録チュートリアル」『心理学研究』92(3): 188-196.\n河野勝. 2016. 「政治学における実験研究」『早稻田政治經濟學雜誌』391: 7-16\n多湖淳. 2021. 「政治学における実験研究 その2」『早稻田政治經濟學雜誌』397: 2-7\n\n\nField Experiments\n\nAlan S. Gerber and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W W Norton & Co Inc\nTrisha Phillips. 2021. “Ethics of Field Experiments,” Annual Review of Political Science. 24: 277-300.\n\n\n\nLaboratory Experiments\n\n\n\n\n\n\n\nSurvey Experiments\n\nSong Jaehyun・秦正樹. 2020. 「オンライン・サーベイ実験の方法: 理論編」『理論と方法』35 (1): 92-108.\n秦正樹・Song Jaehyun. 2020. 「オンライン・サーベイ実験の方法: 実践編」『理論と方法』35 (1): 109-127.\nChristopher Grady. 10 Things to Know About Survey Experiments .\nSergio Martini and Francesco Olmastroni. 2021. “From the lab to the poll: The use of survey experiments in political research.” Italian Political Science Review / Rivista Italiana di Scienza Politica Italian Political Science Review.\nLuke W. Miratrix, Jasjeet S. Sekhon, Alexander G. Theodoridis, and Luis F. Campos. Worth Weighting? “How to Think About and Use Weights in Survey Experiments.” Political Analysis. 26(3): 275-291.\nIngar Haaland, Christopher Roth and Johannes Wohlfart. forthcoming. “Designing Information Provision Experiments,” Journal of Economic Literature\nDavid J. Hauser, Phoebe C. Ellsworth and Richard Gonzalez. 2018. “Are Manipulation Checks Necessary?,” Frontiers in Psychology, 9:998\n\n\nSurvey Experiments: Priming/Framing\n\n\n\n\n\n\n\nSurvey Experiments: List\n\n(多変量解析を用いた被験者の属性ごとの予測値の推定1) Imai Kosuke. 2011. “Multivariate Regression Analysis for the Item Count Technique.” Journal of the American Statistical Association. 106(494):407–416.\n(多変量解析を用いた被験者の属性ごとの予測値の推定2) Blair Graeme, Imai Kosuke. 2012. “Statistical Analysis of List Experiments.” Political Analysis. 20(1):47–77\n(Cobminedリスト実験とその仮定の検定) Peter M. Aronow, Alexander Coppock, Forrest W. Crawford, and Donald P. Green. 2015. “Combining List Experiment and Direct Question Estimates of Sensitive Behavior Prevalence.” Journal of Survey Statistics and Methodology, 3(1):43–66.\n(予測値を説明変数として用いる手法) Kosuke Imai, Bethany Park, and Kenneth F. Greene. 2017. “Using the Predicted Responses from List Experiments as Explanatory Variables in Regression Models.” Political Analysis. 23(2): 180-196.\nWinston Chou, Kosuke Imai, and Bryn Rosenfeld. 2020. “Sensitive Survey Questions with Auxiliary Information.” Sociological Methods & Research. 49(2): 418-454.\n(洗練化されたリスト実験) Tsuchiya Takahiro, Hirai Yoko. 2010. “Elaborate Item Count Questioning: Why Do People Underreport in Item Count Responses?” Survey Research Methods. 4(3):139–149.\n(教育水準の低い被験者における過大推定/統制群にプラセボ項目を挿入し、バイアスを抑制) Guillem Riambau and Kai Ostwald. 2020. “Placebo statements in list experiments: Evidence from a face-to-face survey in Singapore.” Political Science Research and Methods. 9(1):172-179.\n(リスト実験から得られた推定値の不安定性について) Stefanie Gosen, Peter Schmidt, Stefan Thörner and Jürgen Leibold. 2018. “Is the List Experiment Doing its Job?: Inconclusive Evidence!” Einstellungen und Verhalten in der empirischen Sozialforschung. pp.179-205.\nGraeme Blair, Alexander Coppock, and Magaret Moor. 2020. “When to Worry about Sensitivity Bias: A Social Reference Theory and Evidence from 30 Years of List Experiments.” American Political Science Review, 114(4): 1297–1315.\n\n\n\nSurvey Experiments: Conjoint\n\n(解説1) Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. 2014. “Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.” Political Analysis. 22(1):1–30.\n(解説2) Kirk Bansak, Jens Hainmueller, Dan Hopkins, and Teppei Yamamoto. “Conjoint Survey Experiments,” in Jamie Druckman and Don Green ed. Cambridge Handbook of Advances in Experimental Political Science.\n(AMCEについて) Kirk Bansak, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. “Using Conjoint Experiments to Analyze Elections: The Essential Role of the Average Marginal Component Effect (AMCE)” SSRN.\n(MMについて) Thomas J. Leeper, Sara B. Hobolt, and James Tilley. 2019. “Measuring Subgroup Preferences in Conjoint Experiments,” Political Analysis. 28(2): 207-221.\n(属性数について) Kirk Bansak, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. 2019. “Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments,” Political Science Research and Methods.\n(タスク数について) Kirk Bansak, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. 2018. “The Number of Choice Tasks and Survey Satisficing in Conjoint Experiments.” Political Analysis. 26 (1): 112-119.\n(日本の事例) 宋財泫・善教将大. 2016. 「コンジョイント実験の方法論的検討」『法と政治』67(2): 67-108.\n(外的妥当性について1) Jens Hainmueller, Dominik Hangartner, and Teppei Yamamoto. 2015. “Validating vignette and conjoint survey experiments against real-world behavior.” Proceedings of the National Academy of Sciences of the United States of America, 112(8), 2395–2400.\n(外的妥当性について2) Brandon de la Cuesta, Naoki Egami, and Kosuke Imai. 2021. “Improving the External Validity of Conjoint Analysis: The Essential Role of Profile Distribution.” Political Analysis.\nErik Knudsen and Mikael Poul Johannesson. 2019. “Beyond the Limits of Survey Experiments: How Conjoint Designs Advance Causal Inference in Political Communication Research.” Political Communication. 36(2): 259-271"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#matching",
    "href": "notes/toolbox/ci_materials.html#matching",
    "title": "因果推論の道具箱",
    "section": "Matching",
    "text": "Matching"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#propensity-score",
    "href": "notes/toolbox/ci_materials.html#propensity-score",
    "title": "因果推論の道具箱",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nGary King and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis. 27(4): 435-454.\nEdward H. Kennedy. 2017. “Nonparametric Causal Effects Based on Incremental Propensity Score Interventions.” Journal of the American Statistical Association. 114 (526): 645-656.\n芝孝一郎.「傾向（プロペンシティ）スコアの各使用法の仮定・解釈の違いを比較してみた」"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#difference-in-difference",
    "href": "notes/toolbox/ci_materials.html#difference-in-difference",
    "title": "因果推論の道具箱",
    "section": "Difference in Difference",
    "text": "Difference in Difference\n\nDifference-in-Difference\nCoady Wing, Kosali Simon, and Ricardo A. Bello-Gomez. 2018. “Designing Difference in Difference Studies: Best Practices for Public Health Policy Research.” Annual Review of Public Health. 39:453-469.\n(Diff-in-DiffとSCM) Bongho Lee. “Difference in Difference”\nAndrew Goodman-Bacon. 2021. “Difference-in-differences with variation in treatment timing,” Journal of Econometrics\nAsjad Naqvi. DiD literature."
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#synthetic-control-method",
    "href": "notes/toolbox/ci_materials.html#synthetic-control-method",
    "title": "因果推論の道具箱",
    "section": "Synthetic Control Method",
    "text": "Synthetic Control Method\n\n前田豊・鎌田 拓馬. 2019.「Synthetic Control Methodを用いた個別事例の因果効果の識別」『理論と方法』34 (1): 78-96"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#regression-discontinuity-design",
    "href": "notes/toolbox/ci_materials.html#regression-discontinuity-design",
    "title": "因果推論の道具箱",
    "section": "Regression Discontinuity Design",
    "text": "Regression Discontinuity Design\n\nTakuya Ishihara and Masayuki Sawada. 2021. “Manipulation-Robust Regression Discontinuity Designs.” arXiv:2009.07551\nJustin McCrary. 2008. “Manipulation of the running variable in the regression discontinuity design: A density test.” Journal of Econometrics. 142 (2): 698-714.\nAndrew Gelman and Guido Imbens. 2019. “Why High-Order Polynomials Should Not Be Used in Regression Discontinuity Designs.” Journal of Business & Economic Statistics. 37(3): 447-456.\nMarinho Bertanha and Guido W. Imbens. 2019. “External Validity in Fuzzy Regression Discontinuity Designs.” Journal of Business & Economic Statistics. 38(3): 593-612\nErin Hartman. 2020. “Equivalence Testing for Regression Discontinuity Designs.” Political Analysis.\nMatias D. Cattaneo, Nicolás Idrobo, and Rocío Titiunik. 2020. A Practical Introduction to Regression Discontinuity Designs: Foundations. Cambridge University Press. [arXiv]\nMetin Bulus. 2021. “Minimum Detectable Effect Size Computations for Cluster-Level Regression Discontinuity Studies: Specifications Beyond the Linear Functional Form,” Journal of Research on Educational Effectiveness.\nYoichi Arai, Taisuke Otsu, Myung Hwan Seo. 2021. “Regression Discontinuity Design with Potentially Many Covariates,” arXiv:2109.08351"
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#instrumental-variable",
    "href": "notes/toolbox/ci_materials.html#instrumental-variable",
    "title": "因果推論の道具箱",
    "section": "Instrumental Variable",
    "text": "Instrumental Variable\n\nJoshua D. Angrist and Alan B. Krueger. 2001. “Instrumental Variables and the Search for Identification: From Supply and Demand to Natural Experiments,” Journal of Economic Perspectives, 15 (4): 69-85.\nAllison J. Sovey and Donald P. Green. 2010. “Instrumental Variables Estimation in Political Science: A Readers’ Guide,” American Journal of Political Science, 55(1): 188-200.\nKenneth A. Bollen. 2012. “Instrumental Variables in Sociology and the Social Sciences,” Annual Review of Sociology, 38:37–72.\nPeter M. Aronow and Allison Carnegie. 2013. “Beyond LATE: Estimation of the Average Treatment Effect with an Instrumental Variable,” Political Analysis, 21 (4): 492–506."
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#balance-check",
    "href": "notes/toolbox/ci_materials.html#balance-check",
    "title": "因果推論の道具箱",
    "section": "Balance Check",
    "text": "Balance Check\n\nStefan Tübbicke. “Entropy Balancing for Continuous Treatments.” arXiv:2001.06281\n(分散比について) Peter C. Austin. 2009. “Using the Standardized Difference to Compare the Prevalence of a Binary Variable Between Two Groups in Observational Research.” Communications in Statistics-Simulation and Computation. 38 (6): 1228-1234.\n(標準化差分について) Svetlana V. Belitser, Edwin P. Martens, Wiebe R. Pestman, Rolf H.H. Groenwold, Anthonius de Boer, and Olaf H. Klungel. 2011. “Measuring Balance and Model Selection in Propensity Score Methods.” Pharmacoepidemiology and Drug Safety, 20(11): 1115–29.\n(標準化差分について) M. Sanni Ali, Rolf H. H. Groenwold, Wiebe R. Pestman, Svetlana V. Belitser, Kit C. B. Roes, Arno W. Hoes, Anthonius de Boer, and Olaf H. Klungel. 2014. “Propensity Score Balance Measures in Pharmacoepidemiology: A Simulation Study.” Pharmacoepidemiology and Drug Safety, 23(8): 802–11\n(標準化差分について) Stuart, Elizabeth A., Brian K. Lee, and Finbarr P. Leacy. 2013. “Prognostic Score-Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.” Journal of Clinical Epidemiology, 66(8):S84.\n(経験CDF統計量(eCDF)について) Peter C. Austin, and Elizabeth A. Stuart. 2015. “Moving Towards Best Practice When Using Inverse Probability of Treatment Weighting (IPTW) Using the Propensity Score to Estimate Causal Treatment Effects in Observational Studies.” Statistics in Medicine, 34(28):3661–3679."
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#econometrics-and-data-analysis",
    "href": "notes/toolbox/ci_materials.html#econometrics-and-data-analysis",
    "title": "因果推論の道具箱",
    "section": "Econometrics and Data Analysis",
    "text": "Econometrics and Data Analysis\n\nBruce E. Hansen. Econometrics. Web\nOpenIntro Statistics\n\n日本語版あり\n\nshinmee. 『俺の計量経済学』\nRachel Rosenberg et al.. Computational Applications to Behavioral Science\nKohei Kawaguchi. Applied Econometrics Slides."
  },
  {
    "objectID": "notes/toolbox/ci_materials.html#dataset",
    "href": "notes/toolbox/ci_materials.html#dataset",
    "title": "因果推論の道具箱",
    "section": "Dataset",
    "text": "Dataset\n\nHidenori Takahash. Data Source"
  },
  {
    "objectID": "notes/toolbox/toolbox.html",
    "href": "notes/toolbox/toolbox.html",
    "title": "私の道具箱",
    "section": "",
    "text": "Notion\n\nメモ用で使っています。メモを体系的に管理する時は便利です。検索機能もかなり強力です。Markdown文法が使えます。\n\nSublime Text\n\nSublime Textは無料でも、有料でもないシェアウェアです。無料でも問題なく使えますが、たまに「買ってください！」というメッセージが出ます。Sublime Textはそのままメモ帳の代わりとしても使いますが、私が　Sublime Textを使うのは下で紹介するLaTeXで文書&スライドを作成する時です。プログラミングに最適化されたエディターって感じです。LaTeXToolsパッケージをインストールすればコンパイルはむろん、Snippetまで使えます。\n\nVim\n\n最近はほぼ使いませんが、一時期(20年程度前です)はVi(Vim)を使っていました。ショートカットを覚えるのがちょっと面倒くさいですが、慣れたらコード作成のスピードが数倍はアップします。(折衷案はSublime Textにvintageパッケージですかね)\n\nMicrosoft Word & PowerPoint\n\n私が主に使うのは下のLaTeXですが、日本の政治学ではLaTeXの文書を受けてくれるところが少ないので、その場合はMicrosoft WordやPowerPointを使っています。\n\nLaTeX\n\n多くの場合、文書やスライドはLaTeXで作成します。LaTeXの良さはここ[Link]を見てください。レポート、論文、本だけでなくBeamerというテンプレートを使えば簡単にスライドも作成できます。\n文書の見栄は気にせず内容だけに集中できます。\n最近はOverleaf、Cloud LaTexのようなクラウド型LaTeXエディターもあり、ネットさえ繋がっていればどこでも執筆が可能です。また、複数人で同時編集も可能です。\n\nR Markdown / Quarto\n\n最近はR Markdownで文書作成、Web-book執筆、ホームページ作成、スライド作成をしています。\n開発途上のQuartoはR Markdownと非常に似た文法を持っています。いずれかは標準、または標準に近いものになるかも知れません。"
  },
  {
    "objectID": "notes/toolbox/toolbox.html#統計",
    "href": "notes/toolbox/toolbox.html#統計",
    "title": "私の道具箱",
    "section": "統計",
    "text": "統計\n\nMicrosoft Excel\n\n使っていません。*.xls, *.xlsxファイル全て後で紹介するLibreOffice Calcで閲覧・編集します。Microsoft Excelを使うのは「高度」なマクロが含まれているファイルの場合のみです。\n基本的にMicrosoft Officeは商用ソフトです。企業間ならともかく、個人間でxlsやxlsxなどでやり取りするのは、購買の強要でしょうね。csvがいいと思いますが、普通のxlsやxlsxファイルならLibreOffice Calcで十分です。\n\nLibreOffice Calc\n\n統計用ソフトではありませんが、Microsoft Excelの代わりに使っています。無料です。Windows環境でMicrosoft Officeは重く感じないかも知れませんが、macOS(OS X)環境でMicrosoft Officeはかなり重いです。それに比べてLibreOfficeは*NIX系で最適化されており、Microsoft Excelよりも軽いと感じられます。なによりLibreOffice Clacのいいところは「文字化けしない」という点です。ほとんどのエンコーディングに対応しているため、どのファイルを開けても文字化けしません。\n\nSPSS\n\n学部3年生の時に使った私の初めての統計分析ソフトウェアです。\n長所: SYNTAXを使って分析することも出来ますが、マウス操作だけで分析できるので、学部生向けの授業ではよく使われますね。\n短所: JAVA仮想マシン上で動くので、かなり重いです。また、商用ソフトなので個人的に使うためにはかなり高額を払う必要があります。\n\nStata\n\n修士時代に使ったソフトで今もよく使っています。\n長所\n\nSPSSに比べたら早い\n値段も買えなくはない程度です。\nmarginsなどの事後推定のコマンドが充実していて、予測値や限界効果を手軽に出せます(私が未だStataを時々使う理由です)。\n基本的にはコマンド基盤ですが、マウスだけでも操作できます。\n無料のアドオンも充実しています。(SPost13はオススメです)\n\n短所\n\nSPSSと同じ短所ですが、一つのデータセットしか扱えない\n相対的に安いが、数万円はする\n\n\nR\n\n統計分析ソフトとは言ってますが、ほぼプログラミング言語(OOP)です。\n博士課程に進学してから使い始めました。入門はSPSSでいいと思いますが、次のステップはStataよりRの方がおすすめです。\n長所\n\n完全に無料です\nSPSS, Stataよりも早いです。(C, FORTRAN, Python, Juliaなどに比べたら遅いですが…)\nRstudio, JupyterなどのIDEを使えば、かなり快適に作業できます。\nパッケージが豊富で、存在する分析手法はほとんどRでパッケージ化されています。\n同時に複数のデータを扱えますし、データの操作も強力です。(dplyr, tidyr, data.tableパッケージなど)\n綺麗な図を自由自在に描けます。\n\n短所\n\n慣れるまで時間がかかります。\n\nRstan\n\nベイズ統計学が必ずしもMCMCを必要とするわけではありませんが、やはりほとんどの場合、事後分布はMCMCサンプリングする時が多いです。Rstanは最近流行りのMCMCサンプラーであるstanのRパッケージです。他にはJAGSなどがあるようですが、使ったことがありません。離散変数の扱いが難しいという点を除けば、良いです。最近はstanの良書も出ています。\n\n\nその他\n\nPython\n\n勉強中です。統計用の言語でなく、汎用言語ですが、データサイエンス業界ではRと共にメージャーな言語です。\n\nJulia\n\nRのような統計用の言語ですが、高速です\n\nHAD\n\nExcelで作られた統計パッケージです\n\nNetLogo\n\nMulti-Agent Simulationに特化したLogo言語基盤のソフトです。\n\nMplus\n\n構造方程式モデリングでは最強のソフトです。むろん、最近、Rのlavaanパッケージが相当良くなりましたが…ちなみに、macOSだとパス図は出力されません。\nRからMplusを利用してモデルを推定してくれるMplusAutomationというパッケージもあります。\n\nSAS, Matlab, Mathematica, Maple (こんなもんもあるようです)"
  },
  {
    "objectID": "notes/stats/asa_p_value.html",
    "href": "notes/stats/asa_p_value.html",
    "title": "The ASA’s statement on p-values [日本語まとめ]",
    "section": "",
    "text": "ASA Statement on Statistical Significance and P-values Full-text"
  },
  {
    "objectID": "notes/stats/asa_p_value.html#p値とは何かwhat-is-a-p-value",
    "href": "notes/stats/asa_p_value.html#p値とは何かwhat-is-a-p-value",
    "title": "The ASA’s statement on p-values [日本語まとめ]",
    "section": "p値とは何か(What is a p-value?)",
    "text": "p値とは何か(What is a p-value?)\n\nちょっと適当な回答⇒ p値とは特定の統計モデル下で、「データの統計値が観測された値以上に極端的な値が得られる」確率である。\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (for example, the sample mean difference between two compared groups) would be equal to or more extreme than its observed value."
  },
  {
    "objectID": "notes/stats/asa_p_value.html#つの原則",
    "href": "notes/stats/asa_p_value.html#つの原則",
    "title": "The ASA’s statement on p-values [日本語まとめ]",
    "section": "6つの原則",
    "text": "6つの原則\n\np値はデータと特定の統計モデルがどれほど不一致するか示すことができる。\n\nP-values can indicate how incompatible the data are with a specified statistical model.\np値は特定のデータセットとそのデータについて提案された統計モデルがどれくらい不一致するかを要約できる一つのアプローチである。もっともよくあり得る場面はとく手の仮定から成り立った統計モデルと帰無仮説が存在するケースである。帰無仮説は「二群に差がない」あるいは「ある要因とある結果には関係がない」といった「効果の不在」を仮定する。p値を計算する時に用いられた仮定が真だという前提の下でp値が小さけいれば小さいほどデータと帰無仮説の間の統計的不一致性が大きくなる。このような不一致は「帰無仮説」あるいは「前提された仮定」が真ではないと疑える証拠として解釈できる。\n\np値は仮説が真である確率、あるいはデータが偶然によって生成された確率ではない。\n\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n研究者はp値を「帰無仮説か真か否か」あるいは「観察されたデータが偶然によって生成されたか」に関する確率として使いたがる。しかし、p値がどっちらも関係ない。これは特定の仮説的説明に関連するデータに関する内容であり、その説明自体に関する内容ではない。\n\n科学的な結論、ビジネス、政策的な決定はp値がある閾値を超えたか否かだけで判断してはいけない。\n\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n科学的主張や結果を正当化するために、データ分析や科学的推論を機械的に、あるいは恣意的に縮小させる慣行(たとえばp<0.05ルール)は誤った信念や意思決定へ繋がるおそれがある。ある結論がどこかでは自動的に「真」となる、一方では自動的に「偽」になるわけではない。研究者たちは研究の設計、測定手法の室、研究対象となる現象に対する外部の証拠、そしてデータ分析の前提と成る仮定の妥当性などの文脈的要素を科学的な結論でょう出に積極的に使うべきである。\n時には、現実の意思決定は「はいーいいえ」のような二分的意思決定を要求するが、これはp値が単独で「ある決定が正しい」とか「間違っている」ことを保障できることを意味しない。一般的に使われているような、科学的発見に関する主張を保証する手段として「統計的有意性(一般的にp<=0.05)」を用いる慣習は科学的プロセスの深刻な歪曲をもたらしうる。\n\n適切な推論は完全な報告と透明性を必要とする。\n\nProper inference requires full reporting and transparency\np値とこれに関連した分析は選択的に報告されてはいけない。データに対して複数の分析を行い、特定のp値(ある閾値を超えたもの)とそれに関係するものだけを報告するのは、報告されたp値を事実上解釈不可能にする。「データ浚渫(data dredging)」、「有意性志向(significance chasing)」、「有意性探索(significance questing)」、「選択的推論(selective inference)」、「p-hacking」という名で知られている、有望な発見を取捨選択する行為は公刊された文献から統計的に有意な結果が誤って氾濫する結果をもたらすため、積極的に止揚すべきである。\nただし、複数の統計的検定を行うのがこのような問題をもたらすわけではない。研究者が統計的結果に基づき、何を報告するか選択すれば、読者はその決定と根拠を知らないかぎり、結果を正しく解釈することは著しく制限される。研究者たちは研究の間に存在した仮説の数、データ収集に関する全ての意思決定、行われた全ての統計的検定、そして得られた全てのp値を明かすべきである。どれだけ多くの、またどのような分析が用いられたか、そしてそのような分析がどのように選択され、報告されたか(p値を含めて)を知らないかぎり、p値とこれに関連した統計量に基づく正しい科学的決論を導出することは不可能である。\n\np値(or 統計的有意性)は効果の大きさや結果の重要性を測定する尺度ではない。\n\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n統計的有意性と科学的、人間的、経済的重要性は一致しない。より小さいp値がより大きい、あるいはより重要な効果の存在を意味するのではなく、大きなp値が重要性や効果の不在を意味するわけでもない。どのような効果もサンプル(標本)サイズが大きかったり、測定手法が十分に精密したら大きいp値を出すことはできる。反対にどれだけ大きい効果でもサンプルサイズが小さかったり、測定手法が精密でなかったら小さくないp値が得られる。同様に、同じ推定値も測定の精密度が異なると、異なるp値が得られる。\n\np値自体は特定のモデルや仮説に関する良い尺度を提供しない。\n\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n研究者たちは文脈や他の証拠がない状況におけるp値は制限された情報のみを提供するという点を認識すべきである。たとえば、0.05に近いp値はこれ自体では帰無仮説を反証する弱い証拠のみを示すだけである。同様に、比較的に大きいp値が帰無仮説を支持する証拠を暗示するわけでもない。観察されたデータと同等に、より適合した他の仮説はあり得る。したがって、他のアプローチが可能であれば、データ分析をp値の計算で留めてはいけない。"
  },
  {
    "objectID": "notes/stats/asa_p_value.html#他のアプローチ",
    "href": "notes/stats/asa_p_value.html#他のアプローチ",
    "title": "The ASA’s statement on p-values [日本語まとめ]",
    "section": "他のアプローチ",
    "text": "他のアプローチ\nこれまでのp値に対する誤解と誤用の実態を鑑み、ある統計学者たちはp値を他のアプローチから補完するか、代替することを好んでいる。このような代案は\n\n検証において推定を強調する方法：信頼・信用・予測区間の利用\nベイズ統計学\n代替的な尺度：尤度比、ベイズファクター\n決定理論モデリング\n偽発見率(false discovery rate; FDR)\n\nなどがある。このような尺度とアプローチはより多くの仮定に依存するが、これらは効果量の大きさ、あるいは仮説の真・偽のように概念により直接的にアプローチできる長所がある。"
  },
  {
    "objectID": "notes/stats/asa_p_value.html#結論",
    "href": "notes/stats/asa_p_value.html#結論",
    "title": "The ASA’s statement on p-values [日本語まとめ]",
    "section": "結論",
    "text": "結論\n望ましい科学的実践(practice)における必須要素として、望ましい統計的実践は\n\n正しい研究設計と遂行\n多様な数値的、視覚的なデータ要約\n研究対象となる現象に対する理解\n文脈に基いて結果解釈\nデータ要約が何を意味するかに対する完全な報告\n適切かつ論理的な定量的理解\n\nなどの原則を必要とする。いかなる単一指標も科学的論証を代替することはできない。"
  },
  {
    "objectID": "notes/stats/logit_classification.html",
    "href": "notes/stats/logit_classification.html",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "",
    "text": "ロジスティック回帰分析は応答変数が0か1のように二値変数の場合に使う分析手法です。「投票に参加するか否か」などの場合にも使えますが、「AかBか」のような分類の場面でも使える手法です。ここでは説明変数が2つ(\\(x_1\\)と\\(x_2\\))の場合を考えてみましょう。\n最も単純なロジスティック回帰分析の場合、説明変数と応答変数の関係は以下のように表現できます。\n\\[\n\\begin{aligned}\n    Y_i \\sim & \\text{Bernoulli}(\\theta_i),\\\\\n    \\theta = & \\frac{\\exp(y_i^*)}{1 + \\exp(y_i^*)},\\\\\n    y_i^* = & \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2.\n\\end{aligned}\n\\]\nまず、3行目を見ると線形回帰分析のようにy_i^*がx1とx2の線形結合で表現できるとします。これを逆ロジット関数 (inverse logit function)の引数としたものを\\(\\theta_i\\)とします。これは必ず0から1の間の値をとり、これが\\(Y_i\\)が1となる確率になります。最後に\\(Y_i\\)は\\(\\theta_i\\)をパラメーターとするベルヌイ分布から生成されるとします。\n以下の例では\\(\\beta_0 = 1\\)、\\(\\beta_1 = 0.5\\)、\\(\\beta_2 = -0.8\\)にします。"
  },
  {
    "objectID": "notes/stats/logit_classification.html#仮想データの作成",
    "href": "notes/stats/logit_classification.html#仮想データの作成",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "仮想データの作成",
    "text": "仮想データの作成\nまずは実習に使う仮想データを作ってみましょう。\n\nset.seed(19861008)       # 再現性確保のためにseedを設定\nx1 <- runif(300, -5, 5)  # x1は(-5, 5)の一様分布から300個抽出\nx2 <- rnorm(300,  2, 2)  # x2は平均2、標準偏差2の正規分布から300個\n\n# 線形予測子 (linear predictor)\nlin_pred <- 1 + 0.5 * x1 - 0.8 * x2\n# 線形予測子を逆ロジット関数に投入しthetaを計算\ntheta    <- exp(lin_pred) / (1 + exp(lin_pred))\n# 応答変数yはthetaをパラメーターとするベルヌイ分布です\n# ただし、Rにはベルヌーイ分布の乱数生成関数がないため、二項分布で\ny        <- rbinom(300, 1, theta)\n\n# データフレーム化しましょう\ndf <- data.frame(y     = y, \n                 x1    = x1, \n                 x2    = x2,\n                 theta = theta)"
  },
  {
    "objectID": "notes/stats/logit_classification.html#まずは変数間の関係から",
    "href": "notes/stats/logit_classification.html#まずは変数間の関係から",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "まずは変数間の関係から",
    "text": "まずは変数間の関係から\nまず見るのは変数の記述統計ですね。\n\nsummary(df)\n\n       y              x1                x2              theta          \n Min.   :0.00   Min.   :-4.9151   Min.   :-4.4724   Min.   :0.0002794  \n 1st Qu.:0.00   1st Qu.:-2.8506   1st Qu.: 0.6825   1st Qu.:0.1095960  \n Median :0.00   Median :-0.4246   Median : 1.8988   Median :0.3289306  \n Mean   :0.43   Mean   :-0.1772   Mean   : 1.9455   Mean   :0.4094721  \n 3rd Qu.:1.00   3rd Qu.: 2.3687   3rd Qu.: 3.2758   3rd Qu.:0.6994029  \n Max.   :1.00   Max.   : 4.9492   Max.   :10.6261   Max.   :0.9967745  \n\n\n続いて、ちゃんと。プロットのx軸に、y軸にyをプロットします。、左下と右上に点が多く集まっているはずです。\n\n# 点が多いのでポイントを半透明にし、上下に若干のバラツキを与えます。\n# バラツキは平均0、標準偏差0.01の正規分布に従います。\nplot(x = df$theta, y = df$y + rnorm(300, 0, 0.01), \n     pch = 16, col = rgb(0, 0, 0, 0.5),\n     ylim = c(0, 1),\n     xlab = expression(theta), ylab = \"y\")\n\n\n\n\n\n\n\n\n問題ないようですね。次に進みましょう。\nつぎは、説明変数と応答変数間の関係をみます。しかし、x軸をx1、y軸をx2にすると、応答変数をどう表現するか悩みますね。ここでは色に分けてみましょう。ピンクは\\(y = 0\\)、青は\\(y = 1\\)にしてみます。\n\nplot(x = df$x1, y = df$x2, pch = 21, cex = 1.25,\n     bg = c(rgb(1, 0, 0, 0.5), \n            rgb(0, 0, 1, 0.5))[df$y + 1],\n     col = \"black\",\n     xlab = \"x1\", ylab = \"x2\")\nlegend(\"topright\", pch = 21, \n       pt.bg = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5)),\n       col = \"black\", legend = c(\"y = 0\", \"y = 1\"))"
  },
  {
    "objectID": "notes/stats/logit_classification.html#ロジスティック回帰分析の実行",
    "href": "notes/stats/logit_classification.html#ロジスティック回帰分析の実行",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "ロジスティック回帰分析の実行",
    "text": "ロジスティック回帰分析の実行\nロジスティックの回帰分析はglm()関数を使います。分析結果はsummary()関数で確認してみましょう。Markdownならstargazerパッケージを使ってもっと綺麗にできますが、可能な限り、パッケージは使わずにやってみましょう。\n\nfit1 <- glm(y ~ x1 + x2, data = df, family = binomial(link = \"logit\"))\nsummary(fit1)\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial(link = \"logit\"), \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3682  -0.6562  -0.2109   0.6228   2.6755  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.12245    0.24744   4.536 5.72e-06 ***\nx1           0.56304    0.07074   7.959 1.74e-15 ***\nx2          -0.78646    0.11116  -7.075 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 409.99  on 299  degrees of freedom\nResidual deviance: 247.00  on 297  degrees of freedom\nAIC: 253\n\nNumber of Fisher Scoring iterations: 5\n\n\n結果を見ると、\\(\\beta_0, \\beta_1, \\beta_2\\)はそれぞれ1.122、0.563、-0.786であり、最初に設定した1, 0.5, -0.8にかなり近い値が得られましたね。当たり前ですが、\\(y\\)が確率過程で生成された値である以上、完全に一致することはないでしょう。ただし、サンプルサイズを増やせば増やすほど、真の値には近づきます。\n続いて、推定された結果を用いて、各説明変数と応答変数間の関係を見てみましょう。そのためにはx軸を説明変数、y軸を\\(y = 1\\)の確率 (つまり、\\(\\theta\\)です)を線グラフで表現し、参考のために実測値 (\\(y\\))を表示させます。ちなみに、x軸に表示させない変数は平均値に固定します。\nまずは\\(x_1\\)と\\(\\textrm{Pr}(y = 0)\\)の関係から確認しましょう。\n\nx1_u <- seq(min(df$x1), max(df$x1), length.out = 100)\npred_x1 <- predict(fit1, newdata = data.frame(x1 = x1_u, \n                                              x2 = mean(df$x2)),\n                   type = \"response\")\n\nplot(x = x1_u, y = pred_x1, type = \"l\",\n     col = \"red\", lwd = 2, lty = 1,\n     ylim = c(-0.05, 1.05), xlab = \"x1\", ylab = expression(theta))\npoints(x = df$x1, y = df$y + rnorm(300, 0, 0.01), \n       pch = 21, bg = rgb(0, 0, 0, 0.5), col = \"black\")\n\n\n\n\n\n\n\n\n同じことをx2に対してもやってみます。\n\nx2_u <- seq(min(df$x2), max(df$x2), length.out = 100)\npred_x2 <- predict(fit1, newdata = data.frame(x1 = mean(df$x1), \n                                              x2 = x2_u),\n                   type = \"response\")\n\nplot(x = x2_u, y = pred_x2, type = \"l\",\n     col = \"red\", lwd = 2, lty = 1,\n     ylim = c(-0.05, 1.05), xlab = \"x2\", ylab = expression(theta))\npoints(x = df$x2, y = df$y + rnorm(300, 0, 0.01), \n       pch = 21, bg = rgb(0, 0, 0, 0.5), col = \"black\")\n\n\n\n\n\n\n\n\nしかし、可能なら一つのプロットに表現したいですね。三次元プロットを作成する方法もありますが、逆に分かりづらい可能性があります。ここでは、等高線図 (contour plot)を作成してみましょう。そのためには以下のようなデータフレームが必要です。\nここで使える関数がouter()関数です。基本的にouter()関数は2つの引数が必要です。動き方を直接見た方が良いかも知れませんね。\n\nouter(X = 2:4, Y = 1:5)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    4    6    8   10\n[2,]    3    6    9   12   15\n[3,]    4    8   12   16   20\n\n\nこのように3行$$5列のマトリックスが出来ました。1行目には1から5が、1列目には2から4が入ってますね。他はどうでしょう。それは、各行と列の掛け算した値が配置されます。3番目のXは4で、5番目のYは5ですよね。この場合、3行5列のマスには4 \\(\\times\\) 5 = 20が入ります。もし、掛け算ではなく足し算にしたいなら、FUN = \"+\"の引数を追加します。\n\nouter(X = 2:4, Y = 1:5, FUN = \"+\")\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    3    4    5    6    7\n[2,]    4    5    6    7    8\n[3,]    5    6    7    8    9\n\n\n3行5列のマスには4 + 5 = 9が入ります。このFUN引数は関数を入れることも可能です。つまり、各行と列を\\(x_1\\)、\\(x_2\\)が取りうる値とし、この2つの数値を使って\\(\\theta\\)を計算することも可能ということです。もし、\\(x_1 = 1, x_2 = -2\\)の場合の\\(\\theta\\)を計算するなら\n\npredict(fit1, newdata = data.frame(x1 = 1, x2 = -2), type = \"response\")\n\n        1 \n0.9629743 \n\n\nここのx1 = 1, x2 = -2の部分を引数とする関数を作ってみましょう。\n\npred_FUNC <- function(x1, x2) {\n    predict(fit1, newdata = data.frame(x1 = x1, x2 = x2), \n            type = \"response\")\n}\n\nこれだけです。それでは、outer()関数を使ってみましょう。\\(x_1\\)と\\(x_2\\)は最小値から最大値までの100分割とします。そうですね、既にx1_uとx2_uとして作成済みです。これを活用します。\n\nPred_mat <- outer(x1_u, x2_u, pred_FUNC)\n\nhead(Pred_mat)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.8667431 0.8522704 0.8365222 0.8194510 0.8010211 0.7812122 0.7600221\n[2,] 0.8730905 0.8591951 0.8440501 0.8276030 0.8098121 0.7906496 0.7701046\n[3,] 0.8791777 0.8658464 0.8512930 0.8354608 0.8183027 0.7997843 0.7798861\n[4,] 0.8850113 0.8722302 0.8582559 0.8430283 0.8264956 0.8086168 0.7893653\n[5,] 0.8905984 0.8783529 0.8649446 0.8503102 0.8343937 0.8171488 0.7985417\n[6,] 0.8959459 0.8842211 0.8713649 0.8573115 0.8420011 0.8253826 0.8074159\n          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]\n[1,] 0.7374694 0.7135959 0.6884686 0.6621803 0.6348503 0.6066227 0.5776647\n[2,] 0.7481855 0.7249230 0.7003721 0.6746136 0.6477543 0.6199271 0.5912886\n[3,] 0.7586073 0.7359678 0.7120111 0.6868057 0.6604463 0.6330537 0.6047738\n[4,] 0.7687311 0.7467241 0.7233765 0.6987449 0.6729119 0.6459859 0.6181013\n[5,] 0.7785542 0.7571867 0.7344606 0.7104210 0.6851380 0.6587080 0.6312535\n[6,] 0.7880752 0.7673518 0.7452571 0.7218246 0.6971128 0.6712056 0.6442134\n         [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21]\n[1,] 0.5481628 0.5183184 0.4883428 0.4584507 0.4288548 0.3997591 0.3713540\n[2,] 0.5620168 0.5323068 0.5023659 0.4724080 0.4426475 0.4132932 0.3845425\n[3,] 0.5757748 0.5462446 0.5163853 0.4864085 0.4565293 0.4269596 0.3979029\n[4,] 0.5894164 0.5601102 0.5303789 0.5004304 0.4704789 0.4407384 0.4114172\n[5,] 0.6029218 0.5738827 0.5443250 0.5144516 0.4844747 0.4546091 0.4250665\n[6,] 0.6162722 0.5875415 0.5582018 0.5284502 0.4984950 0.4685506 0.4388310\n         [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]\n[1,] 0.3438111 0.3172800 0.2918855 0.2677265 0.2448757 0.2233802 0.2032637\n[2,] 0.3565762 0.3295548 0.3036148 0.2788673 0.2553972 0.2332635 0.2125005\n[3,] 0.3695484 0.3420666 0.3156054 0.2902879 0.2662115 0.2434469 0.2220400\n[4,] 0.3827119 0.3548020 0.3278465 0.3019804 0.2773131 0.2539277 0.2318817\n[5,] 0.3960496 0.3677465 0.3403264 0.3139355 0.2886955 0.2647019 0.2420239\n[6,] 0.4095438 0.3808846 0.3530317 0.3261428 0.3003510 0.2757642 0.2524639\n         [,29]     [,30]     [,31]     [,32]     [,33]     [,34]      [,35]\n[1,] 0.1845284 0.1671576 0.1511191 0.1363674 0.1228473 0.1104962 0.09924639\n[2,] 0.1931201 0.1751143 0.1584575 0.1431103 0.1290216 0.1161319 0.10437560\n[3,] 0.2020128 0.1833663 0.1660826 0.1501286 0.1354582 0.1220155 0.10973760\n[4,] 0.2112078 0.1919166 0.1739988 0.1574279 0.1421635 0.1281541 0.11533958\n[5,] 0.2207056 0.2007677 0.1822098 0.1650131 0.1491435 0.1345541 0.12118860\n[6,] 0.2305056 0.2099209 0.1907189 0.1728889 0.1564037 0.1412220 0.12729156\n          [,36]      [,37]      [,38]      [,39]      [,40]      [,41]\n[1,] 0.08902729 0.07976724 0.07139487 0.06384030 0.05703601 0.05091750\n[2,] 0.09368329 0.08398359 0.07520484 0.06727633 0.06012934 0.05369789\n[3,] 0.09855644 0.08840141 0.07920079 0.07088330 0.06337917 0.05662105\n[4,] 0.10365409 0.09302801 0.08338992 0.07466817 0.06679215 0.05969331\n[5,] 0.10898353 0.09787076 0.08777950 0.07863803 0.07037512 0.06292114\n[6,] 0.11455197 0.10293700 0.09237686 0.08280007 0.07413502 0.06631121\n          [,42]      [,43]      [,44]      [,45]      [,46]      [,47]\n[1,] 0.04542373 0.04049742 0.03608518 0.03213756 0.02860899 0.02545764\n[2,] 0.04791928 0.04273444 0.03808815 0.03392912 0.03020998 0.02688716\n[3,] 0.05054467 0.04508922 0.04019766 0.03581686 0.03189763 0.02839462\n[4,] 0.05330585 0.04756730 0.04241885 0.03780552 0.03367628 0.02998399\n[5,] 0.05620894 0.05017443 0.04475705 0.03990002 0.03555046 0.03165942\n[6,] 0.05926023 0.05291651 0.04721779 0.04210549 0.03752490 0.03342525\n          [,48]      [,49]      [,50]      [,51]      [,52]      [,53]\n[1,] 0.02264533 0.02013728 0.01790191 0.01591066 0.01413771 0.01255980\n[2,] 0.02392081 0.02127457 0.01891540 0.01681334 0.01494133 0.01327493\n[3,] 0.02526627 0.02247462 0.01998508 0.01776631 0.01578990 0.01403021\n[4,] 0.02668534 0.02374071 0.02111396 0.01877226 0.01668584 0.01482781\n[5,] 0.02818181 0.02507631 0.02230516 0.01983402 0.01763171 0.01567003\n[6,] 0.02975963 0.02648500 0.02356194 0.02095455 0.01863018 0.01655929\n          [,54]       [,55]       [,56]       [,57]       [,58]       [,59]\n[1,] 0.01115600 0.009907536 0.008797542 0.007810925 0.006934180 0.006155236\n[2,] 0.01179216 0.010473258 0.009300478 0.008257929 0.007331380 0.006508109\n[3,] 0.01246414 0.011070921 0.009831881 0.008730288 0.007751154 0.006881072\n[4,] 0.01317390 0.011702287 0.010393328 0.009229415 0.008194765 0.007275252\n[5,] 0.01392351 0.012369209 0.010986480 0.009756797 0.008663543 0.007691837\n[6,] 0.01471514 0.013073636 0.011613087 0.010314001 0.009158890 0.008132081\n           [,60]       [,61]       [,62]       [,63]       [,64]       [,65]\n[1,] 0.005463312 0.004848790 0.004303091 0.003818571 0.003388421 0.003006580\n[2,] 0.005776749 0.005127152 0.004550268 0.004038030 0.003583248 0.003179522\n[3,] 0.006108057 0.005421408 0.004811576 0.004270047 0.003789234 0.003362378\n[4,] 0.006458243 0.005732454 0.005087813 0.004515335 0.004007013 0.003555712\n[5,] 0.006828368 0.006061237 0.005379823 0.004774646 0.004237256 0.003760121\n[6,] 0.007219550 0.006408756 0.005688497 0.005048774 0.004480669 0.003976234\n           [,66]       [,67]       [,68]       [,69]       [,70]       [,71]\n[1,] 0.002667654 0.002366843 0.002099881 0.001862975 0.001652751 0.001466215\n[2,] 0.002821155 0.002503079 0.002220785 0.001970265 0.001747955 0.001550691\n[3,] 0.002983463 0.002647135 0.002348633 0.002083721 0.001848634 0.001640026\n[4,] 0.003155079 0.002799459 0.002483823 0.002203696 0.001955100 0.001734499\n[5,] 0.003336533 0.002960522 0.002626774 0.002330563 0.002067685 0.001834403\n[6,] 0.003528387 0.003130823 0.002777930 0.002464715 0.002186738 0.001940051\n           [,72]       [,73]       [,74]        [,75]        [,76]        [,77]\n[1,] 0.001300705 0.001153856 0.001023569 0.0009079807 0.0008054345 0.0007144615\n[2,] 0.001375658 0.001220357 0.001082570 0.0009603249 0.0008518721 0.0007556580\n[3,] 0.001454924 0.001290687 0.001144968 0.0010156837 0.0009009847 0.0007992280\n[4,] 0.001538750 0.001365063 0.001210957 0.0010742302 0.0009529260 0.0008453080\n[5,] 0.001627399 0.001443720 0.001280746 0.0011361477 0.0010078586 0.0008940425\n[6,] 0.001721145 0.001526902 0.001354550 0.0012016297 0.0010659546 0.0009455840\n            [,78]        [,79]        [,80]        [,81]        [,82]\n[1,] 0.0006337573 0.0005621641 0.0004986546 0.0004423167 0.0003923414\n[2,] 0.0006703034 0.0005945842 0.0005274140 0.0004678284 0.0004149718\n[3,] 0.0007089555 0.0006288728 0.0005578312 0.0004948109 0.0004389071\n[4,] 0.0007498347 0.0006651375 0.0005900015 0.0005233487 0.0004642222\n[5,] 0.0007930692 0.0007034919 0.0006240260 0.0005535316 0.0004909967\n[6,] 0.0008387945 0.0007440563 0.0006600113 0.0005854541 0.0005193147\n            [,83]        [,84]        [,85]        [,86]        [,87]\n[1,] 0.0003480106 0.0003086872 0.0002738059 0.0002428652 0.0002154201\n[2,] 0.0003680849 0.0003264940 0.0002896011 0.0002568760 0.0002278480\n[3,] 0.0003893168 0.0003453276 0.0003063073 0.0002716949 0.0002409927\n[4,] 0.0004117729 0.0003652473 0.0003239769 0.0002873684 0.0002548955\n[5,] 0.0004355237 0.0003863156 0.0003426654 0.0003039458 0.0002696001\n[6,] 0.0004606438 0.0004085986 0.0003624316 0.0003214792 0.0002851528\n            [,88]        [,89]        [,90]        [,91]        [,92]\n[1,] 0.0001910759 0.0001694823 0.0001503286 0.0001333393 0.0001182698\n[2,] 0.0002020996 0.0001792604 0.0001590019 0.0001410325 0.0001250936\n[3,] 0.0002137592 0.0001896026 0.0001681755 0.0001491695 0.0001323111\n[4,] 0.0002260912 0.0002005413 0.0001778782 0.0001577759 0.0001399450\n[5,] 0.0002391346 0.0002121110 0.0001881407 0.0001668787 0.0001480193\n[6,] 0.0002529302 0.0002243480 0.0001989951 0.0001765067 0.0001565593\n            [,93]        [,94]        [,95]        [,96]        [,97]\n[1,] 0.0001049032 9.304710e-05 8.253088e-05 7.320312e-05 6.492953e-05\n[2,] 0.0001109559 9.841581e-05 8.729286e-05 7.742694e-05 6.867599e-05\n[3,] 0.0001173578 1.040942e-04 9.232959e-05 8.189445e-05 7.263862e-05\n[4,] 0.0001241290 1.101003e-04 9.765690e-05 8.661972e-05 7.682987e-05\n[5,] 0.0001312909 1.164528e-04 1.032916e-04 9.161761e-05 8.126294e-05\n[6,] 0.0001388659 1.231719e-04 1.092513e-04 9.690384e-05 8.595177e-05\n            [,98]        [,99]       [,100]\n[1,] 5.759098e-05 5.108182e-05 4.530832e-05\n[2,] 6.091404e-05 5.402931e-05 4.792268e-05\n[3,] 6.442882e-05 5.714687e-05 5.068790e-05\n[4,] 6.814640e-05 6.044430e-05 5.361266e-05\n[5,] 7.207847e-05 6.393198e-05 5.670618e-05\n[6,] 7.623740e-05 6.762089e-05 5.997818e-05\n\n\nこれで100$\\(100の行列が作られ、各セルには予測確率 (\\)$)が格納されることになります。これを使って等高線図を作る方法はcontour()関数を使います。\n\nplot(x = x1_u, y = x2_u, type = \"n\", xlab = \"x1\", ylab = \"x2\")\n\n\n\n\n\n\n\n\n先ほどの散布図に等高線を追加したものです。なんか、寂しい気もしますね。予測確率に応じて背景の色がグラデーションで変わるにようにしてみましょう。ここで使うのはimage()関数です。予測確率 (\\(\\theta\\))が高いほど黒、低いほど白になるようにします。\n\nimage(x = x1_u, y = x2_u, z = Pred_mat,\n      col = rgb(seq(1, 0, length.out = 10), \n                seq(1, 0, length.out = 10), \n                seq(1, 0, length.out = 10), 0.75), \n      breaks = seq(0, 1, 0.1), xlab = \"x1\", ylab = \"x2\")\npoints(x = df$x1, y = df$x2, pch = 21, cex = 1.25,\n     bg = c(rgb(1, 0, 0, 0.5), \n            rgb(0, 0, 1, 0.5))[df$y + 1],\n     col = \"black\")\ncontour(x = x1_u, y = x2_u, z = Pred_mat, lwd = 2, col = \"white\", \n        levels = c(0.25, 0.5, 0.75), labcex = 1.5, add = TRUE)\nlegend(\"topright\", pch = 21, \n       pt.bg = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5)),\n       col = \"black\", bty = \"n\", legend = c(\"y = 0\", \"y = 1\"))\n\n\n\n\n\n\n\n\nこれで完成です。色がちょっと気に入らないんですが、これはcolorspaceパッケージなどを使えば、より見やすいプロットが作れると思います。ちなみに、image()関数内のcol = ...引数をcol = cm.colors(10)とかにしてみても良いかも知れませんね（やってみて下さい）。"
  },
  {
    "objectID": "notes/stats/logit_classification.html#いろんな指標で分類性能を確認する",
    "href": "notes/stats/logit_classification.html#いろんな指標で分類性能を確認する",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "いろんな指標で分類性能を確認する",
    "text": "いろんな指標で分類性能を確認する\nこれまでのように可視化を通じて分類器・モデルを評価することは可能ですが、複数のモデルを比較するときには、やはりなんらかの基準が必要です。モデルの評価・比較指標にはいくつかの種類がありますが、ここでは以下の指標を計算してみましょう。\n\n的中率/エラー率 (Accuracy / Error Rate)\n\\(\\kappa\\)統計量\nAkaike/Bayesian Information Criteria (AIC/BIC)\nLeave-One-Out (LOO) Cross Validation\nk-fold Cross Validation\n\n他にもPseudo \\(R^2\\)などがありますが、ここでは省略します。最近流行りのWAICはベイジアン推定後、事後分布のデータが必要なのでここでは難しいですね。\n\n的中率/エラー率\n実際のデータ (\\(y\\))と分類の予測結果 (\\(\\hat{y}\\))を2 \\(\\times\\) 2のテーブルで表すと以下のようになります。ちなみに、以下のような表を混同行列 (confusion matrix)と呼びます。\n\n\n\n\n\\(\\hat{y}=1\\)\n\\(\\hat{y}=0\\)\n\n\n\n\n\\(y=1\\)\nTrue Positive\nFalse Positive\n\n\n\\(y=0\\)\nFalse Positive\nTrue Positive\n\n\n\nここでTPとTNは正しく分類されているケースを意味します。全体の中、TP + TNの比率が的中率になります。この的中率を1から引いたらエラー率になります。実際に計算してみましょう。\n\n# まずは予測確率から\nPred_fit1 <- predict(fit1, type = \"response\")\n# 予測確率が0.5以上なら1、未満なら0にします\nPred_fit1 <- ifelse(Pred_fit1 >= 0.5, 1, 0)\n\n# 混同行列の作成\nTab_Accuracy <- table(df$y, Pred_fit1)\n\nprint(Tab_Accuracy)\n\n   Pred_fit1\n      0   1\n  0 151  20\n  1  29 100\n\n\n正しく分類されているケースの数は151 + 100 = 251であり、これは的中率で言うと0.837になります。同様に、エラー率は1から的中率を引いたものなので0.163になりますね。"
  },
  {
    "objectID": "notes/stats/logit_classification.html#kappa統計量",
    "href": "notes/stats/logit_classification.html#kappa統計量",
    "title": "ロジスティック回帰分析による分類と可視化・評価",
    "section": "\\(\\kappa\\)統計量",
    "text": "\\(\\kappa\\)統計量\n\\(\\kappa\\)はローマ字のKではなく、ギリシャ文字であり、「カッパ」と読みます。的中率は直感的でわかりやすい指標ですが、問題があります。それはデータ内のyの分布に大きく依存するという点です。たとえば、実際のデータに\\(y = 0\\)のケースが90個、\\(y = 1\\)のケースが10個あるとします。この場合、ロジットやKNN、判別分析のような分類機を使わず、すべて0に予測しても的中率は0.9になります。0/1分類の場合、分類機を使わなくても的中率は必ず0.5以上確保できることを意味します。この限界を克服するために考案された指標の一つが\\(\\kappa\\)統計量です。詳細はCohen (1960)1を読んで下さい。ここでは計算方法だけ紹介します。\n先ほどの混同行列を基準に説明します。\n\\[\n\\begin{aligned}\np_0 = & \\frac{TP + TN}{N}, \\\\\np_e = & \\left( \\frac{TP + FP}{N} \\cdot \\frac{TP + FN}{N} \\right) + \\left( \\frac{FN + TN}{N} \\cdot \\frac{FP + TN}{N} \\right),\\\\\n\\kappa = & \\frac{p_0 - p_e}{1 - p_e}\n\\end{aligned}\n\\]\nここで\\(N\\)はデータ内のケース数です。それじゃ混同行列から\\(\\kappa\\)統計量を計算する関数を作ってみましょう。\n\nCalc_Kappa <- function(tab){\n    p0  <- sum(diag(tab)) / sum(tab) # 的中率のことです\n    pe1 <- (sum(tab[, 2]) / sum(tab)) * (sum(tab[2, ]) / sum(tab))\n    pe2 <- (sum(tab[, 1]) / sum(tab)) * (sum(tab[1, ]) / sum(tab))\n    pe  <- pe1 + pe2\n    k   <- (p0 - pe) / (1 - pe)\n\n    return(round(k, 3))\n}\n\nCalc_Kappa(Tab_Accuracy)\n\n[1] 0.664\n\n\n\\(\\kappa\\)統計量は0.664ですね。相関係数と同様、どれくらいの\\(\\kappa\\)が高いかについては合意された基準がありません。ここではViera and Garrett (2005)2の基準だけを紹介します。\n\n\n\n\\(\\kappa\\)の範囲\n評価\n\n\n\n\n\\(\\kappa \\leq 0\\)\nLess than chance agreement\n\n\n\\(0.01 \\leq \\kappa \\leq 0.20\\)\nSlight agreement\n\n\n\\(0.21 \\leq \\kappa \\leq 0.40\\)\nFair agreement\n\n\n\\(0.41 \\leq \\kappa \\leq 0.60\\)\nModerate agreement\n\n\n\\(0.61 \\leq \\kappa \\leq 0.80\\)\nSubstantial agreement\n\n\n\\(0.81 \\leq \\kappa\\)\nAlmost perfect agreement\n\n\n\nこの基準だと、ロジスティック回帰を用いた分類機は「かなり一致 (Substantial agreement)」していると評価できるでしょう。\n\nAIC / BIC\n先ほどの\\(\\kappa\\)統計量にはそれでも何らかの基準はありますが、ここで紹介するAICとBICはそのぼんやりとした基準すらございません。しかし、モデルの選択という場面では今でもAICとBICのような情報量規準は広く使われる指標です。情報量規準については小西・北川 (2004)3やMcElreath (2016)4の第6章を参照して下さい。ここでは計算方法だけ紹介しましょう。\n\\[\n\\begin{aligned}\nAIC = & -2 \\log L + 2K, \\\\\nBIC = & -2 \\log L + K \\log N.\n\\end{aligned}\n\\]\n式内のLは尤度5、\\(K\\)はパラメーターの数、\\(N\\)はサンプルサイズです。今回、パラメーターは切片と説明変数の係数なのでK = 3になります。対数尤度はlogLik()関数から計算できます。\n\nAIC <- -2 * as.numeric(logLik(fit1)) + 2 * 3\nBIC <- -2 * as.numeric(logLik(fit1)) + 3 * log(nrow(df))\n\nprint(AIC); print(BIC)\n\n[1] 252.9979\n\n\n[1] 264.1093\n\n\nAICとBICはそれぞれ252.998、264.109です。良いモデルほどAICとBICは小さくなります。複数のモデルをフィッティングし、どのモデルを採用するか悩む場合、情報量規準は便利な指標です。\n\n\nLOO Cross Validation\n今回はCross Validation (CV)について紹介します。これまで \\(\\kappa\\) 統計量、的中率、情報量規準はCVをするにパソコンのパワーが足りなかった時代に考案されたものです。しかし、機械学習が手軽に実装できるようになった今では、CVを用いたモデル/分類器の評価が広く使われつつあります。\n分類機の第一目標は予測の精度を最大化することです。つまり、\\(X\\)というデータを用いて構築した分類器に、これから入手する新しいデータ (\\(X^{\\prime}\\))を投入することで結果を予測することです。しかし、これまでの評価指標\\(X\\)から生成された分類機に、\\(X\\)を投入したものなので性能が過大評価されやすくなります。なぜなら、分類機は\\(X\\)の予測に最適化されているだけで、\\(X^{\\prime}\\)の予測精度を最大化するものではないからです。\nしかし、\\(X^{\\prime}\\)を入手し、予測を行うとしても、まだ結果 (\\(y^{\\prime}\\))が未知なので、評価のしようがないです。ここで使われるのがCVです。最も単純、かつ強力なCVがLeave-One-Out (LOO) CVです。LOO CVは以下のように計算できます。\n\n既存のデータ (\\(X\\))から\\(x_1\\)を除外し、分類機を生成 → \\(x_1\\)を投入して予測し、結果を比較する。\n\\(x_1\\)を戻し、次は\\(x_2\\)をデータから除外してから分類機を生成 → \\(x_2\\)を投入して予測し、結果を比較する。\n\\(x_2\\)を戻し、次は\\(x_3\\)をデータから除外してから分類機を生成 → \\(x_3\\)を投入して予測し、結果を比較する。\nこのような手順を\\(x_N\\)まで繰り返し、エラー率を計算する。\n\n説明よりは、実際にやってみた方がいいでしょう。まずはindex番目のケースを除外して分類機の生成、index番目データを用いて予測結果を比較するLOO_Logit()関数を作成します。\n\nLOO_Logit <- function(index){\n    # index番目のデータを除いてModel Fitting\n    new_fit <- glm(y ~ x1 + x2, data = df[-index, ], \n                   family = binomial(\"logit\"))\n\n    # index番目のデータを投入して予測\n    new_pred <- predict(new_fit, newdata = data.frame(x1 = df$x1[index],\n                                                      x2 = df$x2[index]),\n                        type = \"response\")\n    new_pred <- as.numeric(new_pred >= 0.5)\n\n    # 実際のデータと比較。一致するなら1、不一致なら0\n    return(as.numeric(new_pred == df$y[index]))\n}\n\n続いて、index = 1からindex = 200まで実行してみます。for()文を使ってもいいですが、ベクトルで渡した方が早いでしょう。\n\nLOOCV <- LOO_Logit(1:200)\n\nprint(LOOCV)\n\n  [1] 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n [38] 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n [75] 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n[149] 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1\n[186] 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1\n\n1 - mean(LOOCV)\n\n[1] 0.18\n\n\nこの0.18がエラー率です。この場合、200ケースの中、164個が正しく予測されましたよね。悪くない性能だと言えます。\nLOO CVは手元のデータをほとんど使う (\\(N - 1\\))という点で強力なCV手法ですが問題があります。それはケースの数だけ繰り返す必要があるという点です。今回の例は一瞬で出来ましたが、データのサイズが大きい場合やNeural network, Support vector machine (SVM)など処理時間が掛かる手法にはかなり無理があります。LOO CVの精度を若干犠牲にして効率性を重視したものが、次に紹介するk-fold CVです。\n\n\nk-fold Cross Validation\nk-fold Cross ValidationはLOO非常に近い概念です。ただし、今回は分析をN回行うのではなく、k回行います。kをどれくらいにするかは一般的な合意はないと思いますが、k = 5かk = 10が主流です。ここではk = 5の例で説明します。\nやり方は簡単です。データをランダムな順でk分割します。そうしたらk個のデータの塊で出来るわけです。ここでk-1個の塊 (train set) で分類機を生成し、残りの1個 (test set)で検証を行い、エラー率を算出します。つづいて、検証で用いたデータは分類規生成用のデータの方に戻し、他の1個の塊を検証用にします。最後に得られたk個のエラー率の平均値を計算します。これならk回でCVが済みますね。それではやってみましょう。\n\n# まずデータの順番をランダム化します\nk_index <- sample(1:nrow(df), nrow(df), replace = FALSE)\n\n# 次は、40個ずつのデータに分割します。\nk1_index <- k_index[1:40]\nk2_index <- k_index[41:80]\nk3_index <- k_index[81:120]\nk4_index <- k_index[121:160]\nk5_index <- k_index[161:200]\n\n# k-fold CVの関数を作成\n# LOO CVとほぼ同じです。\nkFold_Logit <- function(index_vec){\n    new_fit <- glm(y ~ x1 + x2, data = df[-index_vec, ], \n                   family = binomial(\"logit\"))\n    new_pred <- predict(new_fit, newdata = data.frame(x1 = df$x1[index_vec],\n                                                      x2 = df$x2[index_vec]),\n                        type = \"response\")\n    new_pred <- as.numeric(new_pred >= 0.5)\n    cv_tab   <- table(df$y[index_vec], new_pred)\n\n    return(1 - sum(diag(cv_tab)) / sum(cv_tab))\n}\n\n# 5-fold CVの実行\nkFold_Error <- rep(NA, 5)\nkFold_Error[1] <- kFold_Logit(k1_index)\nkFold_Error[2] <- kFold_Logit(k1_index)\nkFold_Error[3] <- kFold_Logit(k1_index)\nkFold_Error[4] <- kFold_Logit(k1_index)\nkFold_Error[5] <- kFold_Logit(k1_index)\n\nprint(mean(kFold_Error))\n\n[1] 0.175\n\n\nCVの結果、平均エラー率は0.175です。先ほどのLOO CVの結果と大きな差はありませんね。\n以上、紹介したモデル評価指標は複数モデル間の比較に有用です。複数のモデルというのは関数型が異なるモデル、あるいは異なる分析手法などを指します。今回はモデルが一つしかありませんでしたが、まだ機会があったら色んな可視化とモデル比較をしてみたいと思います。"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "専門演習 (学部; 月4)\nミクロ政治分析 (学部; 木3)\nミクロ政治データ分析実習 (学部; 木4)\n導入ゼミ (学部; 金5)\n公共領域におけるデータベース (修士; 月7・木6)\n\n\n\n\n\n専門演習 (学部; 月4)\n政治学 (学部; 木2)\nマクロ政治分析 (学部; 木3)\nマクロ政治データ分析実習 (学部; 木4)\n公共領域におけるデータベース (修士; 月7・木6)\n社会科学における因果推論 (修士; 月2)\n\n\n\n\n\n方法論特殊講義III"
  },
  {
    "objectID": "teaching/index.html#過去の担当科目",
    "href": "teaching/index.html#過去の担当科目",
    "title": "Teaching",
    "section": "過去の担当科目",
    "text": "過去の担当科目\nこれまで担当してきた講義のシラバスおよび講義資料を公開します。学期が終了した講義については原則、シラバスのみの公開となります。過去の講義資料の閲覧を希望する方は宋までご連絡ください。\n\n大阪市立大学\n\n法学政治学方法論 (2017前)\n\n\n\n関西大学\n\n展開講義（政治学方法論）(2018)\n計量分析 (2018)\nミクロ政治分析 (2018-)\nミクロ政治データ分析実習 (2018-)\n導入ゼミ (2018-)\n公共領域におけるデータベース (2018-)\n政治学 (2018-)\nマクロ政治分析 (2018-)\nマクロ政治データ分析実習 (2018-)\n\n\n\n京都女子大学\n\nデータ処理論 I (2017前)\nデータ処理論 II (2016後, 2017後)\n\n\n\n京都府立大学\n\n情報処理応用演習 (2020夏集, 2021夏集)\n\n\n\n慶應義塾大学\n\n方法論探求 (2019後)\n\n\n\n神戸大学\n\n計量政治学方法論I (2018夏集, 2019夏集, 2020夏集, 2021夏集)\n\n\n\n東京大学\n\n特殊講義: ヨーロッパ国際政治経済 (2020冬集)\n\n\n\n同志社大学\n\n文化情報学入門 (2020前)\nデータサイエンス入門 (2020前)\nデータサイエンス入門演習 (2020前)\n文化情報学演習3 (2020前)\nデータサイエンス基礎 (2020後)\nデータサイエンス演習 (2020後)\nコロキアム (2020後)\nアドバンスト・コロキアム (2020後)\n文化情報学演習2 (2020後)\n\n\n\n早稲田大学\n\n政治学実験 (2019前)\n研究方法集中セミナー（因果推論） (2021夏集)"
  },
  {
    "objectID": "seminar/index.html",
    "href": "seminar/index.html",
    "title": "宋ゼミを志望する皆様へ",
    "section": "",
    "text": "本ゼミは社会現象のデータ分析（+美味しい飲食店情報の共有）を主な内容とします。ここでは一つの例を見ながら考えてみましょう（宋の専門は政治学であるため、政治の例を挙げます）。\n　2016年度の参院選から18歳投票権が導入されました。若者の政治参加が期待されている中、選挙後年齢別投票率が公開されました。以下のグラフは都道府県別に投票率を示したものです。\n\n\n\n\n\n　これまで日本では都市部の投票率が低く、非都市部の投票率が高いと言われていました。実際、これまで参政権を有していた20歳以上に限定してみると（青い棒）、長野、島根、山形当たりが最上位となります。一方、都市部の東京はやや高めでしたが、その他の大阪、愛知、千葉当たりの投票率は低い傾向を示しています。しかし、18歳の投票率はそうではありませんでした。最も高いトップ3は東京、神奈川、愛知といった都市部でした。若者ほど投票に参加しないのが長年の観察から得られた傾向ですが、これらの都道府県は18歳の投票率が20歳以上の投票率を凌駕しています。これはなぜでしょうか。\n　他にも様々な問いが考えられます。我々が街中で見る「投票に行きましょう」ポスター、これは本当に効果があるでしょうか。世論調査と実際の選挙結果がずれることはなぜでしょうか。みんな仲良くすれば良いのに、なぜ戦争が起こるでしょうか。\n　このように社会には「なぜ？」や「本当に？」といった謎（パズル）がいっぱいあります。本ゼミはこのような社会における謎を対象に、データを用いた実証分析を行います。近年、データの重要性が高まっており、データサイエンティストの需要が急増しています。データは社会の謎を解く一つの鍵でありながら、より良い社会を実現するための鍵でもあります。データを有効活用するためにはデータ分析そのものの発展も不可欠です。本ゼミでは2年間にわたって（１）問題を発見し、（２）問いを設定し、（３）問いに対する答えを導き出すためのスキルを身につけることを目標とします。\n　以下の内容は現時点での予定です。2022年度はゼミ1期目であり、皆さんと相談しながらゼミの進め方を調整していく予定です。\n\n\n\n前期\n\n文献講読（リサーチデザイン）\n分析ツール（R）の使い方の習得\nデータ分析の技法（基礎）\n\n後期\n\nデータ分析の技法（基礎）\nグループ単位の共同研究\n\n\n\n\n\n\n前期\n\nデータ分析の技法（応用）\n既存研究の再現（replication/reproduction）\n\n後期\n\n卒論に向けた個人研究"
  },
  {
    "objectID": "seminar/index.html#宋は何者か",
    "href": "seminar/index.html#宋は何者か",
    "title": "宋ゼミを志望する皆様へ",
    "section": "宋は何者か",
    "text": "宋は何者か\n　私は政治行動論と政治学方法論を専門としています。政治行動論は政治における様々な主体（有権者、市民、政治家、官僚、マスコミ、若者、ネット民など）の行動や意識を観察したり、因果関係を分析する学問ですが、私の主な観察対象は有権者です。この観察や分析を支えるのが政治学方法論です。観察には観察済み（=既に公開されている）のデータを使うこともあれば、自分の問いを検証するために世論調査などを通じて観察する場合もあります。\n　要するに私の仕事は政治的現象をデータを用いて分析することです。政治学 \\(\\times\\) データサイエンスについては以下の動画を参照してください。\n【ロシアの不正vs政治学者】ハーバード大学の天才教授【データで暴く真実】\n\n\n\n\n【選挙守る学者の戦い】ハーバード大学教授の挑戦【サンデルの同僚】\n\n\n\n\n　私の研究活動に興味のある方はResearchページ、使用するツールに興味のある方はTutorialページを参照してください。とりわけ、ゼミの担当教員がどのような研究をする人かを知ることは重要であり、研究の履歴を見ることをおすすめします（本ゼミに限らず、全ゼミに共通します）。\n　趣味はグルメとゲームです。麺類（とりわけラーメン）が好きですが、最近は少し控えております。ゲームは最近、Final Fantasy XIV（2022年7月14日現在、5.0）をプレイしております。"
  },
  {
    "objectID": "seminar/index.html#宋ゼミが求める人物像",
    "href": "seminar/index.html#宋ゼミが求める人物像",
    "title": "宋ゼミを志望する皆様へ",
    "section": "宋ゼミが求める人物像",
    "text": "宋ゼミが求める人物像\n\n1. 積極性\n費やした時間と成果は比例します\n　ネットワーク分析で有名なバラバシ先生は、成功（success）のためには何度もチャレンジしていくことが重要であることを強調しています。大変面白い動画ですのでぜひご覧になってください（日本語字幕もあります）。\nAlbert-László Barabási: The real relationship between your age and your chance of success\n\n\n\n\n\n\n\n[I]f you keep trying, you could still succeed and succeed over and over.\n\n　皆さんがスキルの習得に費やした時間と、実際に得られたスキルは必ず正の関係を持ちます。人によっては100を達成するために1時間を要する人もいれば、5時間を要する人もいるでしょう。スピードに差はあるかも知れませんが、時間とレベルは必ず比例します。体育や芸術などとは違って、データ分析に才能というものは存在しないと、宋は考えております。ただし、年45時間のゼミだけでデータ分析、リサーチデザインなどのスキル全て身につけられるとは期待しないでください。これの数倍、数十倍の時間が必要です。したがって、ゼミ外時間の活動が非常に重要です。特に夏休み・冬休みはスキルアップのための絶好の時期であり、他人と差を付けたいのなら休み期間がチャンスです。ゼミ外時間、そして夏休み・冬休みにも積極的に学習・研究活動に取り組む方を歓迎します。\n　ゼミ生からの要望がありましたら、サブゼミとして分析ツール講習会の開催も検討します。また、2022年度は新型コロナで無理だと考えられますが、今後、方法論の合宿も行う可能性もあります。\nたくさんの失敗を味わいましょう\n　パソコンを触っていくと、何度もエラーメッセージが遭遇したり、自分が欲しい結果が得られない時もあるでしょう。人によってはエラーメッセージなんかに遭遇しない人もいるかも知れません。しかし、現場において最も重宝されるのはたくさんの失敗を経験した人です。成功の経験しかない人は、失敗の時の対応が苦手です。失敗しを繰り返し、それを克服していく過程そのものに大きな意味があります。失敗を恐れない方を歓迎します。\n海外の論文を読みましょう\n　輪読の際、基本的には日本語の文献（本・論文）を読みますが、必要に応じて英語の文献も読む必要があります。多くの学生が「XXに関する先行研究は存在しない。だから本稿ではXXについて分析を行う」と書いたりしますが、これは高確率で誤りです。ほとんど研究の場合、先行研究が存在します。存在を知らないだけです。たまに、天才的な発想で誰も思いつかなかったテーマを見つけるケースも稀にありますが、本当に稀（SSR）です。また、本当に先行研究が存在しないとしてもそれは現実的に研究の遂行が困難なテーマか、学術的にあまり意味のないテーマである可能性が高いです。皆さんが何かのテーマを決めたらそれに関連する文献は必ずあると考えても良いでしょう。そして、それらのほとんどは英語で書かれています。英語の論文を一回も読んだことのない学生にとって、英語の論文は苦痛でしかないでしょう。しかし、英語の論文はかなり構成が論理的であるため、言語の側面を除けば逆に読みやすいです。また、最近は機械翻訳の精度も非常に高く、こちらも合わせて使うことも可能でしょう（ただし、機械翻訳を盲信してはいけません。たまにとんでもない結果を返したりします）。卒業研究に取り組むまでたくさんの論文を読むことになりますが、日本語の文献だけでなく、海外の文献にも挑戦する方を歓迎します。\nサークル活動について\n　サークル活動と並行することは全く問題ございませんが、サークル活動の配慮は致しません。欠席に関しましては公欠扱いが可能でしたら公欠としますが、学習やグループワークにおける遅れは全て自己責任となります。欠席された分、一人で遅れを補う積極性が求められます。\n\n\n2. コミュニケーション能力\n　多くの学問分野において共同作業、つまり「共著」が一般化しています。これまで「共著」といえば理系のイメージが強く、文系は「単著」文化だと認識されてきました。総合情報学部は文理融合学部です。共著も単著も体験して頂きます。文系においてもごく一部の分野を除き、共著文化が定着しつつあります。私が携わっている政治学も例外ではありません。共同作業においてコミュニケーション能力が必須です。これは「他人と楽しくしゃべる」ことを意味するわけではありません。ここでのコミュ力とは何かがあったら皆と相談する、途中経過を共有する、他人からのフィードバックに耳を傾けるといったものを指します。共同研究においてこれらは非常に大事です。\n　最終的には単著で卒業論文を執筆することになりますが、一人で執筆するとしてもコミュニケーション能力は重要です。研究のプロである研究者たちも一つの論文を出すまで何回も研究会・学会で報告したり、他研究者との相談を通じてフィードバックをもらいます。単著は独り善がりの研究になりやすい故、このようなコミュニケーションは非常に重要です。\n　宋にとって日本語は外国語ですし、文法も、アクセントもめちゃくちゃです。それでも人とコミュニケーションをとっています。上手に喋るに越したことはありませんが、大事なのは発信と意見の受け入れを恐れない態度です。\n\n\n3. 探求力と知的好奇心\n　何かを継続していくには、そこから何かの意味を見出さなければなりません。以下の動画を御覧ください（日本語字幕もあります）。行動経済学者のアリエリー先生のTED公演動画です。\nDan Ariely: What makes us feel good about our work?\n\n\n\n\n\n\n　データ分析手法を身につける過程は退屈な作業の連続です。継続していくためには、目標が必要です。単に「データ分析に慣れたい/上達したい」という曖昧な目標だけでは、途中で諦めやすいです。そもそも「慣れ」や「上達」は主観的なものだからです。やはり「XXを明らかにしたい」といった具体的な問いがあって、その問いに答えるためのスキルとしてデータ分析を習得していく必要があります。むろん、美しいコードを作成したり、綺麗なグラフを作ることが目的の方には、これ自体が目的ですので、良いでしょう（宋がこのタイプの人間です）。あるいは、データ分析に関する資格（統計検定など）の取得のような目標も良いかも知れません。「XXを達成したら絶対うれしい！」といった具体的な目標を持つ方を歓迎します。"
  },
  {
    "objectID": "seminar/index.html#宋ゼミの到達点",
    "href": "seminar/index.html#宋ゼミの到達点",
    "title": "宋ゼミを志望する皆様へ",
    "section": "宋ゼミの到達点",
    "text": "宋ゼミの到達点\n　宋ゼミでは（１）リサーチデザイン、（２）当該分野に関する知識、（３）分析ツール、（４）データ収集、（５）統計学と分析手法に関する知識を身に付けることを目的とします。これら5つのスキルについて背景知識があるに越したことはありませんが、必須ではありません。そもそもこれらのスキルを既に身につけているのであれば、わざわざ宋ゼミに来る必要はないでしょう。以下の内容は前提知識ではなく、これから2年間身につけていくスキル、つまり到達目標です。\n\n1. リサーチデザイン\n　何か明らかにしたい問い（リサーチ・クエスチョン）が見つかったら、その問いに答えるためのプロセスを考える必要があり、これがリサーチデザインです。リサーチデザインがしっかりしていれば、研究として最低限の質は保障されます。一方、斬新なテーマ、または問いであってもリサーチデザインがしっかりしていなければ、研究として成り立ちません。自然科学もそうですが、社会科学も今日はリサーチデザインの雛形といったものが決まっております。論文の章立てとしては問題意識、理論・仮説、データ、分析手法およびモデル、分析結果、結論といった順となっており、実際に研究を進めていく際もこの順番に従うことになります。\n　社会科学では問いに対する暫定的な答えを用意し、この暫定的な答えの真偽を判定することになります。この暫定的な答えが仮説と呼ばれるものであり、これは印象論や想像でなく、これまでの先行研究や自明な公理の積み重ねで導出されるものです。そのためには問い関する先行研究を読む必要があります。\n　リサーチデザインは授業として教わることもできますが、個人の経験が重要です。様々な論文、本を読みながらリサーチデザインを習得して頂きます。優れたリサーチデザインの研究は参考の対象となりますし、そうでない研究でも反面教師として利用することができます。むしろ、リサーチデザインが優れていない研究が見つかればラッキーです。なぜなら、そのリサーチデザインを自分で改良することで一つの立派な研究になるからです。\n　本ゼミではリサーチデザインに関する教科書の輪読に加え、いくつかの論文/書籍を読みながらリサーチデザインの感覚を身につけます。これはグループワークおよび卒業論文においても最も重要な内容です。\n\n\n2. 当該分野に関する知識（ドメイン知識）\n　社会現象を分析するためには、その現象に関する知識が必要です。たとえば、「若者の投票率はなぜ低いか」という問いに取り組むのであれば、政治学や政治心理学などの知識が必要です。関西大学総合情報学部には様々な分野に関する科目が開設されております。政治学だと、「政治学」、「政治過程論」、「公共政策論」、「パブリック・アドミニストレーション論」、「ミクロ政治分析」、「マクロ政治分析」などがあります。自分が関心のある分野の授業にも積極的に挑戦してみてください。むろん、これらの分野に関する教科書で独学するのも良いでしょう。総合情報学部は幅広し分野をカバーしますが、全てをカバーしているわけではありません。また政治学の話ですが、総合情報学部では政治理論、政治哲学、国際政治に関する講義は設けられておりません。しかし、これらの分野にも優れた教科書は多数公刊されていますし、一人でも十分学習できます。\n\n\n3. 分析ツール\n　本ゼミでは共同作業における共通言語としてRを使います。これはRがベストな統計ソフト/言語だからではありません。むしろ、Rはプログラミング言語としてはあまり良くないという意見が多いですし、宋もそう思っています。しかし、共同作業において共通言語は必要です。皆が使う統計ソフトがバラバラだったら、共同作業は非常に難しいでしょう。\n　ならば、なぜRでしょうか。1つ目の理由は無料で誰でも使えるからです。世の中には大変便利な商用ソフトで溢れています。しかし、共同作業のメンバー全員がそのソフトを持っているとも限りませんし、これらのソフトウェアは安くても数万円、高ければ百万円以上となります。「自分は買える」は「他人も買える」ではありません。2つ目の理由は世の中に存在するほぼ全ての分析ができるからです。世の中には様々な統計手法がありますが、ほとんどの分析がRでできます。最近は新しい手法を発表したら、それを実装したRパッケージも一緒に公開するケースが多いです。3つ目の理由は学習資料が最も豊富だからです。本だけでなく、ウェブ記事やチュートリアルも非常に充実しており、独学にも向いています。Rの使い方に関しては3回生以上科目の「ミクロ政治データ分析実習」などがあります。\n　データサイエンス業界ではRとPythonが2大言語であり、最近はJuliaも注目されています。どれも無償で使えるため、こちらを使っても構いません。ただし、共同作業の場合、メンバー全員がその言語を使う必要があります。また、卒業論文のような個人研究の場合、SPSS、Stata、SASなどの有償ソフトウェアの使用を認めます。\n　分析ツールの学習は原則、独学となります。ゼミ生の要望がある場合、希望者を対象にサブゼミとして講習会を開くことはできます。また、Rを使用する授業として「ミクロ政治データ分析実習」と「マクロ政治データ分析実習」があります。ゼミ内では主にリサーチデザイン、文献講読、グループワークを中心に行う予定です。\n\n\n4. データ収集\n　データ分析を掲げている以上、データは不可欠です。データを集める前に、私たちは分析の単位を決める必要があります。たとえば、「都市部の有権者は投票率が低い」という仮説があるとします。この場合、分析の単位は個人です。つまり、データの各行は一人一人の個人を表す必要があります。そして、世論調査を行い、回答者が暮らしている地域の規模、そして直近の選挙に投票したか否かを尋ね、データを集める必要があります。この場合、社会調査法の知識が必要となります。しかし、世論調査を行うには非常に大きなコストが必要であり、個人で行うことは困難です（できないという意味ではありません。数万円でできたりもします。）。場合によっては既に他の研究者、機関が行った世論調査データを使うこともあります。このように既に公表されているデータを用いた分析を「2次分析」と言います。ただし、注意すべき点は「集計データ」でなく、「個票データ」を使う必要があるということです。日本の場合、個人データの公開は非常に消極的であるため、注意が必要です。\n　世論調査ができず、公開されているデータもない場合は分析の単位を変えることが考えられます。仮説を「都市部はその他の地域より投票率が低い」に変えると、分析の単位は市区町村、または選挙区になります。このようなデータはネット上で手軽に入手することができます。各市区町村の選管ホームページなどを周りながらデータを集めたり、情報公開請求をするなどが必要ですが、金銭的なコストはほぼ発生しません。ただし、このように個々人のデータを集計したデータ、つまり集計したデータを使用すると生態学的誤謬が生じる可能性がある点には注意が必要です。\n\n\n5. 統計学と分析手法\n　言うまでもありませんが、データ分析を行うためには統計学および分析手法に関する知識が必要です。近年のデータサイエンスのブームの影響もあってデータ分析に関する書籍が数百冊以上出版されており、優れたインターネット記事も多く公開されています。また、総合情報学部にはこれに関連する授業が充実しており、学習環境としては最適です。データ分析のためのソフトウェアを使えば、分析は数秒で終わります。たとえば、Rで回帰分析を行うコードは1行で済みます。チュートリアル本や記事が溢れているため、これらのスキルを身につけることは難しくありません。実際、社会科学におけるデータ分析のプロセスにおいて、分析そのものは全体の1〜2割程度に過ぎず、残りの8〜9割はリサーチデザイン、先行研究のレビュー、データ収集、データの前処理、データ/結果の可視化といった作業です。ただし、これが「データ分析は全体のプロセスにおいてあまり重要ではない」ことを意味しません。ほとんどはパソコンが瞬時にやってくれるという意味です。あくまでも費やす時間として短いだけです。\n　重要なのは「なぜこの分析を使うのか」、「この分析手法は自分の問いに答えるために適切な手法か」、「分析結果をどう解釈するか」、「分析結果は自分の問いに答えているのだろうか」など、パソコンがやってくれる作業以外のことですし、そのためには確率、線形代数、統計的仮説検定、可視化などの知識が必要です。これらの知識は授業、教科書、参考書、ネット記事などから身につける必要があり、非常に長い道のりです。たとえば、「若者と高齢者の間に投票率の差があるか」といった単純な問いに答えるための母平均の差の検定（t検定）を行うこと自体は1行で済みますが、その結果を正しく解釈するためにはt検定の仕組み、その仕組を理解するためには統計的仮説検定、統計的仮説検定を理解するためには中心極限定理と大数の（弱）法則、また、これらをりかいするためには確率の知識が必要です。これらの知識については近年、分析ツールの使い方を学びながら同時並行的に学習できるような資料もたくさん出ております。教科書の相性は人それぞれですので、ぜひ図書館や本屋などでいくつかの教科書に接してみてください。むろん、宋も積極的にサポートします。"
  },
  {
    "objectID": "seminar/index.html#宋ゼミの活動に関連する資料",
    "href": "seminar/index.html#宋ゼミの活動に関連する資料",
    "title": "宋ゼミを志望する皆様へ",
    "section": "宋ゼミの活動に関連する資料",
    "text": "宋ゼミの活動に関連する資料\n以下のリストはほんの一部です。これから少しずつ追加しておきます。以下の資料をゼミ中に全て読むことはできませんが、何冊かピックアップして輪読する予定です。【】が付いている書籍は当該年度に使用した資料です。\n\nリサーチデザイン\n\n【2022】久米郁男. 2013. 『原因を推論する』有斐閣\n【2022】浅野正彦・矢内勇生. 2018. 『Rによる計量政治学』オーム社（第1〜3章）\nG.キング・R.O.コヘイン・S.ヴァーバ (著), 真渕勝 (訳). 2004. 『社会科学のリサーチ・デザイン―定性的研究における科学的推論』勁草書房 (2021年、待望の新版が出ました。)\n伊藤修一郎. 2011.『政策リサーチ入門―仮説検証による問題解決の技法』東京大学出版会\n野村康. 2017.『 社会科学の考え方―認識論、リサーチ・デザイン、手法―』名古屋大学出版会\n\n\n\n分析ツール (R) の使い方\n\nHadley Wickham・Garrett Grolemund (著), 黒川利明 (訳). 2017『Rではじめるデータサイエンス』オライリージャパン (原著はウェブで読めます (https://r4ds.had.co.nz/))\n松村優哉・湯谷啓明・紀ノ定保礼・前田和寛. 2021. 『RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界（改訂2版）』技術評論社\n宋財泫・矢内勇生『私たちのR: ベストプラクティスの探求』web-book.\n無料で読めるR教材のリスト\n\n\n\n様々な分析手法について\n\n【2022】浅野正彦・矢内勇生. 2018. 『Rによる計量政治学』オーム社（第4章〜）\n今井耕介 (著), 粕谷祐子・原田勝孝・久保浩樹 (訳). 2018. 『社会科学のためのデータ分析入門 (上/下)』岩波書店\n飯田健. 2013. 『計量政治分析』共立出版\n\n\n\n因果推論\n\n伊藤公一朗. 2017. 『データ分析の力』光文社新書\n中室牧子・津川友介. 2017.『「原因と結果」の経済学―データから真実を見抜く思考法』ダイヤモンド社\n安井翔太. 2020. 『効果検証入門』技術評論社\n【2022】松林哲也. 2022. 『政治学と因果推論』岩波書店\n高橋将宜. 2022. 『統計的因果推論の理論と実装』共立出版\n因果推論に関する各種資料\n\n\n\n(参考) 政治学の教科書\n政治学分野の教科書/専門書はいつでも紹介できます。ご希望の方は宋に相談してください。\n\n砂原庸介・稗田健志・多湖淳. 2020. 『政治学の第一歩〔新版〕』 有斐閣.\n久米郁男・川出良枝・古城佳子・田中愛治・真渕勝. 2011. 『政治学 補訂版』有斐閣.\n坂本治也・石橋章市朗 (編). 2020. 『ポリティカル・サイエンス入門』法律文化社.\n伊藤光利. 2009. 『ポリティカル・サイエンス事始め (第3版)』有斐閣.\n上神貴佳・三浦まり. 2018. 『日本政治の第一歩』有斐閣.\n飯田健・松林哲也・大村華子. 2015. 『政治行動論ー有権者は政治を変えられるのか』有斐閣\n山田真裕・飯田健. 2009. 『投票行動研究のフロンティア』おうふう.\n建林正彦・曽我謙悟・待鳥聡史. 2008. 『比較政治制度論』有斐閣.\n伊藤光利・真渕勝・田中愛治. 2000. 『政治過程論』有斐閣."
  },
  {
    "objectID": "seminar/index.html#ゼミ生",
    "href": "seminar/index.html#ゼミ生",
    "title": "宋ゼミを志望する皆様へ",
    "section": "ゼミ生",
    "text": "ゼミ生\n\n2022年度入ゼミの第1期生 (50音順)\n\n井上達望（いのうえ・たつみ）さん\n川東奈央（かわとう・なお）さん\n岐部夢奈（きべ・ゆめな）さん\n佐伯のの花（さえき・ののか）さん\n末本健心（すえもと・けんしん）さん\nD・Sさん 【ゼミ代表】\n中尾真広（なかお・まひろ）さん\n永江倫佳（ながえ・りんか）さん\n樋口遼子（ひぐち・りょうこ）さん\n本田良太郎（ほんだ・りょうたろう）さん\n孟揚帆（もう・ようほ）さん\n山本薫（やまもと・かおる）さん\n\n\n\n2023年度入ゼミの第2期生 (50音順)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Jaehyun Song, Ph.D.",
    "section": "",
    "text": "CV\n Open in new window\n\n\n\n\nContact\n\nMail: song [at] kansai-u.ac.jp\nAddress\n\n〒569-1095 大阪府高槻市霊仙寺町2-1-1\n\n個人研究室: TA227 (A棟2階)\n\nRyozenji 2-1-1, Takatsuki, Osaka 569-1095, Japan\n\nOffice: TA227, Bldg. A (2nd floor)\n\n\nAccess\n\n高槻 (JR)、摂津富田 (JR)からバス\nタクシー\n\n高槻 (JR)から約2200円、摂津富田 (JR)から約2000円\n参考) taxisite調べ"
  }
]